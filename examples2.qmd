---
code-fold: show
jupyter: python3
editor_options: 
  chunk_output_type: console
---

# Przykłady NN

Naturalnym podejściem do obu poprzednich zadań może być również wykorzystanie
sieci neuronowych.

## Przykład 1

Rozwiązanie przykładu z poprzedniego rozdziału można dokonać z dużo lepszą
precyzją wykorzystując sieci neuronowe. W tym przykładzie po raz kolejny
wygenerujemy dane do uczenia[^examples2-1], a następnie wytrenujemy sieć (dosyć
płytką) MLP.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_regression
from sklearn.model_selection import RepeatedKFold
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Dense, Dropout, BatchNormalization, ReLU, LeakyReLU, ELU
from keras.callbacks import EarlyStopping
import keras
```

```{python}
# generowanie danych do zadania
X, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)

n_neurons = [10,20,50]
```

Model będzie bardzo prosty, składający się z tylko jednej warstwy ukrytej.

```{python}
def get_model(n_inputs, n_outputs, n_neurons):
  model = Sequential()
  model.add(Dense(int(n_neurons), input_dim=n_inputs, activation='relu'))
  model.add(Dense(n_outputs, activation='linear'))
  model.compile(loss='mse', optimizer='adam')
  return model

```

W tym przykładzie chciałem również pokazać jak wykonywać trenowanie sieci z
użyciem sprawdzianu krzyżowego, który pomoże nam ustalić optymalną liczę
neuronów w warstwie ukrytej.

```{python}
# ocena dopasowania modelu z wykorzystaniem CV
def evaluate_model(X, y, n_neurons):
	results = list()
	n_inputs, n_outputs = X.shape[1], y.shape[1]
	# definicja CV
	cv = RepeatedKFold(n_splits=5, random_state=1)
	# pętla po foldach
	for train_ix, test_ix in cv.split(X):
		# przygotowanie danych
		X_tr, X_te = X[train_ix], X[test_ix]
		y_tr, y_te = y[train_ix], y[test_ix]
		# określenie modelu
		model = get_model(n_inputs, n_outputs, n_neurons)
		# dopasowanie modelu
		model.fit(X_tr, y_tr, verbose=0, epochs=100)
		# ocena dopasowania na foldzie testowym
		mae = model.evaluate(X_te, y_te, verbose=0)
		results.append(mae)
	return results

```

```{python}
#| eval: false

results = []
for i in n_neurons:
  # dopasuj i oceń model na zbiorze uczącym
  results.append(np.mean(evaluate_model(X_train, y_train, i)))

```

```{python}
results = np.load("./data/mlp_eval.npz")
results = results['arr_0'].tolist()
for i in range(len(n_neurons)):
  print(f"Dla {n_neurons[i]} neuronów MAE: {results[i]:.0f}")

```

Najlepszy rezultat osiągamy dla 50 neuronów i taki parametr dobierzemy w
ostatecznym modelu.

```{python}
keras.utils.set_random_seed(44)
my_callbacks = [
    EarlyStopping(patience=2)
]

model = get_model(X_train.shape[1], y_train.shape[1], 50)
history = model.fit(X_train, y_train, 
                    verbose=0, epochs=1000, 
                    validation_split=0.2, callbacks=my_callbacks)
```

Proces uczenia przebiegał prawidłowo i osiągnięto niski poziom funkcji straty.

```{python}
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.show()
```

Wyniki dopasowania znacznie przekraczają wyniki uzyskane metodami z
poprzedniego rozdziału.

```{python}
y_pred = model.predict(X_test, verbose=0)
rmse_mlp = mean_squared_error(y_test, y_pred, squared=False)
r2_mlp = r2_score(y_test, y_pred)
print(f"R2 on test sample: {r2_mlp:.2f}")
print(f"RMSE on test sample: {rmse_mlp:.1f}")
```

[^examples2-1]: te same co z przykładu regresyjnego z poprzedniego wykładu

## Przykład 2

W tym przykładzie jeszcze raz rozpatrzymy zadanie klasyfikacyjne z wieloma
wyjściami z poprzedniego rozdziału. Przeprowadzimy czynności *preprocessingu*
podobne jak w poprzednim rozdziale, dodając jeszcze standaryzację, która dla
sieci neuronowych jest bardzo ważna.

```{python}
from sklearn.preprocessing import StandardScaler
dt = pd.read_csv("./data/original.csv", index_col = "id")
combinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()
idx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()
bad_combinations = combinations[idx<10]
idx = []
for i in range(len(dt)):
  idx.append(True)
  for j in range(len(bad_combinations)):
    if all(dt.iloc[i, 31:] == bad_combinations[j]):
      idx[i]=False

dt = dt.iloc[idx,:]

y = dt.iloc[:,31:].to_numpy()
X = dt.iloc[:,:31].to_numpy()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)

scaler = StandardScaler().fit(X_train)
X_test = scaler.transform(X_test)
X_train = scaler.transform(X_train)
```

Następnie przygotujemy model sieci neuronowej, która pozwoli na właściwe
klasyfikacje obiektów.

```{python}
model = Sequential()
model.add(Dense(15, input_dim=X_train.shape[1]))
model.add(ReLU())
model.add(Dropout(0.1))
model.add(Dense(y_train.shape[1], activation='sigmoid'))

opt = keras.optimizers.Nadam(0.001)
model.compile(loss='binary_crossentropy', optimizer=opt)
model.summary()
```

```{python}
keras.utils.set_random_seed(44)
my_callbacks = [
    EarlyStopping(patience=10)
]
history = model.fit(X_train, y_train, epochs=1000,  validation_split=0.4, verbose=0,
callbacks=my_callbacks)
```

```{python}
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper right')
plt.show()
```

```{python}
from sklearn.metrics import accuracy_score
y_pred = model.predict(X_test, verbose=0)
y_class = y_pred.round()
acc = accuracy_score(y_test, y_class)
print(f"ACC on test sample: {acc:.3f}")
```

### Podsumowanie

Model prostej sieci neuronowej nie poprawił znacząco jakości dopasowania w
stosunku do modelu lasu losowego adaptowanego do zadania z wieloma wyjściami.
