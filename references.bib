@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97–111},
  numpages = {15}
}



@article{spyromitros-xioufisMultiTargetRegressionInput2016,
  title = {Multi-{{Target Regression}} via {{Input Space Expansion}}: {{Treating Targets}} as {{Inputs}}},
  shorttitle = {Multi-{{Target Regression}} via {{Input Space Expansion}}},
  author = {{Spyromitros-Xioufis}, Eleftherios and Tsoumakas, Grigorios and Groves, William and Vlahavas, Ioannis},
  year = {2016},
  month = jul,
  journal = {Machine Learning},
  volume = {104},
  number = {1},
  eprint = {1211.6581},
  primaryclass = {cs},
  pages = {55--98},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-016-5546-z},
  urldate = {2024-01-09},
  abstract = {Real world prediction problems often involve the simultaneous prediction of multiple target variables using the same set of predictive variables. When the target variables are binary, the prediction task is called multi-label classification while when the target variables are real-valued the task is called multi-target regression. Although multi-label classification can be seen as a specific case of multitarget regression, the recent advances in this field motivate a study of whether newer stateof-the-art algorithms developed for multilabel classification are applicable and equally successful in the domain of multi-target regression. In this paper we introduce two novel algorithms for multi-target regression, multi-target regressor stacking and regressor chains, inspired by two popular and successful multi-label classification approaches. Furthermore, we develop an extension of the regressor chains algorithm which aims at improving its predictive performance and which is also applicable in the classification domain. All methods are empirically evaluated on 6 multi-target regression data sets, 4 of which are firstly introduced in this paper. The results of the evaluation show that all the proposed multi-target methods are able to improve the accuracy of the baseline approach which performs a separate regression for each target and that the corrected regressor chains method achieves the best overall accuracy.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/majerek/Zotero/storage/FTQX6LVX/Spyromitros-Xioufis et al. - 2016 - Multi-Target Regression via Input Space Expansion.pdf}
}


@article{evgeniou2004,
	title = {Regularized multi--task learning},
	author = {Evgeniou, Theodoros and Pontil, Massimiliano},
	year = {2004},
	month = {08},
	date = {2004-08-22},
	journal = {Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
	doi = {10.1145/1014052.1014067},
	url = {http://dx.doi.org/10.1145/1014052.1014067}
}

@inproceedings{tawiahEmpiricalComparisonMultilabel2013,
  title = {Empirical Comparison of Multi-Label Classification Algorithms},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Tawiah, Clifford and Sheng, Victor},
  year = {2013},
  volume = {27},
  pages = {1645--1646},
  isbn = {2374-3468}
}


@article{tsoumakasMultilabelClassificationOverview2007,
  title = {Multi-Label Classification: {{An}} Overview},
  author = {Tsoumakas, Grigorios and Katakis, Ioannis},
  year = {2007},
  journal = {International Journal of Data Warehousing and Mining (IJDWM)},
  volume = {3},
  number = {3},
  pages = {1--13},
  publisher = {{IGI Global}}
}


@article{borchaniSurveyMultioutputRegression2015,
  title = {A Survey on Multi-Output Regression},
  author = {Borchani, Hanen and Varando, Gherardo and Bielza, Concha and Larra{\~n}aga, Pedro},
  year = {2015},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {5},
  number = {5},
  pages = {216--233},
  issn = {1942-4795},
  doi = {10.1002/widm.1157},
  urldate = {2024-01-09},
  abstract = {In recent years, a plethora of approaches have been proposed to deal with the increasingly challenging task of multi-output regression. This study provides a survey on state-of-the-art multi-output regression methods, that are categorized as problem transformation and algorithm adaptation methods. In addition, we present the mostly used performance evaluation measures, publicly available data sets for multi-output regression real-world problems, as well as open-source software frameworks. WIREs Data Mining Knowl Discov 2015, 5:216{\textendash}233. doi: 10.1002/widm.1157 This article is categorized under: Technologies {$>$} Machine Learning},
  copyright = {{\textcopyright} 2015 John Wiley \& Sons, Ltd},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/KTH8XZ66/widm.html}
}


@article{santanaImprovedPredictionSoil2020,
  title = {Improved Prediction of Soil Properties with Multi-Target Stacked Generalisation on {{EDXRF}} Spectra},
  author = {Santana, Everton Jose and dos Santos, Felipe Rodrigues and Mastelini, Saulo Martiello and Melquiades, Fabio Luiz and Barbon Jr, Sylvio},
  year = {2020},
  journal = {arXiv preprint arXiv:2002.04312},
  eprint = {2002.04312},
  archiveprefix = {arxiv}
}


@article{melkiMultitargetSupportVector2017,
  title = {Multi-Target Support Vector Regression via Correlation Regressor Chains},
  author = {Melki, Gabriella and Cano, Alberto and Kecman, Vojislav and Ventura, Sebasti{\'a}n},
  year = {2017},
  month = nov,
  journal = {Information Sciences},
  volume = {415--416},
  pages = {53--69},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2017.06.017},
  urldate = {2024-01-09},
  abstract = {Multi-target regression is a challenging task that consists of creating predictive models for problems with multiple continuous target outputs. Despite the increasing attention on multi-label classification, there are fewer studies concerning multi-target (MT) regression. The current leading MT models are based on ensembles of regressor chains, where random, differently ordered chains of the target variables are created and used to build separate regression models, using the previous target predictions in the chain. The challenges of building MT models stem from trying to capture and exploit possible correlations among the target variables during training. This paper presents three multi-target support vector regression models. The first involves building independent, single-target Support Vector Regression (SVR) models for each output variable. The second builds an ensemble of random chains using the first method as a base model. The third calculates the targets' correlations and forms a maximum correlation chain, which is used to build a single chained support vector regression model, improving the models' prediction performance while reducing the computational complexity. The experimental study evaluates and compares the performance of the three approaches with seven other state-of-the-art multi-target regressors on 24 multi-target datasets. The experimental results are then analyzed using non-parametric statistical tests. The results show that the maximum correlation SVR approach improves the performance of using ensembles of random chains.},
  keywords = {Multi-output regression,Multi-target regression,Regressor chains,Support vector regressor}
}


@article{
  izenmanReducedrankRegressionMultivariate1975,
  title={Reduced-rank regression for the multivariate linear model},
  volume={5},
  ISSN={0047-259X},
  number={2},
  journal={Journal of multivariate analysis},
  publisher={Elsevier},
  author={Izenman, Alan Julian},
  year={1975},
pages={248–264} }

@article{
  izenmanReducedrankRegressionMultivariate1975,
  title={Reduced-rank regression for the multivariate linear model},
  volume={5},
  ISSN={0047-259X},
  number={2},
  journal={Journal of multivariate analysis},
  publisher={Elsevier},
  author={Izenman, Alan Julian},
  year={1975},
pages={248–264} }

@article{
  xuTwinMultiClassClassification2013,
  title={A Twin Multi-Class Classification Support Vector Machine},
  volume={5},
  ISSN={1866-9964},
  url={https://doi.org/10.1007/s12559-012-9179-7},
  DOI={10.1007/s12559-012-9179-7},
  number={4},
  journal={Cognitive Computation},
  author={Xu, Yitian and Guo, Rui and Wang, Laisheng},
  year={2013},
  month={Dec},
pages={580–588} }

@article{
  vazquezMultiOutputSuppportVector2003,
  series={13th IFAC Symposium on System Identification (SYSID 2003), Rotterdam, The Netherlands, 27-29 August, 2003},
  title={Multi-Output Suppport Vector Regression},
  volume={36},
  ISSN={1474-6670},
  url={https://www.sciencedirect.com/science/article/pii/S1474667017350188},
  DOI={10.1016/S1474-6670(17)35018-8},
  number={16},
  journal={IFAC Proceedings Volumes},
  author={Vazquez, Emmanuel and Walter, Eric},
  year={2003},
  month={Sep},
  pages={1783–1788},
collection={13th IFAC Symposium on System Identification (SYSID 2003), Rotterdam, The Netherlands, 27-29 August, 2003} }

@article{
  kocevTreeEnsemblesPredicting2013,
  title={Tree ensembles for predicting structured outputs},
  volume={46},
  ISSN={0031-3203},
  url={https://www.sciencedirect.com/science/article/pii/S003132031200430X},
  DOI={10.1016/j.patcog.2012.09.023},
  number={3},
  journal={Pattern Recognition},
  author={Kocev, Dragi and Vens, Celine and Struyf, Jan and Džeroski, Sašo},
  year={2013},
  month={Mar},
pages={817–833} }

@article{
  zhangGBDTMOGradientBoosted2019,
  title={GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputs},
  url={http://arxiv.org/abs/1909.04373},
  DOI={10.48550/arXiv.1909.04373},
author={Zhang, Zhendong and Jung, Cheolkon} }

@inproceedings{
  struyfConstraintBasedInduction2006,
  place={Berlin, Heidelberg},
  series={Lecture Notes in Computer Science},
  title={Constraint Based Induction of Multi-objective Regression Trees},
  ISBN={978-3-540-33293-0},
  DOI={10.1007/11733492_13},
  booktitle={Knowledge Discovery in Inductive Databases},
  publisher={Springer},
  author={Struyf, Jan and Džeroski, Sašo},
  editor={Bonchi, Francesco and Boulicaut, Jean-François},
  year={2006},
  pages={222–233},
collection={Lecture Notes in Computer Science} }

@inproceedings{
  faddoulBoostingMultiTaskWeak2010,
  place={Washington, DC, USA},
  title={Boosting Multi-Task Weak Learners with Applications to Textual and Social Data},
  ISBN={978-1-4244-9211-4},
  url={http://ieeexplore.ieee.org/document/5708858/},
  DOI={10.1109/ICMLA.2010.61},
  booktitle={2010 Ninth International Conference on Machine Learning and Applications},
  publisher={IEEE},
  author={Faddoul, Jean Baptiste and Chidlovskii, Boris and Torre, Fabien and Gilleron, Remi},
  year={2010},
  month={Dec},
pages={367–372} }

@inproceedings{
  faddoulLearningMultipleTasks2012,
  place={Berlin, Heidelberg},
  series={ECMLPKDD’12},
  title={Learning multiple tasks with boosted decision trees},
  ISBN={978-3-642-33459-7},
  booktitle={Proceedings of the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I},
  publisher={Springer-Verlag},
  author={Faddoul, Jean Baptiste and Chidlovskii, Boris and Gilleron, Rémi and Torre, Fabien},
  year={2012},
  month={Sep},
  pages={681–696},
collection={ECMLPKDD’12} }

@book{
  quinlan1993,
  title={C4. 5: Programs for Machine Learning},
  note={Citation Key: quinlan1993},
  publisher={Morgan Kaufmann},
  author={Quinlan, J Ross},
year={1993} }

@article{
  segalTreeStructuredMethodsLongitudinal1992,
  title={Tree-Structured Methods for Longitudinal Data},
  volume={87},
  ISSN={0162-1459},
  url={https://www.jstor.org/stable/2290271},
  DOI={10.2307/2290271},
  number={418},
  journal={Journal of the American Statistical Association},
  publisher={[American},
  author={Segal, Mark Robert},
  year={1992},
pages={407–418} }

@book{
  breiman2017,
  title={Classification and Regression Trees.},
  ISBN={978-0-412-04841-8},
  url={http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site},
  note={Citation Key: breiman2017},
  publisher={Routledge},
  author={Breiman, Leo and Friedman, J. H. and Olshen, Richard A. and Stone, Charles J.},
year={2017} }

@inbook{
  glockerJointClassificationRegressionForests2012,
  place={Berlin, Heidelberg},
  title={Joint Classification-Regression Forests for Spatially Structured Multi-object Segmentation},
  volume={7575},
  ISBN={978-3-642-33764-2 978-3-642-33765-9},
  url={http://link.springer.com/10.1007/978-3-642-33765-9_62},
  note={Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-642-33765-9_62},
  booktitle={Computer Vision – ECCV 2012},
  publisher={Springer Berlin Heidelberg},
  author={Glocker, Ben and Pauly, Olivier and Konukoglu, Ender and Criminisi, Antonio},
  editor={Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  year={2012},
pages={870–881} }

@article{knuth84,
  author = {Knuth, Donald E.},
  title = {Literate Programming},
  year = {1984},
  issue_date = {May 1984},
  publisher = {Oxford University Press, Inc.},
  address = {USA},
  volume = {27},
  number = {2},
  issn = {0010-4620},
  url = {https://doi.org/10.1093/comjnl/27.2.97},
  doi = {10.1093/comjnl/27.2.97},
  journal = {Comput. J.},
  month = may,
  pages = {97–111},
  numpages = {15}
}



@book{burgerDigitalImageProcessing2016,
  title = {Digital {{Image Processing}}: {{An Algorithmic Introduction Using Java}}},
  shorttitle = {Digital {{Image Processing}}},
  author = {Burger, Wilhelm and Burge, Mark J.},
  year = {2016},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer}},
  address = {{London}},
  doi = {10.1007/978-1-4471-6684-9},
  isbn = {978-1-4471-6683-2 978-1-4471-6684-9},
  langid = {english},
  keywords = {Digital Image Processing,Discrete Fourier Transform,GSM - Postponed Project,Image Processing,Image Processing System,ImageJ,Java,Medical Imaging}
}


@book{szeliskiComputerVisionAlgorithms2022,
  title = {Computer {{Vision}}: {{Algorithms}} and {{Applications}}},
  shorttitle = {Computer {{Vision}}},
  author = {Szeliski, Richard},
  year = {2022},
  series = {Texts in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-34372-9},
  isbn = {978-3-030-34371-2 978-3-030-34372-9},
  langid = {english},
  keywords = {3D Reconstruction,Computational Photography,Computer Vision,Deep Learning,Feature Detection and Matching,Image Processing,Image Segmentation,Image Stitching,Image-Based Rendering,Motion Estimation,Scene Recognition,Structure from Motion}
}


@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  month = nov,
  edition = {Illustrated edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  isbn = {978-0-262-03561-3},
  langid = {english}
}


@book{ketkarDeepLearningPython2017,
  title = {Deep Learning with {{Python}}},
  author = {Ketkar, Nikhil and Santana, Eder},
  year = {2017},
  volume = {1},
  publisher = {{Springer}}
}


@misc{DeepLearningPython,
  title = {Deep {{Learning}} with {{Python}}, {{Second Edition}}},
  journal = {Manning Publications},
  abstract = {In this extensively revised new edition of the bestselling original, Keras creator offers insights for both novice and experienced machine learning practitioners.},
  howpublished = {https://www.manning.com/books/deep-learning-with-python-second-edition},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/NEFSKNHG/deep-learning-with-python-second-edition.html}
}


@book{cholletDeepLearning2018,
  title = {Deep {{Learning}} with {{R}}},
  author = {Chollet, Francois and Allaire, J. J.},
  year = {2018},
  month = feb,
  publisher = {{Manning Publications}},
  abstract = {SummaryDeep Learning with R introduces the world of deep learning using the powerful Keras library and its R language interface. The book builds your understanding of deep learning through intuitive explanations and practical examples. Continue your journey into the world of deep learning with Deep Learning with R in Motion, a practical, hands-on video course available exclusively at Manning.com (www.manning.com/livevideo/deep-\hspace{0pt}learning-with-r-in-motion).Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications.About the TechnologyMachine learning has made remarkable progress in recent years. Deep-learning systems now enable previously impossible smart applications, revolutionizing image recognition and natural-language processing, and identifying complex patterns in data. The Keras deep-learning library provides data scientists and developers working in R a state-of-the-art toolset for tackling deep-learning tasks.About the BookDeep Learning with R introduces the world of deep learning using the powerful Keras library and its R language interface. Initially written for Python as Deep Learning with Python by Keras creator and Google AI researcher Fran\c{c}ois Chollet and adapted for R by RStudio founder J. J. Allaire, this book builds your understanding of deep learning through intuitive explanations and practical examples. You'll practice your new skills with R-based applications in computer vision, natural-language processing, and generative models.What's InsideDeep learning from first principlesSetting up your own deep-learning environmentImage classification and generationDeep learning for text and sequencesAbout the ReaderYou'll need intermediate R programming skills. No previous experience with machine learning or deep learning is assumed.About the AuthorsFran\c{c}ois Chollet is a deep-learning researcher at Google and the author of the Keras library.J.J. Allaire is the founder of RStudio and the author of the R interfaces to TensorFlow and Keras.Table of ContentsPART 1 - FUNDAMENTALS OF DEEP LEARNINGWhat is deep learning?Before we begin: the mathematical building blocks of neural networksGetting started with neural networksFundamentals of machine learningPART 2 - DEEP LEARNING IN PRACTICEDeep learning for computer visionDeep learning for text and sequencesAdvanced deep-learning best practicesGenerative deep learningConclusions},
  googlebooks = {xnIRtAEACAAJ},
  isbn = {978-1-61729-554-6},
  langid = {english},
  keywords = {Computers / Intelligence (AI) \& Semantics,Computers / Machine Theory,Computers / Natural Language Processing,Computers / Neural Networks}
}


@article{mindas2007,
	title = {Mind as machine: a history of cognitive science},
	year = {2007},
	month = {07},
	date = {2007-07-01},
	journal = {Choice Reviews Online},
	volume = {44},
	number = {11},
	doi = {10.5860/choice.44-6202},
	url = {http://dx.doi.org/10.5860/choice.44-6202},
	langid = {en}
}

@article{winston1976,
	title = {The psychology of computer vision},
	author = {Winston, Patrick Henry},
	year = {1976},
	month = {07},
	date = {1976-07},
	journal = {Pattern Recognition},
	pages = {193},
	volume = {8},
	number = {3},
	doi = {10.1016/0031-3203(76)90020-0},
	url = {http://dx.doi.org/10.1016/0031-3203(76)90020-0},
	langid = {en}
}

@book{hansonComputerVisionSystems1978,
  title = {Computer {{Vision Systems}}},
  author = {Hanson, Allen},
  year = {1978},
  month = jan,
  publisher = {{Elsevier}},
  abstract = {Computer Vision Systems is a collection of papers presented at the Workshop on Computer Vision Systems held at the University of Massachusetts in Amherst, Massachusetts, on June 1-3, 1977. Contributors discuss the breadth of problems that must be taken into account in the development of general computer vision systems. Topics covered include the application of system engineering techniques to the design of artificial intelligence systems; representation and segmentation of natural scenes; and pragmatic aspects of machine vision. Psychophysical measures of representation and interpretation are also considered. This monograph is divided into four sections: Issues and Research Strategies, Segmentation, Theory and Psychology, and Systems. The first chapter explores the problem of recovering the intrinsic characteristics of scenes from images, along with its implications for machine and human vision. The discussion then turns to special-purpose low-level vision systems that can be flexibly reconfigured as the need arises; design, development, and implementation of large systems from the human engineering point of view; and representation of visual information. The next section examines hierarchical relaxation for waveform parsing; the topology and semantics of intensity arrays; and visual images as spatial representations in active memory. The use of edge cues to recognize real-world objects is also analyzed. This text will be a useful resource for systems designers, computer engineers, and scientists as well as psychologists.},
  googlebooks = {gMGkDLLhnFUC},
  isbn = {978-0-323-15120-7},
  langid = {english},
  keywords = {Technology \& Engineering / Materials Science / General}
}


@book{robertsMachinePerceptionThreedimensional1980,
  title = {Machine {{Perception}} of {{Three-dimensional Solids}}},
  author = {Roberts, Lawrence G.},
  year = {1980},
  publisher = {{Garland Pub.}},
  googlebooks = {THWwAAAAIAAJ},
  isbn = {978-0-8240-4427-5},
  langid = {english}
}


@article{barrowComputationalVision1981,
  title = {Computational Vision},
  author = {Barrow, H.G. and Tenenbaum, J.M.},
  year = {1981},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {69},
  number = {5},
  pages = {572--595},
  issn = {1558-2256},
  doi = {10.1109/PROC.1981.12026},
  abstract = {Research is beginning to uncover fundamental computational principles underlying vision that apply equally to artificial and natural systems. These principles provide insights into the limitations of early machine vision systems and lay a foundation for building future systems capable of high performance in a broad range of visual domains. We present this emerging computational view of visual perception, discuss some early work in the field in its context, and put forward current thoughts on the overall organization and operation of a general-purpose computer vision system, synthesizing recent theoretical and experimental results.},
  keywords = {Computer graphics,Computer vision,Data structures,Image processing,Image segmentation,Layout,Machine vision,Pattern analysis,Pattern recognition,Velocity measurement},
  file = {/Users/majerek/Zotero/storage/HYP3R5IS/1456294.html}
}


@article{devPerceptionDepthSurfaces1975,
  title = {Perception of Depth Surfaces in Random-Dot Stereograms : A Neural Model},
  shorttitle = {Perception of Depth Surfaces in Random-Dot Stereograms},
  author = {Dev, Parvati},
  year = {1975},
  month = jul,
  journal = {International Journal of Man-Machine Studies},
  volume = {7},
  number = {4},
  pages = {511--528},
  issn = {0020-7373},
  doi = {10.1016/S0020-7373(75)80030-7},
  abstract = {A model has been presented of a neural process that segments the visual field into spatially disjoint regions, each region characterized by a specific feature such as a texture or color. The neural connectivity hypothesized to be necessary for the segmentation process has been formulated in mathematical terms and the corresponding neural network has been simulated on the digital computer. The properties of the network that result from the postulated patterns of excitatory and inhibitory connectivity have been investigated. It is shown that the required connectivity is that of excitatory connections only between neurons detecting similar features and inhibitory connections between all feature-detecting neurons. The resulting segmentation model is used to model the phenomenon of stereopsis as investigated through the use of random-dot stereograms. The process of depth perception through stereopsis can be viewed as a segmentation process with each segment, that is, each surface at a specific depth, characterized by a specific retinal disparity. It is shown that the segmentation model suffices to detect the different depth surfaces embedded in the random-dot patterns.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/GM96NCK2/S0020737375800307.html}
}


@misc{CooperativeComputationStereo,
  title = {Cooperative {{Computation}} of {{Stereo Disparity}} | {{Science}}},
  howpublished = {https://www.science.org/doi/10.1126/science.968482},
  file = {/Users/majerek/Zotero/storage/X764KYLU/science.html}
}


@article{barnardComputationalStereo1982,
  title = {Computational {{Stereo}}},
  author = {Barnard, Stephen T. and Fischler, Martin A.},
  year = {1982},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {14},
  number = {4},
  pages = {553--572},
  issn = {0360-0300},
  doi = {10.1145/356893.356896}
}


@article{hornObtainingShapeShading,
  title = {Obtaining {{Shape}} from {{Shading Information}}},
  author = {Horn, Berthold K P},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/AS2K9QDS/Horn - Obtaining Shape from Shading Information.pdf}
}


@article{blakeSurfaceDescriptionsStereo1985,
  title = {Surface Descriptions from Stereo and Shading},
  author = {Blake, Andrew and Zisserman, Andrew and Knowles, Greg},
  year = {1985},
  month = nov,
  journal = {Image and Vision Computing},
  series = {Papers from the 1985 {{Alvey Computer Vision}} and {{Image Interpretation Meeting}}},
  volume = {3},
  number = {4},
  pages = {183--191},
  issn = {0262-8856},
  doi = {10.1016/0262-8856(85)90006-X},
  abstract = {Surface reconstruction from stereo and shape-from-shading have both been discussed extensively in the literature. In this paper methods that attempt to supplement stereo with analysis of shading, are reviewed, with comments on their robustness. Theoretical results on uniqueness in shape from shading are presented, and on the difficulty of estimating surface shape with local intensity operators. Preliminary experiments based on the method of Koenderinck and van Doorn, for analysing variation of shading with viewpoint, are reported. It is concluded that there is a variety of techniques for deriving qualitative shape information, which might be sufficiently robust for use in computer vision systems.},
  langid = {english},
  keywords = {shading,stereo vision,surface shape},
  file = {/Users/majerek/Zotero/storage/XS3G7XJQ/026288568590006X.html}
}


@article{cannyComputationalApproachEdge1986,
  title = {A {{Computational Approach}} to {{Edge Detection}}},
  author = {Canny, John},
  year = {1986},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-8},
  number = {6},
  pages = {679--698},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1986.4767851},
  abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
  keywords = {Detectors,Edge detection,feature extraction,Feature extraction,Gaussian approximation,Image edge detection,image processing,machine vision,Machine vision,multiscale image analysis,Performance analysis,Shape measurement,Signal synthesis,Signal to noise ratio,Uncertainty},
  file = {/Users/majerek/Zotero/storage/YU2A25YY/4767851.html}
}


@article{nalwaDetectingEdges1986,
  title = {On {{Detecting Edges}}},
  author = {Nalwa, Vishvjit S. and Binford, Thomas O.},
  year = {1986},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-8},
  number = {6},
  pages = {699--714},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1986.4767852},
  abstract = {An edge in an image corresponds to a discontinuity in the intensity surface of the underlying scene. It can be approximated by a piecewise straight curve composed of edgels, i.e., short, linear edge-elements, each characterized by a direction and a position. The approach to edgel-detection here, is to fit a series of one-dimensional surfaces to each window (kernel of the operator) and accept the surface-description which is adequate in the least squares sense and has the fewest parameters. (A one-dimensional surface is one which is constant along some direction.) The tanh is an adequate basis for the stepedge and its combinations are adequate for the roofedge and the line-edge. The proposed method of step-edgel detection is robust with respect to noise; for (step-size/{$\sigma$}noise) {$\geq$} 2.5, it has subpixel position localization ({$\sigma$}position {$<$} {$\frac{1}{3}$}) and an angular localization better than 10\textdegree; further, it is designed to be insensitive to smooth shading. These results are demonstrated by some simple analysis, statistical data, and edgelimages. Also included is a comparison of performance on a real image, with a typical operator (Difference-of-Gaussians). The results indicate that the proposed operator is superior with respect to detection, localization, and resolution.},
  keywords = {Adequate basis for step-edges,Artificial intelligence,digital image processing,directional edge operator,edge-detection,Image edge detection,image segmentation,Information systems,Kernel,Laboratories,Layout,Least squares approximation,Least squares methods,Noise robustness,one-dimensional surface-fitting,subpixel edge localization,Surface fitting},
  file = {/Users/majerek/Zotero/storage/L9CFYNLY/4767852.html}
}


@article{woodhamAnalysingImagesCurved1981,
  title = {Analysing Images of Curved Surfaces},
  author = {Woodham, Robert J.},
  year = {1981},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {17},
  number = {1},
  pages = {117--140},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(81)90022-9},
  abstract = {A reflectance map makes the relationship between image intensity and surface orientation explicit. Trade-offs between image intensity and surface orientation emerge which cannot be resolved locally in a single view. Existing methods for determining surface orientation from a single view embody assumptions about surface curvature. The Hessian matrix is introduced to represent surface curvature. Properties of surface curvature are expressed as properties of the Hessian matrix. For several classes of surface, image analysis simplifies. This result has already been established for planar surfaces forming trihedral corners. Similar simplification is demonstrated for developable surfaces and for the subclass of surfaces known as generalized cones. These studies help to delineate shape information that can be determined from geometric measurements at object boundaries and shape information that can be determined from intensity measurements over sections of smoothly curved surface. A novel technique called photometric stereo is discussed. The idea of stereo is to obtain multiple images in order to determine the underlying scene precisely. In photometric stereo, the viewing direction is constant. Multiple images are obtained by varying the incident illumination. It is shown that this provides sufficient information to determine surface orientation at each image point.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/DX5LS72A/0004370281900229.html}
}


@article{witkinRecoveringSurfaceShape1981,
  title = {Recovering Surface Shape and Orientation from Texture},
  author = {Witkin, Andrew P.},
  year = {1981},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {17},
  number = {1},
  pages = {17--45},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(81)90019-9},
  abstract = {Texture provides an important source of information about the three-dimensional structure of visible surfaces, particularly for stationary monocular views. To recover 3d structure, the distorting effects of projection must be distinguished from properties of the texture on which the distortion acts. This requires that assumptions must be made about the texture, yet the unpredictability of natural textures precludes the use of highly restrictive assumptions. The recovery method reported in this paper exploits the minimal assumption that textures do not mimic projective effects. This assumption determines the strategy of attributing as much as possible of the variation observed in the image to projection. Equivalently, the interpretation is chosen for which the texture, prior to projection, is made as uniform as possible. This strategy was implemented using statistical methods, first for the restricted case of planar surfaces and then, by extension, for curved surfaces. The technique was applied successfully to natural images.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/E2MGG63Y/0004370281900199.html}
}


@inproceedings{fischlerReadingsComputerVision1987,
  title = {Readings in Computer Vision: Issues, Problems, Principles, and Paradigms},
  shorttitle = {Readings in Computer Vision},
  author = {Fischler, M. and Firschein, O.},
  year = {1987},
  abstract = {Each chapter of the book addresses a problem, provides a survey of major issues, ideas, and research projects, and presents reprints of key papers. In total, the book presents sixty research papers, most written since 1980.}
}


@book{mundyGeometricInvarianceComputer1992,
  title = {Geometric Invariance in Computer Vision},
  editor = {Mundy, Joseph L. and Zisserman, Andrew},
  year = {1992},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  isbn = {978-0-262-13285-5}
}


@article{kassSnakesActiveContour1988,
  title = {Snakes: {{Active}} Contour Models},
  shorttitle = {Snakes},
  author = {Kass, Michael and Witkin, Andrew and Terzopoulos, Demetri},
  year = {1988},
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {1},
  number = {4},
  pages = {321--331},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/BF00133570},
  abstract = {A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the capture region surrounding a feature. Snakes provide a unified account of a number of visual problems, including detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/F45K5BPG/Kass et al. - 1988 - Snakes Active contour models.pdf}
}


@book{blakeActiveContoursApplication2012,
  title = {Active {{Contours}}: {{The Application}} of {{Techniques}} from {{Graphics}}, {{Vision}}, {{Control Theory}} and {{Statistics}} to {{Visual Tracking}} of {{Shapes}} in {{Motion}}},
  shorttitle = {Active {{Contours}}},
  author = {Blake, Andrew and Isard, Michael},
  year = {2012},
  month = dec,
  publisher = {{Springer Science \& Business Media}},
  abstract = {Active Contours deals with the analysis of moving images - a topic of growing importance within the computer graphics industry. In particular it is concerned with understanding, specifying and learning prior models of varying strength and applying them to dynamic contours. Its aim is to develop and analyse these modelling tools in depth and within a consistent framework.},
  googlebooks = {yZPxBwAAQBAJ},
  isbn = {978-1-4471-1555-7},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Optical Data Processing,Computers / Software Development \& Engineering / Computer Graphics,Computers / Software Development \& Engineering / General}
}


@article{malladiShapeModelingFront1995,
  title = {Shape Modeling with Front Propagation: A Level Set Approach},
  shorttitle = {Shape Modeling with Front Propagation},
  author = {Malladi, R. and Sethian, J.A. and Vemuri, B.C.},
  year = {1995},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {17},
  number = {2},
  pages = {158--175},
  issn = {1939-3539},
  doi = {10.1109/34.368173},
  abstract = {Shape modeling is an important constituent of computer vision as well as computer graphics research. Shape models aid the tasks of object representation and recognition. This paper presents a new approach to shape modeling which retains some of the attractive features of existing methods and overcomes some of their limitations. The authors' techniques can be applied to model arbitrarily complex shapes, which include shapes with significant protrusions, and to situations where no a priori assumption about the object's topology is made. A single instance of the authors' model, when presented with an image having more than one object of interest, has the ability to split freely to represent each object. This method is based on the ideas developed by Osher and Sethian (1988) to model propagating solid/liquid interfaces with curvature-dependent speeds. The interface (front) is a closed, nonintersecting, hypersurface flowing along its gradient field with constant speed or a speed that depends on the curvature. It is moved by solving a "Hamilton-Jacobi" type equation written for a function in which the interface is a particular level set. A speed term synthesized from the image is used to stop the interface in the vicinity of object boundaries. The resulting equation of motion is solved by employing entropy-satisfying upwind finite difference schemes. The authors present a variety of ways of computing the evolving front, including narrow bands, reinitializations, and different stopping criteria. The efficacy of the scheme is demonstrated with numerical experiments on some synthesized images and some low contrast medical images.{$<>$}},
  keywords = {Biomedical imaging,Computer graphics,Computer vision,Difference equations,Finite difference methods,Level set,Narrowband,Shape,Solid modeling,Topology},
  file = {/Users/majerek/Zotero/storage/S69PW2RV/368173.html}
}


@book{ponceCategoryLevelObjectRecognition2007,
  title = {Toward {{Category-Level Object Recognition}}},
  author = {Ponce, Jean and Hebert, Martial and Schmid, Cordelia and Zisserman, Andrew},
  year = {2007},
  month = jan,
  publisher = {{Springer}},
  abstract = {Although research in computer vision for recognizing 3D objects in photographs dates back to the 1960s, progress was relatively slow until the turn of the millennium, and only now do we see the emergence of effective techniques for recognizing object categories with different appearances under large variations in the observation conditions. Tremendous progress has been achieved in the past five years, thanks largely to the integration of new data representations, such as invariant semi-local features, developed in the computer vision community with the effective models of data distribution and classification procedures developed in the statistical machine-learning community. This volume is a post-event proceedings volume and contains selected papers based on presentations given, and vivid discussions held, during two workshops held in Taormina in 2003 and 2004. The main goals of these two workshops were to promote the creation of an international object recognition community, with common datasets and evaluation procedures, to map the state of the art and identify the main open problems and opportunities for synergistic research, and to articulate the industrial and societal needs and opportunities for object recognition research worldwide. The 30 thoroughly revised papers presented are organized in the following topical sections: recognition of specific objects, recognition of object categories, recognition of object categories with geometric relations, and joint recognition and segmentation.},
  googlebooks = {vXQKBwAAQBAJ},
  isbn = {978-3-540-68795-5},
  langid = {english},
  keywords = {Computers / Artificial Intelligence / Computer Vision \& Pattern Recognition,Computers / Artificial Intelligence / General,Computers / Optical Data Processing,Computers / Programming / Algorithms,Computers / Software Development \& Engineering / Computer Graphics,Computers / Software Development \& Engineering / General}
}


@article{fergusWeaklySupervisedScaleInvariant2007,
  title = {Weakly {{Supervised Scale-Invariant Learning}} of {{Models}} for {{Visual Recognition}}},
  author = {Fergus, R. and Perona, P. and Zisserman, A.},
  year = {2007},
  month = mar,
  journal = {International Journal of Computer Vision},
  volume = {71},
  number = {3},
  pages = {273--303},
  issn = {1573-1405},
  doi = {10.1007/s11263-006-8707-x},
  abstract = {We investigate a method for learning object categories in a weakly supervised manner. Given a set of images known to contain the target category from a similar viewpoint, learning is translation and scale-invariant; does not require alignment or correspondence between the training images, and is robust to clutter and occlusion. Category models are probabilistic constellations of parts, and their parameters are estimated by maximizing the likelihood of the training data. The appearance of the parts, as well as their mutual position, relative scale and probability of detection are explicitly described in the model. Recognition takes place in two stages. First, a feature-finder identifies promising locations for the model''s parts. Second, the category model is used to compare the likelihood that the observed features are generated by the category model, or are generated by background clutter. The flexible nature of the model is demonstrated by results over six diverse object categories including geometrically constrained categories (e.g. faces, cars) and flexible objects (such as animals).},
  langid = {english},
  keywords = {constellation model,object recognition,parts and structure model,semi-supervised learning}
}


@article{felzenszwalbPictorialStructuresObject2005,
  title = {Pictorial {{Structures}} for {{Object Recognition}}},
  author = {Felzenszwalb, Pedro F. and Huttenlocher, Daniel P.},
  year = {2005},
  month = jan,
  journal = {International Journal of Computer Vision},
  volume = {61},
  number = {1},
  pages = {55--79},
  issn = {1573-1405},
  doi = {10.1023/B:VISI.0000042934.15159.49},
  abstract = {In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.},
  langid = {english},
  keywords = {energy minimization,part-based object recognition,statistical models}
}


@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computer science,Mathematics and computing}
}


@misc{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  number = {arXiv:1505.04597},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1505.04597},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/majerek/Zotero/storage/MIZG3DGR/1505.html}
}


@misc{zhouUNetNestedUNet2018,
  title = {{{UNet}}++: {{A Nested U-Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  year = {2018},
  month = jul,
  number = {arXiv:1807.10165},
  eprint = {1807.10165},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.10165},
  abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/Users/majerek/Zotero/storage/AQVTJVXU/1807.html}
}


@misc{PictureProcessingPsychopictorics,
  title = {Picture {{Processing}} and {{Psychopictorics}} - 1st {{Edition}}},
  howpublished = {https://www.elsevier.com/books/picture-processing-and-psychopictorics/lipkin/978-0-12-451550-5},
  file = {/Users/majerek/Zotero/storage/GZENQHXM/978-0-12-451550-5.html}
}


@article{davisSurveyEdgeDetection1975,
  title = {A Survey of Edge Detection Techniques},
  author = {Davis, Larry S.},
  year = {1975},
  month = sep,
  journal = {Computer Graphics and Image Processing},
  volume = {4},
  number = {3},
  pages = {248--270},
  issn = {0146-664X},
  doi = {10.1016/0146-664X(75)90012-X},
  abstract = {Methods of detecting ``edges,'' i.e., boundaries between regions in a picture, are reviewed. Included are both parallel (linear, nonlinear, optimal) and sequential methods, as well as methods using planning or a priori knowledge.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/5B79D7KY/0146664X7590012X.html}
}


@misc{OpticalElectroOpticalInformation,
  title = {Optical and {{Electro-Optical Information Processing}}},
  journal = {MIT Press},
  abstract = {This book comprises the Proceedings of a Symposium on Optical and Electro-Optical Information Processing Technology held in Boston. The aims of the Symposium...},
  langid = {american},
  file = {/Users/majerek/Zotero/storage/KMSDZKAY/optical-and-electro-optical-information-processing.html}
}


@article{kirschComputerDeterminationConstituent1971,
  title = {Computer Determination of the Constituent Structure of Biological Images},
  author = {Kirsch, Russell A.},
  year = {1971},
  month = jun,
  journal = {Computers and Biomedical Research},
  volume = {4},
  number = {3},
  pages = {315--328},
  issn = {0010-4809},
  doi = {10.1016/0010-4809(71)90034-6},
  abstract = {A class of algorithms is described which enables computer quantized images to be decomposed into constituent reflecting the structure of the images. This decomposition is viewed as the morphological precursor to a higher level syntactic analysis. Numerical results for a typical biological image are presented.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/IQIS2QRE/0010480971900346.html}
}


@article{marrTheoryEdgeDetection1980,
  title = {Theory of Edge Detection},
  author = {Marr, D. and Hildreth, E.},
  year = {1980},
  month = feb,
  journal = {Proceedings of the Royal Society of London. Series B, Biological Sciences},
  volume = {207},
  number = {1167},
  pages = {187--217},
  issn = {0950-1193},
  doi = {10.1098/rspb.1980.0020},
  abstract = {A theory of edge detection is presented. The analysis proceeds in two parts. (1) Intensity changes, which occur in a natural image over a wide range of scales, are detected separately at different scales. An appropriate filter for this purpose at a given scale is found to be the second derivative of a Gaussian, and it is shown that, provided some simple conditions are satisfied, these primary filters need not be orientation-dependent. Thus, intensity changes at a given scale are best detected by finding the zero values of delta 2G(x,y)*I(x,y) for image I, where G(x,y) is a two-dimensional Gaussian distribution and delta 2 is the Laplacian. The intensity changes thus discovered in each of the channels are then represented by oriented primitives called zero-crossing segments, and evidence is given that this representation is complete. (2) Intensity changes in images arise from surface discontinuities or from reflectance or illumination boundaries, and these all have the property that they are spatially. Because of this, the zero-crossing segments from the different channels are not independent, and rules are deduced for combining them into a description of the image. This description is called the raw primal sketch. The theory explains several basic psychophysical findings, and the operation of forming oriented zero-crossing segments from the output of centre-surround delta 2G filters acting on the image forms the basis for a physiological model of simple cells (see Marr \& Ullman 1979).},
  langid = {english},
  pmid = {6102765},
  keywords = {Animals,Form Perception,Humans,Mathematics,Vision; Ocular}
}


@article{zhang1984,
	title = {A fast parallel algorithm for thinning digital patterns},
	author = {Zhang, T. Y. and Suen, C. Y.},
	year = {1984},
	month = {03},
	date = {1984-03},
	journal = {Communications of the ACM},
	pages = {236--239},
	volume = {27},
	number = {3},
	doi = {10.1145/357994.358023},
	url = {http://dx.doi.org/10.1145/357994.358023},
	langid = {en}
}

@article{ciresanFlexibleHighPerformance,
  title = {Flexible, {{High Performance Convolutional Neural Networks}} for {{Image Classification}}},
  author = {Ciresan, Dan C and Meier, Ueli and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, Jurgen},
  abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53\%, 19.51\%, 0.35\%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42\%, 0.97\% and 0.48\% after 1, 3 and 17 epochs, respectively.},
  langid = {english},
  file = {/Users/majerek/Zotero/storage/SHX6DMQX/Ciresan et al. - Flexible, High Performance Convolutional Neural Ne.pdf}
}


@article{krizhevsky2017,
	title = {ImageNet classification with deep convolutional neural networks},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2017},
	month = {05},
	date = {2017-05-24},
	journal = {Communications of the ACM},
	pages = {84--90},
	volume = {60},
	number = {6},
	doi = {10.1145/3065386},
	url = {http://dx.doi.org/10.1145/3065386},
	langid = {en}
}

@article{Simonyan2014,
	title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = {2014},
	date = {2014},
	doi = {10.48550/ARXIV.1409.1556},
	url = {https://arxiv.org/abs/1409.1556}
}

@inbook{bengio,
	title = {Neural Probabilistic Language Models},
	author = {Bengio, Yoshua and Schwenk, Holger and {Senécal}, {Jean-Sébastien} and Morin, {Fréderic} and Gauvain, Jean-Luc},
	publisher = {Springer-Verlag},
	pages = {137--186},
	doi = {10.1007/3-540-33486-6_6},
	url = {http://dx.doi.org/10.1007/3-540-33486-6_6}
}

@article{bengio1994,
	title = {Learning long-term dependencies with gradient descent is difficult},
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	year = {1994},
	month = {03},
	date = {1994-03},
	journal = {IEEE Transactions on Neural Networks},
	pages = {157--166},
	volume = {5},
	number = {2},
	doi = {10.1109/72.279181},
	url = {http://dx.doi.org/10.1109/72.279181}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2023-03-16},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}
}


@misc{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  number = {arXiv:1512.03385},
  eprint = {arXiv:1512.03385},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1512.03385},
  urldate = {2023-03-18},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/majerek/Zotero/storage/PEPHQGNZ/1512.html}
}


@misc{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  number = {arXiv:1502.03167},
  eprint = {arXiv:1502.03167},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1502.03167},
  urldate = {2023-03-18},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/majerek/Zotero/storage/65ENDYR6/1502.html}
}


@article{selvarajuGradCAMVisualExplanations2020,
  title = {Grad-{{CAM}}: {{Visual Explanations}} from {{Deep Networks}} via {{Gradient-based Localization}}},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  year = {2020},
  month = feb,
  journal = {International Journal of Computer Vision},
  volume = {128},
  number = {2},
  eprint = {1610.02391},
  primaryclass = {cs},
  pages = {336--359},
  issn = {0920-5691, 1573-1405},
  doi = {10.1007/s11263-019-01228-7},
  urldate = {2023-03-18},
  abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/majerek/Zotero/storage/UPI6RGPW/1610.html}
}


@article{imager,
	title = {imager: Image Processing Library Based on 'CImg'},
	author = {Barthelme, Simon},
	year = {2023},
	date = {2023},
	url = {https://CRAN.R-project.org/package=imager}
}

@article{imagerExtra,
	title = {imagerExtra: Extra Image Processing Library Based on 'imager'},
	author = {Ochi, Shota},
	year = {2019},
	date = {2019},
	url = {https://CRAN.R-project.org/package=imagerExtra}
}

@article{magick,
	title = {magick: Advanced Graphics and Image-Processing in R},
	author = {Ooms, Jeroen},
	year = {2023},
	date = {2023},
	url = {https://CRAN.R-project.org/package=magick}
}

@article{imageseg,
	title = {imageseg: Deep Learning Models for Image Segmentation},
	author = {Niedballa, Juergen and Axtner, Jan},
	year = {2022},
	date = {2022},
	url = {https://CRAN.R-project.org/package=imageseg}
}

@article{pliman,
	title = {pliman: Tools for Plant Image Analysis},
	author = {Olivoto, Tiago},
	year = {2021},
	date = {2021},
	url = {https://CRAN.R-project.org/package=pliman}
}

@article{Rvision,
	title = {{\textbraceleft}Rvision{\textbraceright} - A computer vision library for R},
	author = {Garnier, Simon and Muschelli, John},
	year = {2022},
	date = {2022},
	url = {https://swarm-lab.github.io/Rvision/}
}

@article{
  choPropertiesNeuralMachine2014,
  title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
  url={http://arxiv.org/abs/1409.1259},
  DOI={10.48550/arXiv.1409.1259},
author={Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua} }

@article{
  bengioNeuralProbabilisticLanguage,
  title={A Neural Probabilistic Language Model},
author={Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian} }

@article{
  vaswaniAttentionAllYou2023,
  title={Attention Is All You Need},
  url={http://arxiv.org/abs/1706.03762},
  DOI={10.48550/arXiv.1706.03762},
author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia} }


@book{
  xenakis1963musiques,
  series={Revue musicale},
  title={Musiques formelles: nouveaux principes formels de composition musicale},
  url={https://books.google.pl/books?id=xfdqswEACAAJ},
  note={Citation Key: xenakis1963musiques tex.lccn: 00522528},
  publisher={Richard-Masse},
  author={Xenakis, I.},
  year={1963},
collection={Revue musicale} }

@article{
  gravesGeneratingSequencesRecurrent2014,
  title={Generating Sequences With Recurrent Neural Networks},
  url={http://arxiv.org/abs/1308.0850},
  DOI={10.48550/arXiv.1308.0850},
author={Graves, Alex} }

@article{
  gravesGeneratingSequencesRecurrent2014,
  title={Generating Sequences With Recurrent Neural Networks},
  url={http://arxiv.org/abs/1308.0850},
  DOI={10.48550/arXiv.1308.0850},
author={Graves, Alex} }
