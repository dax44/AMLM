---
code-fold: show
editor_options: 
  chunk_output_type: console
---

# Modele językowe

W tym rozdziale przedstawimy modele głębokiego uczenia, które mogą przetwarzać
tekst (rozumiany jako ciąg słów lub ciąg znaków). Dwa podstawowe algorytmy
głębokiego uczenia się dla przetwarzania sekwencji słów to rekurencyjne sieci
neuronowe (ang. *recurent neural networks*) i sieci splotowe 1D, jako
jednowymiarowa wersja sieci splotowych 2D. Zastosowania tych algorytmów
obejmują:

-   Klasyfikację dokumentów i klasyfikację szeregów czasowych, np.
    identyfikacja tematu artykułu lub autora książki;
-   Porównywanie szeregów czasowych, np. szacowanie, jak blisko siebie są dwa
    dokumenty lub dwa indeksy giełdowe;
-   Uczenie się od sekwencji do sekwencji, np. dekodowanie zdania angielskiego
    na francuskie.
-   Analiza nastrojów (ang. *sentiment analysis*), np. klasyfikacja nastrojów
    tweetów lub recenzji filmowych jako pozytywnych lub negatywnych;
-   Prognozowanie w szeregu czasowym, np. przewidywanie przyszłej pogody w
    danym miejscu na podstawie ostatnich danych pogodowych (patrze poprzedni
    rozdział).

Przykłady w tym rozdziale skupią się na analizie sentymentu na zbiorze danych
IMDB. Jednak techniki, które zademonstrujemy są istotne dla wszystkich
zastosowań, które właśnie wymieniliśmy, i wielu innych. I choć zastosowania
modele rekurencynje znajdują w różnych dziedzinach nauki, to my skupimy się na
modelowaniu języków naturalnych.

## Rys historyczny

W informatyce, ludzkie języki, takie jak angielski czy mandaryński, określa się
mianem "naturalnych", aby odróżnić je od języków zaprojektowanych dla maszyn,
takich jak Assembly, LISP czy XML. Każdy język maszynowy został
zaprojektowany - jego punkt wyjścia stanowił inżynier, który spisał zbiór
formalnych zasad opisujących, jakie stwierdzenia można formułować w danym
języku i co one oznaczają. Zasady powstały jako pierwsze, a ludzie zaczęli
używać języka dopiero po ukończeniu zestawu reguł. W przypadku języka ludzkiego
jest odwrotnie, najpierw pojawia się użycie, a zasady powstają później. Język
naturalny został ukształtowany przez proces ewolucji, podobnie jak organizmy
biologiczne - to czyni go "naturalnym". Jego "zasady", takie jak gramatyka
języka polskiego, zostały sformalizowane dopiero po fakcie i często są
ignorowane lub łamane przez jego użytkowników. W rezultacie, chociaż język
"maszynowy", czytelny dla maszyn jest wysoce uporządkowany i rygorystyczny,
używając precyzyjnych zasad składniowych do łączenia dokładnie zdefiniowanych
pojęć z ustalonego słownictwa, język naturalny jest nieuporządkowany –
niejednoznaczny, chaotyczny, rozległy i ciągle podlegający zmianom.

Tworzenie algorytmów zdolnych do rozumienia języka naturalnego to poważne
wyzwanie, bo w końcu język, a w szczególności tekst, stanowi podstawę
większości naszych komunikatów i rozwoju kulturowego. Internet to w większości
tekst. Język to sposób, w jaki przechowujemy niemal całą naszą wiedzę. Nasze
myśli w dużej mierze opierają się na języku. Jednak zdolność rozumienia języka
naturalnego przez długi czas była poza zasięgiem maszyn. Niektórzy ludzie
naiwnie sądzili, że można po prostu spisać "zestaw zasad języka angielskiego",
podobnie jak można spisać zestaw zasad LISP[^llm-1]. Wczesne próby budowy
systemów przetwarzania języka naturalnego (NLP) były więc podejmowane przez
pryzmat "stosowanej lingwistyki". Inżynierowie i lingwiści ręcznie tworzyli
złożone zestawy zasad, aby wykonać podstawowe tłumaczenia maszynowe lub
stworzyć proste *chatboty*, takie jak słynny program ELIZA[^llm-2] z lat 60.,
który używał dopasowywania wzorców, aby podtrzymać bardzo podstawową
konwersację. Ale język jest rzeczą niepokorną: nie poddaje się łatwo
formalizacji. Po kilku dekadach wysiłków możliwości tych systemów pozostały
rozczarowujące.

Ręcznie tworzone reguły utrzymywały się jako dominujące podejście aż do lat 90.
XX wieku. Jednak zaczynając od końca lat 80., szybsze komputery i większa
dostępność danych zaczęły czynić alternatywę bardziej wykonalną. Gdy znajdziemy
się w sytuacji, w której budujemy systemy będące dużymi stosami reguł ad hoc,
prawdopodobnie zaczniemy zadawać sobie pytanie: "Czy mogę użyć korpusu danych,
aby zautomatyzować proces znajdowania tych reguł? Czy mogę szukać reguł w
pewnej przestrzeni reguł, zamiast samemu je wymyślać?" I tak właśnie przeszło
się do uczenia maszynowego. W związku z tym, pod koniec lat 80. zaczęliśmy
obserwować podejścia uczenia maszynowego do przetwarzania języka naturalnego.
Najwcześniejsze z nich opierały się na drzewach decyzyjnych – intencją było
dosłownie zautomatyzowanie rozwoju rodzaju reguł if/then/else poprzednich
systemów. Następnie podejścia statystyczne zaczęły zyskiwać na popularności,
zaczynając od regresji logistycznej. Z czasem, modele parametryczne oparte na
uczeniu w pełni przejęły kontrolę, a lingwistyka zaczęła być postrzegana
bardziej jako przeszkoda niż użyteczne narzędzie. Frederick Jelinek, wczesny
badacz rozpoznawania mowy, żartował w latach 90.: "Za każdym razem, gdy
zwalniam lingwistę, wydajność systemu rozpoznawania mowy rośnie."

To na czym polega współczesne przetwarzanie języka naturalnego (NLP), to
wykorzystanie uczenia maszynowego i dużych zbiorów danych, aby dać komputerom
zdolność nie tyle rozumienia języka, co bardziej ambitnego celu, przyswajania
fragmentu języka jako danych wejściowych i zwracania czegoś użytecznego, na
przykład przewidywania następujących kwestii:

-   "Jaki jest temat tego tekstu?" (klasyfikacja tekstu);
-   "Czy ten tekst zawiera treści obraźliwe?" (filtrowanie treści);
-   "Czy ten tekst brzmi pozytywnie czy negatywnie?" (analiza sentymentu);
-   "Jaki powinno być następne słowo w tym niekompletnym zdaniu?" (modelowanie
    języka);
-   "Jak powiedziałbyś to po niemiecku?" (tłumaczenie);
-   "Jak można by streścić ten artykuł w jednym akapicie?" (streszczanie);
-   I tak dalej.

Oczywiście, powinniśmy pamiętać, że modele przetwarzania tekstu, które będziemy
szkolić, nie będą posiadać ludzkiego zrozumienia języka, raczej będą po prostu
szukać statystycznych reguł w danych wejściowych, co okazuje się wystarczające
do dobrego wykonywania wielu prostych zadań. W podobny sposób, w jaki
rozpoznawanie obrazów, to rozpoznawanie wzorców stosowane do pikseli,
przetwarzanie języka naturalnego (NLP) to rozpoznawanie wzorców stosowane do
słów, zdań i akapitów.

Narzędzia NLP - drzewa decyzyjne i regresja logistyczna - ewoluowały, choć
powoli od lat 90. do wczesnych lat 2010. Większość badań skupiała się na
inżynierii cech. Kiedy François Chollet wygrał swój pierwszy konkurs NLP na
Kaggle w 2013 roku, jego model opierał się na drzewach decyzyjnych i regresji
logistycznej. Jednak około 2014-2015 roku sytuacja zaczęła się wreszcie
zmieniać. Wielu badaczy zaczęło badać zdolności rozumienia języka przez
rekurencyjne sieci neuronowe, w szczególności LSTM.

Na początku 2015 roku, `keras` udostępnił pierwszą otwartą, łatwą w użyciu
implementację LSTM, tuż na początku ogromnej fali zainteresowania sieciami
neuronowymi rekurencyjnymi. Następnie od 2015 do 2017 roku, sieci neuronowe
rekurencyjne zdominowały rozwijającą się scenę NLP. Modele LSTM dwukierunkowe,
w szczególności, ustanowiły standard w wielu ważnych zadaniach, od
streszczania, przez odpowiedzi na pytania, po tłumaczenie maszynowe. W końcu
około 2017–2018 roku pojawiła się nowa architektura, która zastąpiła RNN -
*transformer*, o którym dowiemy się więcej w drugiej części tego rozdziału.
Transformatory umożliwiły znaczący postęp w całej dziedzinie w krótkim czasie,
a obecnie większość systemów NLP opiera się na nich.

[^llm-1]: LISP (skrót od LISt Processing) jest jednym z najstarszych języków
    programowania, nadal używanych. Został zaprojektowany w 1958 roku przez
    Johna McCarthy'ego w Massachusetts Institute of Technology (MIT)

[^llm-2]: ELIZA to jeden z pierwszych programów komputerowych, który imitował
    rozmowę z człowiekiem. Został stworzony w połowie lat 60. XX wieku przez
    Josepha Weizenbauma w Massachusetts Institute of Technology (MIT)

## Dane tekstowe

Tekst jest jedną z najbardziej rozpowszechnionych form danych sekwencyjnych.
Może być rozumiany jako ciąg znaków lub ciąg słów, choć najczęściej pracuje się
na poziomie słów. Modele głębokiego uczenia przetwarzające sekwencje, które
przedstawimy w kolejnych rozdziałach, mogą wykorzystać tekst do stworzenia
podstawowej formy rozumienia języka naturalnego, wystarczającej do zastosowań
takich jak klasyfikacja dokumentów, analiza sentymentu, identyfikacja autorów,
a nawet odpowiadanie na pytania (w ograniczonym kontekście).

Modele głębokiego uczenia, będąc funkcjami różniczkowalnymi, mogą przetwarzać
tylko tensory liczbowe: nie mogą przyjmować surowego tekstu jako danych
wejściowych. Wektoryzacja tekstu to proces przekształcania tekstu w tensory
liczbowe. Procesy wektoryzacji tekstu mają wiele kształtów i form, ale
wszystkie przebiegają według tego samego schematu (patrz @fig-token1):

-   Najpierw standaryzujesz tekst, aby ułatwić jego przetwarzanie, np.
    zamieniając go na małe litery lub usuwając interpunkcję.
-   Następnie dzielimy tekst na jednostki (zwane tokenami), takie jak znaki,
    słowa lub grupy słów. Nazywa się to tokenizacją.
-   Przekształcasz każdy taki token w wektor liczbowy. Zazwyczaj wymaga to
    uprzedniego zindeksowania wszystkich tokenów występujących w danych.

![Przebieg procesu zamiany tekstu na wektory
liczbowe](images/Zrzut%20ekranu%202023-03-17%20o%2019.57.48.png){#fig-token1
fig-align="center" width="600"}

## Standaryzacja tekstu

Standaryzacja tekstu jest podstawową formą inżynierii cech, która ma na celu
usunięcie różnic w kodowaniu, z którymi nie chcemy, aby nasz model miał do
czynienia. Nie jest to wyłączna dziedzina uczenia maszynowego - musielibyśmy
zrobić to samo, gdybyśmy budowali wyszukiwarkę. Jednym z najprostszych i
najbardziej rozpowszechnionych schematów standaryzacji jest "konwersja na małe
litery i usunięcie znaków interpunkcyjnych".

Innym częstym przekształceniem jest konwersja znaków specjalnych do
standardowej formy, np. zastąpienie "é" przez "e", "æ" przez "ae" itd. Np.
token "méxico" stałby się wtedy "mexico".

Ostatnim, znacznie bardziej zaawansowanym wzorcem standaryzacji, który jest
rzadziej używany w kontekście uczenia maszynowego, jest *stemming*:
przekształcanie odmian słów (takich jak różne formy koniugacyjne czasownika) w
jedną wspólną reprezentację, jak przekształcanie "złapany" i "łapiąc" w
"\[łapać\]" lub "koty" w "\[kot\]". Dzięki stemmingowi, "rozpoczynając" i
"rozpoczęty" stałyby się czymś w rodzaju "\[rozpoczynać\]".

Dzięki tym technikom standaryzacji, nasz model będzie wymagał mniej danych
treningowych i będzie lepiej generalizował - nie będzie potrzebował wielu
przykładów zarówno "Zachodu słońca", jak i "zachodów słońca", aby nauczyć się,
że oznaczają one to samo, i będzie w stanie nadać sens słowu "Meksyk", nawet
jeśli widział tylko "meksyk" w swoim zestawie treningowym. Oczywiście
standaryzacja może również wymazać pewną ilość informacji, więc zawsze należy
pamiętać o kontekście: na przykład, jeśli piszesz model, który wyodrębnia
pytania z artykułów z wywiadami, powinien on zdecydowanie traktować "?" jako
oddzielny token zamiast go upuszczać, ponieważ jest to przydatny sygnał dla
tego konkretnego zadania.

## Tokenizacja

Kiedy tekst jest już znormalizowany, musimy podzielić go na jednostki do
wektoryzacji (*tokeny*) - krok zwany tokenizacją. Można to zrobić na trzy różne
sposoby:

-   Tokenizacja na poziomie słowa - gdzie tokeny są oddzielonymi spacjami (lub
    interpunkcją) podciągami. Wariantem tego jest dalsze dzielenie słów na
    podsłowia, gdy ma to zastosowanie, na przykład traktowanie "zaczyna" jako
    "zaczyna+jąc" lub "wezwany" jako "wezwani".
-   N-gram tokenizacji - gdzie tokeny są grupami N kolejnych słów. Na przykład
    "the cat" lub "he was" byłyby tokenami 2-gramowymi (zwanymi również
    bigramami). Generalnie N-gramy słów to grupy N (lub mniej) kolejnych słów,
    które można wyodrębnić ze zdania. Ta sama koncepcja może być również
    zastosowana do znaków zamiast słów.
-   Tokenizacja na poziomie znaków - gdzie każdy znak jest swoim własnym
    tokenem. W praktyce, ten schemat jest rzadko używany i naprawdę widzisz go
    tylko w specjalistycznych kontekstach, takich jak generowanie tekstu lub
    rozpoznawanie mowy.

Ogólnie rzecz biorąc, zawsze będziemy używać tokenizacji na poziomie słowa lub
N-gramu. Istnieją dwa rodzaje modeli przetwarzania tekstu: te, które dbają o
kolejność słów, zwane modelami sekwencyjnymi, oraz te, które traktują słowa
wejściowe jako zestaw, odrzucając ich oryginalną kolejność, zwane modelami
*bag-of-words*. Jeśli budujesz model sekwencyjny, używasz tokenizacji na
poziomie słów, a jeśli budujesz model worka słów, używasz tokenizacji N-gramów.
N-gramy są sposobem na sztuczne wprowadzenie do modelu niewielkiej ilości
informacji o lokalnym porządku słów. W tym rozdziale dowiesz się więcej o
każdym typie modelu i o tym, kiedy należy ich używać.

Przykładowo zdanie "the cat sat on the mat" można zamienić na bigramy w
następujący sposób:

```{r}
c("the", "the cat", "cat", "cat sat", "sat",
 "sat on", "on", "on the", "the mat", "mat")
```

natomiast w 3-gramy:

```{r}
c("the", "the cat", "cat", "cat sat", "the cat sat",
 "sat", "sat on", "on", "cat sat on", "on the",
 "sat on the", "the mat", "mat", "on the mat")
```

## Indeksowanie słownika

Gdy nasz tekst jest podzielony na tokeny, musimy zakodować każdy token w
reprezentacji numerycznej. Wszystkie procesy wektoryzacji tekstu polegają na
zastosowaniu pewnego schematu tokenizacji, a następnie skojarzeniu wektorów
liczbowych z wygenerowanymi tokenami. Wektory te, spakowane w tensory
sekwencji, są wprowadzane do głębokich sieci neuronowych. Istnieje wiele
sposobów na powiązanie wektora z tokenem. W tej sekcji przedstawimy dwa główne:
kodowanie tokenów metodą *one-hot* oraz osadzanie tokenów (ang. *embeddings* -
zwykle używane wyłącznie dla słów). Pozostała część tego rozdziału wyjaśnia te
techniki i pokazuje jak ich użyć, aby przejść od surowego tekstu do tensora,
który można wysłać do sieci.

Technicznie rzecz ujmując, należy zauważyć, że na tym etapie często ogranicza
się słownictwo tylko do 20000 lub 30000 najczęściej występujących słów
znalezionych w danych treningowych. Każdy zbiór danych tekstowych ma tendencję
do zawierania ogromnej liczby unikalnych terminów, z których większość pojawia
się tylko raz lub dwa. Indeksowanie tych rzadkich terminów skutkowałoby
nadmiernie dużą przestrzenią cech, gdzie większość cech miałaby prawie żadną
zawartość informacyjną.

Ważny szczegół, który nie powinien umknąć naszej uwadze: gdy szukamy nowego
tokenu w naszym indeksie słownictwa, może się okazać, że go tam nie ma. Nasze
dane treningowe mogły nie zawierać żadnego wystąpienia słowa
"cherimoya"[^llm-3] (lub być może wykluczyliśmy je z indeksu, ponieważ było
zbyt rzadkie), więc wykonanie polecenia
`token_index = match("cherimoya", vocabulary)` może zwrócić `NA`. Aby sobie z
tym poradzić, należy użyć indeksu "poza słownictwem" (skrótowo OOV index) -
rodzaju schowka na wszystkie tokeny, które nie znalazły się w indeksie. Zwykle
jest to indeks 1: tak naprawdę wykonujesz
`token_index = match("cherimoya", vocabulary, nomatch = 1)`. Dekodując
sekwencję liczb całkowitych z powrotem na słowa, zastąpisz 1 czymś w rodzaju
"\[UNK\]" (co nazwałbyś "tokenem OOV").

"Dlaczego używamy 1, a nie 0?" można zapytać. Ponieważ 0 jest już zajęte.
Istnieją dwa specjalne tokeny, których będziemy często używać: token OOV
(indeks 1) i token maski (indeks 0). Chociaż token OOV oznacza "to jest słowo,
którego nie rozpoznaliśmy", to token maski mówi nam "zignoruj mnie, nie jestem
słowem". Używa się go w szczególności do uzupełniania danych sekwencyjnych:
ponieważ partie danych muszą być ciągłe, wszystkie sekwencje w partii danych
sekwencyjnych muszą mieć tę samą długość, więc krótsze sekwencje powinny być
uzupełniane do długości najdłuższej sekwencji. Jeśli chcemy utworzyć partię
danych z sekwencjami `c(5, 7, 124, 4, 89)` i `c(8, 34, 21)`, musiałaby ona
wyglądać następująco:

```{r}
rbind(c(5,  7, 124, 4, 89),
      c(8, 34,  21, 0,  0))
```

Wszystkie omówione do tej pory kroki można w prosty sposób zaprogramować w R:

```{r}
library(keras) # póki co potrzebny tylko by mieć %>%
new_vectorizer <- function() {
  self <- new.env(parent = emptyenv()) # Utworzenie nowego środowiska dla wektoryzatora
  attr(self, "class") <- "Vectorizer"   # Nadanie środowisku klasy "Vectorizer"

  self$vocabulary <- c("[UNK]")         # Inicjalizacja słownika słów z tokenem "[UNK]" (nieznane słowo)

  self$standardize <- function(text) {
    text <- tolower(text)               # Przekształcenie tekstu na małe litery
    gsub("[[:punct:]]", "", text)       # Usunięcie znaków interpunkcyjnych z tekstu
  }

  self$tokenize <- function(text) {
    unlist(strsplit(text, "[[:space:]]+")) # Podział tekstu na tokeny (słowa) na podstawie spacji
  }

  self$make_vocabulary <- function(text_dataset) {
    tokens <- text_dataset %>%
      self$standardize() %>%
      self$tokenize()                   # Standardyzacja i tokenizacja tekstu
    self$vocabulary <- unique(c(self$vocabulary, tokens)) # Aktualizacja słownika o unikalne tokeny
  }

  self$encode <- function(text) {
    tokens <- text %>%
      self$standardize() %>%
      self$tokenize()                   # Standardyzacja i tokenizacja tekstu
    match(tokens, table = self$vocabulary, nomatch = 1) # Zamiana tokenów na ich indeksy w słowniku
  }

  self$decode <- function(int_sequence) {
    vocab_w_mask_token <- c("", self$vocabulary) # Słownik z dodanym pustym tokenem na początku
    vocab_w_mask_token[int_sequence + 1]         # Zamiana sekwencji indeksów na słowa
  }

  self # Zwrócenie środowiska wektoryzatora
}

```

A tak wygląda on w działaniu... Najpierw tworzymy słownik na podstawie prostego
korpusu.

```{r}
vectorizer <- new_vectorizer()

dataset <- c(
    "I write, erase, rewrite",
    "Erase again, and then",
    "A poppy blooms."
)

vectorizer$make_vocabulary(dataset)
vectorizer$vocabulary
```

A następnie wykorzystujemy go do nowego zdania.

```{r}
test_sentence <- "I write, rewrite, and still rewrite again"
encoded_sentence <- vectorizer$encode(test_sentence)
print(encoded_sentence)
decoded_sentence <- vectorizer$decode(encoded_sentence)
print(decoded_sentence)
```

Choć jak widać wszystko działa poprawnie, to w praktycznych zastosowaniach
będziemy korzystali raczej z rozwiązań w `keras` typu warstwa
`layer_text_vectorization()`.

```{r}
text_vectorization <- layer_text_vectorization(output_mode = "int")
```

Domyślnie, `layer_text_vectorization()` będzie używać ustawienia "konwertuj na
małe litery i usuń znaki interpunkcyjne" do standaryzacji tekstu oraz "dziel
względem znaków przerw (typu spacja)" do tokenizacji. Ale co ważne, można
dostarczyć niestandardowe funkcje do standaryzacji i tokenizacji, co oznacza,
że warstwa jest wystarczająco elastyczna, aby obsłużyć każdy przypadek użycia.
Należy pamiętać, że takie niestandardowe funkcje powinny działać na tensorach
typu `tf.string`, a nie na zwykłych wektorach znaków R! Na przykład, domyślne
zachowanie warstwy jest równoważne następującemu:

```{r}
library(tensorflow)
custom_standardization_fn <- function(string_tensor) {
  string_tensor %>%
    tf$strings$lower() %>%
    tf$strings$regex_replace("[[:punct:]]", "")
}

custom_split_fn <- function(string_tensor) {
  tf$strings$split(string_tensor)
}

text_vectorization <- layer_text_vectorization(
  output_mode = "int",
  standardize = custom_standardization_fn,
  split = custom_split_fn
)
```

Aby zindeksować słownictwo korpusu tekstowego, wystarczy wywołać metodę
`adapt()` warstwy z obiektem TF Dataset, który daje ciągi znaków, lub po prostu
z wektorem znaków R:

```{r}
dataset <- c("I write, erase, rewrite",
             "Erase again, and then",
             "A poppy blooms.")
adapt(text_vectorization, dataset)
```

Należy pamiętać, że obliczone słownictwo można pobrać za pomocą funkcji
`get_vocabulary()`. Może to być przydatne, jeśli trzeba przekonwertować tekst
zakodowany jako sekwencje liczb całkowitych z powrotem na słowa. Pierwsze dwa
wpisy w słowniku to token maski (indeks 0) i token OOV (indeks 1). Wpisy na
liście słownictwa są sortowane według częstotliwości, więc w przypadku zbioru
danych z rzeczywistego świata bardzo popularne słowa, takie jak "the" lub "a",
będą na pierwszym miejscu.

```{r}
get_vocabulary(text_vectorization)
```

Na potrzeby prezentacji, spróbujmy zakodować, a następnie zdekodować
przykładowe zdanie:

```{r}
vocabulary <- text_vectorization %>% get_vocabulary()
test_sentence <- "I write, rewrite, and still rewrite again"
encoded_sentence <- text_vectorization(test_sentence)
decoded_sentence <- paste(vocabulary[as.integer(encoded_sentence) + 1],
                          collapse = " ")

encoded_sentence
decoded_sentence
```

::: callout-tip
Ponieważ `layer_text_vectorization()` jest głównie operacją wyszukiwania
słownika, która konwertuje tokeny na liczby całkowite, nie może być wykonywana
na GPU (lub TPU) - tylko na CPU. Jeśli więc trenujemy nasz model na GPU,
funkcja `layer_text_vectorization()` zostanie uruchomiona na CPU przed
wysłaniem danych wyjściowych do GPU. Ma to istotny wpływ na wydajność.

Istnieją dwa sposoby wykorzystania funkcji `layer_text_vectorization()`.
Pierwszą opcją jest umieszczenie jej w potoku TF Dataset, tak jak poniżej:

```{r}
#| eval: false
int_sequence_dataset <- string_dataset %>%
  dataset_map(text_vectorization, num_parallel_calls = 4)
```

Drugą opcją jest uczynienie go częścią modelu (w końcu jest to warstwa
`keras`), jak poniżej (w pseudokodzie):

```{r}
#| eval: false
text_input <- layer_input(shape = shape(), dtype = "string")
vectorized_text <- text_vectorization(text_input)
embedded_input <- vectorized_text %>% layer_embedding(...)
output <- embedded_input %>% ...
model <- keras_model(text_input, output)
```

Jest między nimi ważna różnica: jeśli krok wektoryzacji jest częścią modelu,
będzie on wykonywany synchronicznie z resztą modelu. Oznacza to, że na każdym
etapie uczenia reszta modelu (umieszczona na GPU) będzie musiała poczekać, aż
dane wyjściowe `layer_text_vectorization()` (umieszczone na CPU) będą gotowe,
zanim będzie mogła rozpocząć pracę. Tymczasem umieszczenie warstwy w potoku TF
Dataset umożliwia asynchroniczne wstępne przetwarzanie danych na CPU: podczas
gdy GPU uruchamia model na jednej partii zwektoryzowanych danych, CPU pozostaje
zajęty wektoryzacją następnej partii surowych ciągów.

Jeśli trenujemy model na GPU lub TPU, prawdopodobnie będziemy chcieli wybrać
pierwszą opcję[^llm-4], aby uzyskać najlepszą wydajność. Podczas trenowania na
CPU, przetwarzanie synchroniczne jest w porządku: uzyskamy wówczas 100%
wykorzystania rdzeni, niezależnie od wybranej opcji.

Teraz, jeśli mielibyśmy wyeksportować nasz model do środowiska produkcyjnego,
chcielibyśmy wysłać model, który akceptuje surowe ciągi znaków jako dane
wejściowe, jak w powyższym fragmencie kodu dla drugiej opcji; w przeciwnym
razie musielibyśmy ponownie wdrożyć standaryzację tekstu i tokenizację w naszym
środowisku produkcyjnym (np. w JavaScript). Stanęlibyśmy w obliczu ryzyka
wprowadzenia niewielkich zmian w przetwarzaniu wstępnym, które zaszkodziłyby
dokładności modelu. Na szczęście funkcja `layer_text_vectorization()` umożliwia
włączenie wstępnego przetwarzania tekstu bezpośrednio do modelu, co ułatwia
jego wdrożenie, nawet jeśli pierwotnie warstwa była używana jako część potoku
TF Dataset.
:::

[^llm-3]: przypominający kształtem serce owoc, jest słodki jak budyń, dlatego
    Anglicy nazywają go *custard apple* - czyli jabłko o smaku kremu
    budyniowego

[^llm-4]: czyli osadzenie wektoryzacji w potoku

## One-hot encoding

Kodowanie one-hot jest najczęstszym, najbardziej podstawowym sposobem
przekształcenia tokena w wektor. Polega ono na skojarzeniu unikalnego indeksu z
każdym słowem, a następnie przekształceniu tego indeksu $i$ w wektor binarny o
rozmiarze $N$ (rozmiar słownika); wektor składa się ze wszystkich zer, z
wyjątkiem $i$-tego wpisu, który jest 1.

```{r}
#| eval: false

one_hot_encode_token <- function(token) {
  vector <- array(0, dim = length(vocabulary))
  token_index <- match(token, vocabulary)
  vector[token_index] <- 1
  vector
}
```

Metoda ta, zważywszy na swoją rzadką reprezentację (większość wartości to 0),
rzadko stosowana w praktyce. Słaba wydajność tej techniki spowodowała powstanie
*embedingów.* Zanim jednak przejdziemy do *embedingów* przyjrzymy się
dokładniej dwóm podejściom do reprezentacji grup słów: zbiorom słów (ang.
*bag-of-words*) i ciągom słów, w których kolejność jest ważna. Zrobimy to na
przykładzie danych IMDB.

## Tokenizacja na przykładach

Sposób, w jaki model uczenia maszynowego powinien reprezentować poszczególne
słowa, jest stosunkowo niekontrowersyjną kwestią: są to cechy kategorialne
(wartości z predefiniowanego zestawu) i wiemy, jak sobie z nimi radzić. Powinny
one być zakodowane jako wymiary w przestrzeni cech lub jako wektory kategorii
(w tym przypadku wektory słów). Znacznie bardziej problematyczną kwestią jest
jednak to, jak zakodować sposób, w jaki słowa są wplecione w zdania - kolejność
słów.

Problem kolejności w języku naturalnym jest interesujący. W przeciwieństwie do
kroków szeregu czasowego, słowa w zdaniu nie mają naturalnej, kanonicznej
kolejności. Różne języki porządkują podobne słowa na bardzo różne sposoby. Na
przykład, struktura zdań w języku angielskim jest zupełnie inna niż w języku
japońskim. Nawet w obrębie danego języka można zazwyczaj powiedzieć to samo na
różne sposoby, zmieniając nieco kolejność słów. Co więcej, jeśli słowa w
krótkim zadaniu ułożysz losowo, to nadal możesz w dużej mierze dowiedzieć się,
co zostało powiedziane, choć w wielu przypadkach pojawia się dwuznaczność.
Kolejność jest wyraźnie ważna, ale jej związek ze znaczeniem nie jest prosty.

Sposób reprezentowania kolejności słów jest kluczowym pytaniem, z którego
wynikają różne rodzaje architektur NLP. Najprostszą rzeczą, jaką można zrobić,
jest po prostu odrzucenie kolejności i traktowanie tekstu jako
nieuporządkowanego zbioru słów - daje to modele worków słów. Można również
zdecydować, że słowa powinny być przetwarzane ściśle w kolejności, w jakiej się
pojawiają, pojedynczo, jak kroki w szeregu czasowym - można wtedy wykorzystać
modele rekurencyjne z poprzedniego rozdziału. Wreszcie, możliwe jest również
podejście hybrydowe: architektura *transformer* jest technicznie niezależna od
kolejności, ale wprowadza informacje o pozycji słów do przetwarzanych
reprezentacji, co pozwala jej jednocześnie patrzeć na różne części zdania (w
przeciwieństwie do RNN), a jednocześnie jest świadoma kolejności. Ponieważ
uwzględniają one kolejność słów, zarówno RNN, jak i transformatory nazywane są
modelami sekwencyjnymi.

Historycznie rzecz ujmując, większość wczesnych zastosowań uczenia maszynowego
w NLP obejmowała po prostu modele *bag-of-words*. Zainteresowanie modelami
sekwencyjnymi zaczęło rosnąć dopiero w 2015 roku, wraz z odrodzeniem się
rekurencyjnych sieci neuronowych. Obecnie oba podejścia pozostają istotne.

### Przygotowanie danych IMDB

Zacznijmy od pobrania danych.

```{r}
#| eval: false
# Zdefiniowanie URL, z którego ma być pobrany plik
url <- "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" 

filename <- paste0("/Users/majerek/", basename(url)) # Pobranie nazwy pliku z URL, w tym przypadku 'aclImdb_v1.tar.gz'

options(timeout = 60*10) # Ustawienie limitu czasu na pobieranie na 10 minut (60 sekund * 10)

download.file(url, destfile = filename) # Pobranie pliku z zadanego URL i zapisanie go pod nazwą 

untar(filename, exdir = "/Users/majerek/") # Rozpakowanie pobranego pliku tar.gz
```

Tak wygląda struktura katalogu po rozpakowaniu.

```{r}
fs::dir_tree("/Users/majerek/aclImdb", recurse = 1, type = "directory")
```

Na przykład katalog `train/pos/` zawiera zestaw 12500 plików tekstowych, z
których każdy zawiera tekst recenzji filmu o pozytywnym wydźwięku, który
zostanie wykorzystany jako dane szkoleniowe. Recenzje o negatywnym wydźwięku
znajdują się w katalogach "neg". W sumie istnieje 25000 plików tekstowych do
szkolenia i kolejne 25000 do testowania. Znajduje się tam również podkatalog
`train/unsup`, którego nie potrzebujemy. Usuńmy go:

```{r}
#| eval: false
fs::dir_delete("/Users/majerek/aclImdb/train/unsup/")
```

Spójrzmy na zawartość kilku z tych plików tekstowych.

```{r}
writeLines(readLines("/Users/majerek/aclImdb/train/pos/4000_10.txt", warn = FALSE))
```

Następnie przygotujmy zestaw walidacyjny, oddzielając 20% treningowych plików
tekstowych w nowym katalogu, `aclImdb/val`.

```{r}
#| eval: false
library(fs)
set.seed(1337)
base_dir <- path("/Users/majerek/aclImdb")

for (category in c("neg", "pos")) {
  filepaths <- dir_ls(base_dir / "train" / category)
  num_val_samples <- round(0.2 * length(filepaths))
  val_files <- sample(filepaths, num_val_samples)

  dir_create(base_dir / "val" / category)
  file_move(val_files,
            base_dir / "val" / category)
}
```

W zagadnieniach z analizy obrazów używaliśmy funkcji
`image_dataset_from_directory()` do utworzenia zbiorów obrazów i ich etykiet
dla struktury katalogów. Dokładnie to samo można zrobić dla plików tekstowych
za pomocą narzędzia `text_dataset_from_directory()`. Utwórzmy trzy obiekty TF
Dataset do uczenia, walidacji i testowania:

```{r}
library(keras)
library(tfdatasets)

train_ds <- text_dataset_from_directory("/Users/majerek/aclImdb/train")
val_ds <- text_dataset_from_directory("/Users/majerek/aclImdb/val")
test_ds <- text_dataset_from_directory("/Users/majerek/aclImdb/test")
```

Te zestawy danych tworzą dane wejściowe, które są tensorami `tf.string` i
wyjścia, które są tensorami `int32` kodującymi wartość "0" lub "1".

```{r}
c(inputs, targets) %<-% iter_next(as_iterator(train_ds))
str(inputs)
str(targets)

inputs[1]
targets[1]
```

Wszystko gotowe. Teraz spróbujmy nauczyć się czegoś z tych danych.

### Modelowanie za pomocą bag-of-words

Najprostszym sposobem zakodowania fragmentu tekstu do przetwarzania przez model
uczenia maszynowego jest odrzucenie kolejności i potraktowanie go jako zbioru
("worka") tokenów. Najczęściej stosuje się tu unigramy, czyli podział na
pojedyncze słowa lub znaki. Zdarza się jednak, gdy chcemy zachować część
informacji o kolejności słów, że stosuje się N-gramy.

Dla przywoływanego już przykładu "the cat sat on the mat" podział na unigramy
jest następujący:

```{r}
c("cat", "mat", "on", "sat", "the")
```

Główną zaletą tego kodowania jest możliwość reprezentowania całego tekstu jako
pojedynczego wektora, gdzie każdy element jest wskaźnikiem obecności danego
słowa. Na przykład, korzystając z kodowania binarnego (ang. *multi-hot
encoding*), zakodujemy tekst jako wektor mający tyle współrzędnych, ile jest
słów w naszym słowniku, z 0 niemal wszędzie i kilkoma 1 dla wymiarów, które
kodują słowa obecne w tekście.

Najpierw przetwórzmy nasze surowe zbiory danych tekstowych za pomocą warstwy
`layer_text_vectorization()`, tak aby uzyskać wielokrotnie zakodowane binarnie
wektory słów. Nasza warstwa będzie patrzeć tylko na pojedyncze słowa (czyli
unigramy).

```{r}
text_vectorization <-
  layer_text_vectorization(max_tokens = 20000,
                           output_mode = "multi_hot")

text_only_train_ds <- train_ds %>%
  dataset_map(function(x, y) x)

adapt(text_vectorization, text_only_train_ds)

binary_1gram_train_ds <- train_ds %>%
  dataset_map( ~ list(text_vectorization(.x), .y),
               num_parallel_calls = 4)
binary_1gram_val_ds <- val_ds %>%
  dataset_map( ~ list(text_vectorization(.x), .y),
               num_parallel_calls = 4)
binary_1gram_test_ds <- test_ds %>%
  dataset_map( ~ list(text_vectorization(.x), .y),
               num_parallel_calls = 4)
```

Sprawdźmy dane wyjściowe dla jednego z tych zestawów.

```{r}
c(inputs, targets) %<-% iter_next(as_iterator(binary_1gram_train_ds))
str(inputs)
str(targets)
inputs[1, ]
targets[1]
```

Napiszmy funkcję budowania modelu wielokrotnego użytku, której będziemy używać
we wszystkich naszych eksperymentach w tej sekcji.

```{r}
get_model <- function(max_tokens = 20000, hidden_dim = 16) {
  inputs <- layer_input(shape = c(max_tokens))
  outputs <- inputs %>%
    layer_dense(hidden_dim, activation = "relu") %>%
    layer_dropout(0.5) %>%
    layer_dense(1, activation = "sigmoid")
  model <- keras_model(inputs, outputs)
  model %>% compile(optimizer = "rmsprop",
                    loss = "binary_crossentropy",
                    metrics = "accuracy")
  model
}
```

Na koniec wytrenujmy i przetestujmy nasz model.

```{r}
#| eval: false
model <- get_model()
model
callbacks = list(
  callback_model_checkpoint("models/binary_1gram.keras", save_best_only = TRUE)
)

model %>% fit(
  dataset_cache(binary_1gram_train_ds),
  validation_data = dataset_cache(binary_1gram_val_ds),
  epochs = 10,
  callbacks = callbacks
)
```

```{r}
model <- load_model_tf("models/binary_1gram.keras")
cat(sprintf(
  "Test acc: %.3f\n", evaluate(model, binary_1gram_test_ds)["accuracy"]))
```

Dokładność modelu na zbiorze testowym jest na poziomie 88,7%: nieźle! Należy
zauważyć, że w tym przypadku, ponieważ zbiór danych jest zrównoważonym
dwuklasowym zbiorem danych klasyfikacyjnych (jest tyle samo próbek pozytywnych,
co negatywnych), "naiwny poziom bazowy", który moglibyśmy osiągnąć bez
trenowania rzeczywistego modelu, wynosiłby tylko 50%. Tymczasem najlepszy
wynik, jaki można osiągnąć na tym zbiorze danych bez wykorzystywania danych
zewnętrznych, wynosi około 95% dokładności testu.

Oczywiście odrzucenie kolejności słów jest bardzo redukcyjne, ponieważ nawet
pojęcia elementarne można wyrazić za pomocą wielu słów: termin "Stany
Zjednoczone" przekazuje pojęcie, które jest zupełnie odmienne od znaczenia słów
"stany" i "zjednoczone" rozpatrywanych osobno. Z tego powodu zwykle kończy się
to ponownym wprowadzeniem informacji o lokalnym porządku do reprezentacji worka
słów, patrząc na N-gramy, a nie na pojedyncze słowa (najczęściej bigramy).

Warstwę `layer_text_vectorization()` można skonfigurować tak, aby zwracała
dowolne N-gramy: bigramy, trygramy itd. Wystarczy przekazać argument
`ngrams = N`, jak na poniższym listingu.

```{r}
text_vectorization <-
  layer_text_vectorization(ngrams = 2,
                           max_tokens = 20000,
                           output_mode = "multi_hot")
```

Przetestujmy nasz model oparty na bigramach.

```{r}
adapt(text_vectorization, text_only_train_ds)

dataset_vectorize <- function(dataset) {
  dataset %>%
    dataset_map(~ list(text_vectorization(.x), .y),
                num_parallel_calls = 4)
}

binary_2gram_train_ds <- train_ds %>% dataset_vectorize()
binary_2gram_val_ds <- val_ds %>% dataset_vectorize()
binary_2gram_test_ds <- test_ds %>% dataset_vectorize()
```

```{r}
#| eval: false
model <- get_model()
model
callbacks = list(callback_model_checkpoint("models/binary_2gram",
                                           save_best_only = TRUE))

model %>% fit(
  dataset_cache(binary_2gram_train_ds),
  validation_data = dataset_cache(binary_2gram_val_ds),
  epochs = 10,
  callbacks = callbacks
)
```

```{r}
model <- load_model_tf("models/binary_2gram")
evaluate(model, binary_2gram_test_ds)["accuracy"] %>%
  sprintf("Test acc: %.3f\n", .) %>% cat()
```

Uzyskujemy teraz 90,0% dokładności testu, co stanowi znaczną poprawę! Okazuje
się, że lokalna kolejność jest dość ważna.

Można również dodać nieco więcej informacji do tej reprezentacji, licząc, ile
razy występuje każde słowo lub N-gram, czyli biorąc histogram słów w tekście:

```{r}
c("the" = 2, "the cat" = 1, "cat" = 1, "cat sat" = 1, "sat" = 1,
  "sat on" = 1, "on" = 1, "on the" = 1, "the mat" = 1, "mat" = 1)
```

Jeśli przeprowadzamy klasyfikację tekstu, wiedza o tym, ile razy słowo
występuje w próbce, ma kluczowe znaczenie: każda wystarczająco długa recenzja
filmu może zawierać słowo "okropny" niezależnie od nastroju, ale recenzja
zawierająca wiele wystąpień słowa "okropny" jest prawdopodobnie negatywna. Oto
jak policzyć wystąpienia bigramów za pomocą `layer_text_vectorization()`:

```{r}
text_vectorization <-
  layer_text_vectorization(ngrams = 2,
                           max_tokens = 20000,
                           output_mode = "count")
```

Oczywiście niektóre słowa będą występować częściej niż inne, niezależnie od
tego, o czym jest tekst. Słowa "the", "a", "is" i "are" zawsze będą dominować w
histogramach liczby słów, zagłuszając inne słowa, mimo że są w zasadzie
bezużytecznymi cechami w kontekście klasyfikacji. Jak możemy temu zaradzić?

Poprzez normalizację. Moglibyśmy po prostu znormalizować liczbę słów, odejmując
średnią i dzieląc przez wariancję (obliczoną dla całego zbioru danych
szkoleniowych). To miałoby sens. Z wyjątkiem tego, że większość wektoryzowanych
zdań składa się prawie wyłącznie z zer (nasz poprzedni przykład zawiera 12
niezerowych wpisów i 19 988 zerowych wpisów), co jest właściwością zwaną
"rzadkością". Jest to świetna właściwość, ponieważ znacznie zmniejsza
obciążenie obliczeniowe i zmniejsza ryzyko przeuczenia. Gdybyśmy odjęli średnią
od każdej cechy, "zniszczylibyśmy" rzadkość. W związku z tym każdy schemat
normalizacji, którego używamy, powinien być oparty wyłącznie na dzieleniu.
Czego zatem powinniśmy użyć jako mianownika? Najlepszą praktyką jest
zastosowanie czegoś, co nazywa się normalizacją TF-IDF - co oznacza
"częstotliwość terminów, odwrotność częstotliwości dokumentów".

::: callout-note
Im częściej dany termin pojawia się w dokumencie, tym ważniejszy jest on dla
zrozumienia jego treści. Jednocześnie częstotliwość, z jaką termin pojawia się
we wszystkich dokumentach w zbiorze danych, również ma znaczenie: terminy,
które pojawiają się w prawie każdym dokumencie (takie jak "the" lub "a") nie są
szczególnie pouczające, podczas gdy terminy, które pojawiają się tylko w
niewielkim podzbiorze wszystkich tekstów (takich jak "Herzog") są bardzo
charakterystyczne, a zatem ważne. TF-IDF to metryka, która łączy te dwie
koncepcje. Waży dany termin, biorąc "częstotliwość terminu", ile razy termin
pojawia się w bieżącym dokumencie i dzieląc go przez miarę "częstotliwości
dokumentu", która szacuje, jak często termin pojawia się w całym zbiorze
danych. Można to obliczyć w następujący sposób:

```{r}
tf_idf <- function(term, document, dataset) {
  term_freq <- sum(document == term)
  doc_freqs <- sapply(dataset, function(doc) sum(doc == term))
  doc_freq <- log(1 + sum(doc_freqs))
  term_freq / doc_freq
}
```

TF-IDF jest tak powszechny, że jest wbudowany w funkcję
`layer_text_vectorization()`. Wszystko, co musimy zrobić, aby zacząć go używać,
to przełączyć argument `output_mode` na "tf_idf".

```{r}
text_vectorization <-
  layer_text_vectorization(ngrams = 2,
                           max_tokens = 20000,
                           output_mode = "tf_idf")
```
:::

Przetrenujmy model z tym schematem.

```{r}
# Wysyłamy tę operację tylko do CPU, ponieważ wykorzystuje ona operacje, których urządzenie GPU jeszcze nie obsługuje.
with(tf$device("CPU"), {
  adapt(text_vectorization, text_only_train_ds)
})

tfidf_2gram_train_ds <- train_ds %>% dataset_vectorize()
tfidf_2gram_val_ds <- val_ds %>% dataset_vectorize()
tfidf_2gram_test_ds <- test_ds %>% dataset_vectorize()
```

```{r}
#| eval: false
model <- get_model()
model
callbacks <- list(callback_model_checkpoint("models/tfidf_2gram",
                                            save_best_only = TRUE))
model %>% fit(
  dataset_cache(tfidf_2gram_train_ds),
  validation_data = dataset_cache(tfidf_2gram_val_ds),
  epochs = 10,
  callbacks = callbacks
)

```

```{r}
model <- load_model_tf("models/tfidf_2gram")
evaluate(model, tfidf_2gram_test_ds)["accuracy"] %>%
  sprintf("Test acc: %.3f", .) %>% cat("\n")
```

Daje nam to 89,6% dokładności testowej w zadaniu klasyfikacji IMDB: nie wydaje
się to być szczególnie pomocne w tym przypadku. Jednak w przypadku wielu
zestawów danych do klasyfikacji tekstu typowy byłby jednoprocentowy wzrost przy
użyciu TF-IDF w porównaniu do zwykłego kodowania binarnego.

::: callout-note
W poprzednich przykładach przeprowadziliśmy standaryzację, podział i
indeksowanie tekstu w ramach potoku TF Dataset. Jeśli jednak chcemy
wyeksportować samodzielny model niezależny od tego potoku, powinniśmy upewnić
się, że zawiera on własne wstępne przetwarzanie tekstu (w przeciwnym razie
konieczne będzie ponowne wdrożenie w środowisku produkcyjnym, co może być
trudne lub może prowadzić do subtelnych rozbieżności między danymi
szkoleniowymi a danymi produkcyjnymi). Na szczęście jest to łatwe.

Wystarczy utworzyć nowy model, który ponownie wykorzysta warstwę
`text_vectorization` i doda do niej właśnie wytrenowany model:

```{r}
inputs <- layer_input(shape = c(1), dtype = "string")
outputs <- inputs %>%
  text_vectorization() %>%
  model()
inference_model <- keras_model(inputs, outputs)
```

Wynikowy model może przetwarzać partie nieprzetworzonych ciągów znaków:

```{r}
raw_text_data <- "That was an excellent movie, I loved it." %>%
  as_tensor(shape = c(-1, 1))

predictions <- inference_model(raw_text_data)
str(predictions)
cat(sprintf("%.2f percent positive\n",
            as.numeric(predictions) * 100))
```
:::

### Modelowanie za pomocą sekwencji

Te kilka ostatnich przykładów wyraźnie pokazuje, że kolejność słów ma
znaczenie: ręczna inżynieria funkcji opartych na kolejności, takich jak
bigramy, zapewnia niewielki wzrost dokładności. Zapamiętajmy - historia
głębokiego uczenia polega na odejściu od ręcznej inżynierii cech, w kierunku
umożliwienia modelom uczenia się własnych cech na podstawie samej ekspozycji na
dane. Co by było, gdybyśmy zamiast ręcznie tworzyć funkcje oparte na
kolejności, wystawili model na surowe sekwencje słów i pozwolili mu
samodzielnie wymyślić takie funkcje? O to właśnie chodzi w modelach opartych na
sekwencji.

Aby zaimplementować model sekwencji, należy zacząć od reprezentowania próbek
wejściowych jako sekwencji indeksów liczb całkowitych (jedna liczba całkowita
oznacza jedno słowo). Następnie mapujemy każdą liczbę całkowitą na wektor, aby
uzyskać sekwencje wektorowe. Wreszcie, te sekwencje wektorów należy wprowadzić
do stosu warstw, które mogą korelować krzyżowo cechy z sąsiednich wektorów,
takich jak konwolucje 1D, RNN lub Transformery.

Przez pewien czas, około 2016-2017 roku, dwukierunkowe RNN (w szczególności
dwukierunkowe LSTM) były uważane za najnowocześniejsze rozwiązanie do
modelowania sekwencji. Ponieważ jesteśmy już zaznajomieni z tą architekturą, to
właśnie jej użyjemy w naszych pierwszych przykładach modeli sekwencji. Jednak
obecnie modelowanie sekwencji jest prawie zawsze wykonywane za pomocą
transformatorów, które omówimy wkrótce.

Wypróbujmy pierwszy model sekwencji w praktyce. Najpierw przygotujmy zestawy
danych zwracające sekwencje liczb całkowitych.

```{r}
max_length <- 600
max_tokens <- 20000

text_vectorization <- layer_text_vectorization(
  max_tokens = max_tokens,
  output_mode = "int",
  output_sequence_length = max_length
)

adapt(text_vectorization, text_only_train_ds)

int_train_ds <- train_ds %>% dataset_vectorize()
int_val_ds <- val_ds %>% dataset_vectorize()
int_test_ds <- test_ds %>% dataset_vectorize()
```

Następnie stwórzmy model. Najprostszym sposobem konwersji naszych sekwencji
liczb całkowitych na sekwencje wektorowe jest zakodowanie liczb całkowitych
metodą one-hot (każdy wymiar reprezentowałby jeden możliwy termin w słowniku).
Na tych wektorach one-hot dodamy prostą dwukierunkową sieć LSTM.

```{r}
inputs  <- layer_input(shape(NULL), dtype = "int64")
embedded <- tf$one_hot(inputs, depth = as.integer(max_tokens))
outputs <- embedded %>%
  bidirectional(layer_lstm(units = 32)) %>%
  layer_dropout(.5) %>%
  layer_dense(1, activation = "sigmoid")

model <- keras_model(inputs, outputs)
model %>% compile(optimizer = "rmsprop",
                  loss = "binary_crossentropy",
                  metrics = "accuracy")
model

callbacks <- list(
  callback_model_checkpoint("models/one_hot_bidir_lstm.keras",
                            save_best_only = TRUE))
```

Podczas uczenia modelu można poczynić dwie obserwacje. Po pierwsze, model ten
trenuje się bardzo wolno, szczególnie w porównaniu do modeli z poprzedniego
podrozdziału. Wynika to z faktu, że nasze dane wejściowe są dość duże: każda
próbka wejściowa jest zakodowana jako macierz o rozmiarze (600, 20000) (600
słów na próbkę, 20 000 możliwych słów). To 12 000 000 wartości dla pojedynczej
recenzji filmu. Po drugie, model osiąga tylko 84,7% dokładności testowej, więc
nie radzi sobie tak dobrze, jak nasz najlepszy model.

```{r}
#| eval: false
# aby nie przekroczyć zasobów pamięci GPU zmnieszamy wielkość paczek do 16
int_train_ds_smaller <- int_train_ds %>%
  dataset_unbatch() %>%
  dataset_batch(16)

model %>% fit(int_train_ds_smaller, validation_data = int_val_ds,
              epochs = 10, callbacks = callbacks)
```

```{r}
#| eval: false
# predykcja trwa długo dlatego nie pozwalam na wywołanie tego chunk-a
model <- load_model_tf("models/one_hot_bidir_lstm.keras")
sprintf("Test acc: %.3f", evaluate(model, int_test_ds)["accuracy"])
```

Najwyraźniej użycie kodowania one-hot do przekształcenia słów w wektory, nie
było dobrym pomysłem. Jest lepszy sposób - osadzanie słów.

## Osadzenia

Kiedy kodujemy coś za pomocą kodowania one-hot, podejmujemy decyzję o
inżynierii cech. Wprowadzamy do naszego modelu fundamentalne założenie
dotyczące struktury przestrzeni cech. Założeniem tym jest to, że różne tokeny,
które kodujemy, są od siebie niezależne: w rzeczywistości wektory one-hot są
względem siebie ortogonalne. W przypadku słów założenie to jest oczywiście
błędne. Słowa tworzą ustrukturyzowaną przestrzeń: dzielą się ze sobą
informacjami. Słowa "movie" i "film" są wymienne w większości zdań, więc wektor
reprezentujący "movie" nie powinien być ortogonalny do wektora reprezentującego
"film" - powinny być tym samym wektorem lub bardzo zbliżonym.

Podchodząc do sprawy niecobardziej abstrakcyjnie, geometryczna relacja między
dwoma wektorami słów powinna odzwierciedlać semantyczną relację między tymi
słowami. Na przykład, w rozsądnej przestrzeni wektorów słów można oczekiwać, że
synonimy będą osadzone w podobnych wektorach słów, a ogólnie rzecz biorąc,
można oczekiwać, że odległość geometryczna (taka jak odległość cosinusowa lub
odległość L2) między dowolnymi dwoma wektorami słów będzie odnosić się do
"odległości semantycznej" między powiązanymi słowami. Słowa, które oznaczają
różne pojęcia, powinny znajdować się daleko od siebie, podczas gdy podobne
słowa powinny znajdować się bliżej.

Osadzenia słów to wektorowe reprezentacje słów, które osiągają ten cel: mapują
ludzki język na ustrukturyzowaną przestrzeń geometryczną. Podczas gdy wektory
uzyskane w wyniku kodowania one-hot są binarne, rzadkie (w większości składają
się z zer) i wielowymiarowe (taka sama wymiarowość jak liczba słów w słowniku),
osadzenia słów są niskowymiarowymi wektorami typu *float* (tj. gęstymi
wektorami, w przeciwieństwie do rzadkich wektorów) - patrz @fig-embd1. W
przypadku bardzo dużych słowników często spotyka się osadzenia słów, które są
256-wymiarowe, 512-wymiarowe lub 1024-wymiarowe. Z drugiej strony, kodowanie
słów metodą *one-hot* zazwyczaj prowadzi do wektorów, które mają 20 000
wymiarów lub więcej (słownik składający się z 20,000 tokenów). Tak więc
osadzanie słów zawiera więcej informacji w znacznie mniejszej liczbie wymiarów.

![](images/Zrzut%20ekranu%202023-03-16%20o%2016.42.03.png){#fig-embd1
fig-align="center" width="500"}

Oprócz tego, że są gęstymi reprezentacjami, osadzenia słów są również
reprezentacjami strukturalnymi, a ich struktura jest uczona na podstawie
danych. Podobne słowa są osadzone w bliskich lokalizacjach, a ponadto określone
kierunki w przestrzeni osadzania mają znaczenie. Aby to wyjaśnić, spójrzmy na
konkretny przykład.

![Osadzenie
słów](images/Zrzut%20ekranu%202024-02-9%20o%2022.26.38.png){#fig-vectors
fig-align="center" width="250"}

Na @fig-vectors cztery słowa są osadzone na płaszczyźnie 2D: cat, dog, wolf i
tiger. Dzięki reprezentacjom wektorowym, które tutaj wybraliśmy, niektóre
relacje semantyczne między tymi słowami można zakodować jako transformacje
geometryczne. Na przykład ten sam wektor pozwala nam przejść od kota do tygrysa
i od psa do wilka: wektor ten można interpretować jako wektor "od zwierzęcia
domowego do dzikiego zwierzęcia". Podobnie, inny wektor pozwala nam przejść od
psa do kota i od wilka do tygrysa, co można interpretować jako wektor "od psa
do kota".

W rzeczywistych przestrzeniach osadzania słów, typowymi przykładami znaczących
transformacji geometrycznych są wektory "płci" i wektory "liczby mnogiej". Na
przykład, dodając wektor "żeński" do wektora "król", otrzymujemy wektor
"królowa". Dodając wektor "liczby mnogiej", otrzymujemy "królów". Przestrzenie
osadzania słów zazwyczaj zawierają tysiące takich interpretowalnych i
potencjalnie użytecznych wektorów.

Jeszcze inny przykład osadzenia słów można dostrzec na @fig-embd2

![Przykład osadzenia
słów](https://editor.analyticsvidhya.com/uploads/450121_sAJdxEsDjsPMioHyzlN3_A.png){#fig-embd2
fig-align="center" width="600"}

Istnieją dwa sposoby na uzyskanie osadzenia słów:

-   Uczenie się embeddingów wspólnie z głównym zadaniem (takim jak klasyfikacja
    dokumentów lub przewidywanie sentymentu). W tej konfiguracji zaczynamy od
    losowych wektorów słów, a następnie uczymy się wektorów słów w taki sam
    sposób, w jaki uczymy się wag sieci neuronowej.
-   Wczytanie do modelu osadzenia słów, które zostały wstępnie wytrenowane przy
    użyciu innego zadania uczenia maszynowego niż to, które próbujemy
    rozwiązać. Są to tzw. wstępnie wytrenowane osadzenia słów.

Przyjrzyjmy się obu tym metodom.

### Osadzenie połączone z siecią

Czy istnieje jakaś idealna przestrzeń słowotwórcza, która doskonale
odwzorowywałaby ludzki język i mogłaby być wykorzystana do każdego zadania
związanego z przetwarzaniem języka naturalnego? Możliwe, ale nie udało nam się
jeszcze takiej znaleźć. Nie ma też czegoś takiego jak język ludzki - istnieje
wiele różnych języków i nie są one izomorficzne, ponieważ język jest
odzwierciedleniem konkretnej kultury i konkretnego kontekstu. Bardziej
pragmatycznie możemy stwierdzić, że to co czyni przestrzeń osadzania słów
dobrą, zależy w dużej mierze od zadania: idealna przestrzeń osadzania słów dla
anglojęzycznego modelu analizy sentymentów w recenzji filmowej może wyglądać
inaczej niż idealna przestrzeń osadzania dla anglojęzycznego modelu
klasyfikacji dokumentów prawnych, ponieważ znaczenie pewnych relacji
semantycznych różni się w zależności od zadania.

Dlatego rozsądne jest uczenie się nowej przestrzeni osadzania z każdym nowym
zadaniem. Na szczęście wsteczna propagacja to ułatwia, a pakiet `keras` czyni
to jeszcze łatwiejszym. Chodzi o uczenie wag warstwy za pomocą
`layer_embedding`.

```{r}
embedding_layer <- layer_embedding(input_dim = max_tokens, output_dim = 256)
```

Funkcja `layer_embedding()` jest najlepiej rozumiana jako słownik, który mapuje
indeksy liczb całkowitych (które oznaczają określone słowa) na wektory gęste.
Przyjmuje liczby całkowite jako dane wejściowe, wyszukuje tych liczb
całkowitych w wewnętrznym słowniku i zwraca powiązane wektory.

Warstwa osadzania przyjmuje jako dane wejściowe tensor liczb całkowitych, o
kształcie (`batch_size, sequence_length`), gdzie każdy wpis jest sekwencją
liczb całkowitych. Następnie warstwa zwraca tensor zmiennoprzecinkowy 3D o
kształcie (`batch_size, sequence_length, embedding_dimensionality`).

Po utworzeniu instancji `layer_embedding()`, jej wagi (wewnętrzny słownik
wektorów tokenów) są początkowo losowe, tak jak w przypadku każdej innej
warstwy. Podczas uczenia te wektory słów są stopniowo dostosowywane za pomocą
wstecznej propagacji, strukturyzując przestrzeń w coś, co może wykorzystać
dalszy model. Po pełnym wytrenowaniu, przestrzeń osadzania będzie wykazywać
dużą strukturę - rodzaj struktury wyspecjalizowanej dla konkretnego problemu,
dla którego trenujemy nasz model.

Zbudujmy model zawierający funkcję `layer_embedding()` i przetestujmy go w
naszym zadaniu.

```{r}
inputs <- layer_input(shape(NA), dtype = "int64")
embedded <- inputs %>%
  layer_embedding(input_dim = max_tokens, output_dim = 256)
outputs <- embedded %>%
  bidirectional(layer_lstm(units = 32)) %>%
  layer_dropout(0.5) %>%
  layer_dense(1, activation = "sigmoid")
model <- keras_model(inputs, outputs)
model %>%
  compile(optimizer = "rmsprop",
          loss = "binary_crossentropy",
          metrics = "accuracy")
model

callbacks = list(callback_model_checkpoint("models/embeddings_bidir_lstm",
                                           save_best_only = TRUE))
```

```{r}
#| eval: false
model %>%
  fit(int_train_ds,
      validation_data = int_val_ds,
      epochs = 10,
      callbacks = callbacks)
```

```{r}
model <- load_model_tf("models/embeddings_bidir_lstm")
evaluate(model, int_test_ds)["accuracy"] %>%
  sprintf("Test acc: %.3f\n", .) %>% cat("\n")
```

Model ten uczy się znacznie szybciej niż model *one-hot* (ponieważ LSTM musi
przetwarzać tylko 256-wymiarowe wektory zamiast 20 000-wymiarowych), a jego
dokładność na zbiorze testowym jest porównywalna (86,4%). Jednak wciąż jesteśmy
daleko od wyników naszego podstawowego modelu bigramowego. Częściowo wynika to
z faktu, że model ten analizuje nieco mniej danych: model bigramowy przetwarza
pełne recenzje, podczas gdy nasz model sekwencji obcina sekwencje po 600
słowach.

Jedną z rzeczy, która nieco obniża wydajność modelu, jest to, że nasze
sekwencje wejściowe są pełne zer. Wynika to z naszego użycia opcji
`output_sequence_length = max_length` w `layer_text_vectorization()` (z
`max_length` równą 600) - zdania dłuższe niż 600 tokenów są obcinane do
długości 600 tokenów, a zdania krótsze niż 600 tokenów są wypełniane zerami na
końcu, aby można je było połączyć z innymi sekwencjami w celu utworzenia
ciągłych partii.

Używamy dwukierunkowej sieci RNN - dwie warstwy RNN działające równolegle, z
których jedna przetwarza tokeny w ich naturalnej kolejności, a druga przetwarza
te same tokeny w odwrotnej kolejności. RNN, która patrzy na tokeny w ich
naturalnej kolejności, poświęci swoje ostatnie iteracje na "oglądaniu" tylko
tych wektory, które kodują *padding* - prawdopodobnie przez kilkaset iteracji,
jeśli oryginalne zdanie było krótkie. Informacje przechowywane w wewnętrznym
stanie RNN będą stopniowo zanikać, gdy będą narażone na bezsensowne dane
wejściowe.

Potrzebujemy zatem jakiegoś sposobu, aby przekazać RNN, że powinna pominąć te
iteracje. Jest do tego API - maskowanie. `layer_embedding()` jest w stanie
wygenerować "maskę", która odpowiada danym wejściowym. Maska ta jest tensorem
jedynek i zer (lub `TRUE/FALSE`), o kształcie (`batch_size, sequence_length`),
gdzie wpis `mask[i, t]` odpowiada, czy krok czasowy `t` próbki `i` powinien
zostać pominięty, czy nie (krok czasowy zostanie pominięty, jeśli `mask[i, t]`
ma wartość 0 lub FALSE, i przetworzony w przeciwnym razie).

Domyślnie opcja ta nie jest aktywna - można ją włączyć, przekazując
`mask_zero = TRUE` do funkcji `layer_embedding()`. Maskę można pobrać za pomocą
metody `compute_mask()`:

```{r}
embedding_layer <- layer_embedding(input_dim = 10, output_dim = 256,
                                   mask_zero = TRUE)
some_input <- rbind(c(4, 3, 2, 1, 0, 0, 0),
                    c(5, 4, 3, 2, 1, 0, 0),
                    c(2, 1, 0, 0, 0, 0, 0))
mask <- embedding_layer$compute_mask(some_input)
mask
```

W praktyce prawie nigdy nie będziemy musieli ręcznie zarządzać maskami. Zamiast
tego, `keras` automatycznie przekaże maskę do każdej warstwy, która jest w
stanie ją przetworzyć (jako metadane dołączone do sekwencji, którą
reprezentuje). Maska ta będzie używana przez warstwy RNN do pomijania
zamaskowanych kroków. Jeśli model zwraca całą sekwencję, maska będzie również
używana przez funkcję straty do pomijania zamaskowanych kroków w sekwencji
wyjściowej. Spróbujmy przeformułować nasz model na model z włączoną funkcją
maskowania.

```{r}
inputs <- layer_input(c(NA), dtype = "int64")
embedded <- inputs %>%
  layer_embedding(input_dim = max_tokens,
                  output_dim = 256,
                  mask_zero = TRUE)

outputs <- embedded %>%
  bidirectional(layer_lstm(units = 32)) %>%
  layer_dropout(0.5) %>%
  layer_dense(1, activation = "sigmoid")

model <- keras_model(inputs, outputs)
model %>% compile(optimizer = "rmsprop",
                  loss = "binary_crossentropy",
                  metrics = "accuracy")
model

callbacks = list(
  callback_model_checkpoint("models/embeddings_bidir_lstm_with_masking",
                            save_best_only = TRUE)
)
```

```{r}
#| eval: false
model %>% fit(
  int_train_ds,
  validation_data = int_val_ds,
  epochs = 10,
  callbacks = callbacks
)
```

```{r}
#| eval: false
model <- load_model_tf("models/embeddings_bidir_lstm_with_masking")
cat(sprintf("Test acc: %.3f\n",
            evaluate(model, int_test_ds)["accuracy"]))
```

Tym razem osiągnęliśmy 86,8% dokładności na zbiorze testowym - niewielka, ale
zauważalna poprawa.

### Użycie wstępnie wytrenowanego osadzenia

Czasami dostępnych jest tak mało danych szkoleniowych, aby użyć ich do
nauczenia się odpowiedniego, specyficznego dla danego zadania osadzenia
słownictwa. W takich przypadkach, zamiast uczyć osadzania słów razem z głównym
problemem, który chcemy rozwiązać, możemy załadować wektory osadzania ze
wstępnie obliczonej przestrzeni osadzania, o której wiemy, że jest wysoce
ustrukturyzowana i wykazuje użyteczne właściwości - takie, które wychwytują
ogólne aspekty struktury języka. Uzasadnienie używania wstępnie wytrenowanych
osadzeń słów w przetwarzaniu języka naturalnego jest takie samo, jak w
przypadku używania wstępnie wytrenowanych sieci konwolucyjnych w klasyfikacji
obrazów: nie mamy wystarczającej ilości danych, aby samodzielnie nauczyć się
odpowiednich funkcji, ale spodziewamy się, że funkcje, których potrzebujemy, są
dość ogólne - to znaczy, że mają wspólne cechy wizualne lub cechy semantyczne.
W takim przypadku sensowne jest ponowne wykorzystanie cech wyuczonych dla
innego problemu.

Takie osadzenia słów są zazwyczaj tworzone przy użyciu statystyk występowania
słów (obserwacje dotyczące tego, jakie słowa współwystępują w zdaniach lub
dokumentach), przy użyciu różnych technik, z których niektóre obejmują sieci
neuronowe, a inne nie. Idea gęstej, niskowymiarowej przestrzeni osadzania słów,
obliczanej w sposób nienadzorowany, została początkowo zbadana przez Bengio i
in. na początku XXI wieku [@bengioNeuralProbabilisticLanguage], ale zaczęła się
rozwijać w badaniach i zastosowaniach przemysłowych dopiero po wydaniu jednego
z najbardziej znanych i udanych schematów osadzania słów: algorytmu Word2Vec
(<https://code.google.com/archive/p/word2vec>), opracowanego przez Tomasa
Mikolova w Google w 2013 roku. Osadzenie Word2Vec wychwytuje specyficzne
właściwości semantyczne, takie jak płeć.

Można pobrać różne wstępnie wytrenowane bazy danych osadzania słów i użyć ich w
funkcji Keras `layer_embedding()`. Word2Vec jest jedną z nich. Inną popularną
wersją osadzenia słów jest Global Vectors for Word Representation (GloVe,
<https://nlp.stanford.edu/projects/glove>), która została opracowana przez
naukowców ze Stanford w 2014 roku. Ta technika osadzania opiera się na
faktoryzacji macierzy statystyk współwystępowania słów. Jej twórcy udostępnili
wstępnie obliczone osadzenia dla milionów angielskich tokenów, uzyskanych z
Wikipedii i danych Common Crawl[^llm-5].

Przyjrzyjmy się, jak można rozpocząć korzystanie z GloVe embeddings w modelu
Keras. Ta sama metoda jest odpowiednia dla osadzeń Word2Vec lub dowolnej innej
bazy danych osadzeń słów. Zaczniemy od pobrania plików GloVe i przeanalizowania
ich. Następnie załadujemy wektory słów do warstwy Keras `layer_embedding()`,
której użyjemy do zbudowania nowego modelu.

Najpierw pobierzmy osadzenia słów GloVe wstępnie obliczone na zbiorze danych
angielskiej Wikipedii z 2014 roku. Jest to plik zip o rozmiarze 822 MB
zawierający 100-wymiarowe wektory osadzania dla 400 000 słów (lub tokenów
niebędących słowami):

```{r}
#| eval: false
download.file("http://nlp.stanford.edu/data/glove.6B.zip",
              destfile = "glove.6B.zip")
zip::unzip("glove.6B.zip")
```

Przeanalizujmy rozpakowany plik (plik .txt), aby utworzyć indeks, który mapuje
słowa (jako ciągi) na ich reprezentację wektorową.

```{r}
path_to_glove_file <- "~/Downloads/glove.6B/glove.6B.100d.txt"
embedding_dim <- 100

df <- readr::read_table(
  path_to_glove_file,
  col_names = FALSE,
  col_types = paste0("c", strrep("n", 100))
)
embeddings_index <- as.matrix(df[, -1])
rownames(embeddings_index) <- df[[1]]
colnames(embeddings_index) <- NULL
rm(df)
```

Oto jak wygląda `embedding_matrix`:

```{r}
str(embeddings_index)
```

Następnie zbudujmy macierz osadzania, którą można załadować do funkcji
`layer_embedding()`, Musi to być macierz o kształcie
(`max_words, embedding_dim`), gdzie każdy wpis i zawiera wektor wymiaru
`embedding_dim` dla słowa o indeksie `i` w indeksie słowa referencyjnego
(zbudowanym podczas tokenizacji).

```{r}
vocabulary <- text_vectorization %>% get_vocabulary()
str(vocabulary)

tokens <- head(vocabulary[-1], max_tokens)

i <- match(vocabulary, rownames(embeddings_index),
           nomatch = 0)

embedding_matrix <- array(0, dim = c(max_tokens, embedding_dim))
embedding_matrix[i != 0, ] <- embeddings_index[i, ]
str(embedding_matrix)
```

Na koniec użyjemy funkcji `initializer_constant()`, aby załadować wstępnie
wytrenowane osadzenia do `layer_embedding()`. Aby nie zakłócać wstępnie
wytrenowanych reprezentacji podczas szkolenia, zamrażamy warstwę za pomocą
`trainable = FALSE`:

```{r}
embedding_layer <- layer_embedding(
  input_dim = max_tokens,
  output_dim = embedding_dim,
  embeddings_initializer = initializer_constant(embedding_matrix),
  trainable = FALSE,
  mask_zero = TRUE
)
```

Jesteśmy teraz gotowi do trenowania nowego modelu - identycznego z naszym
poprzednim modelem, ale wykorzystującego 100-wymiarowe wstępnie wytrenowane
osadzenia GloVe zamiast 128-wymiarowych wyuczonych osadzeń.

```{r}
inputs <- layer_input(shape(NA), dtype="int64")
embedded <- embedding_layer(inputs)
outputs <- embedded %>%
  bidirectional(layer_lstm(units = 32)) %>%
  layer_dropout(0.5) %>%
  layer_dense(1, activation = "sigmoid")
model <- keras_model(inputs, outputs)

model %>% compile(optimizer = 'rmsprop',
                  loss = "binary_crossentropy",
                  metrics = "accuracy")
model

callbacks <- list(callback_model_checkpoint("models/glove_embeddings_sequence_model2",
                            save_best_only = TRUE))

```

```{r}
#| eval: false
model %>%
  fit(int_train_ds, validation_data = int_val_ds,
      epochs = 10, callbacks = callbacks)
```

```{r}
model <- load_model_tf("models/glove_embeddings_sequence_model2")
cat(sprintf(
  "Test acc: %.3f\n", evaluate(model, int_test_ds)["accuracy"]))
```

Przekonaliśmy się, że w tym konkretnym zadaniu wstępnie wytrenowane osadzenia
nie są zbyt pomocne (dopasowanie na zbiorze testowym na poziomie 68,4%),
ponieważ zbiór danych zawiera wystarczającą liczbę próbek, aby można było
nauczyć się od podstaw wystarczająco wyspecjalizowanej przestrzeni osadzania.
Jednak wykorzystanie wstępnie wytrenowanych osadzeń może być bardzo pomocne,
gdy pracujesz z mniejszym zbiorem danych.

[^llm-5]: Common Crawl to non-profit organizacja udostępniająca darmowe,
    ogromne archiwa danych internetowych do wykorzystania w badaniach, analizie
    i projektach z zakresu technologii informatycznych.

## Transformery

Począwszy od 2017 roku, nowa architektura modelu zaczęła wyprzedzać
rekurencyjne sieci neuronowe w większości zadań przetwarzania języka
naturalnego. Jest nią transformer lub transformator (ang. *transformer*).
Transformery zostały wprowadzone w przełomowym artykule "Attention Is All You
Need" autorstwa @vaswaniAttentionAllYou2023. Sedno artykułu znajduje się w
tytule: jak się okazało, prosty mechanizm zwany "uwagą neuronową" (ang. *neural
attention*) można wykorzystać do zbudowania potężnych modeli sekwencji, które
nie zawierają żadnych warstw rekurencyjnych ani warstw splotowych.

Odkrycie to zapoczątkowało rewolucję w przetwarzaniu języka naturalnego i nie
tylko. Uwaga neuronowa szybko stała się jedną z najbardziej wpływowych idei w
uczeniu głębokim. W tym podrozdziale przybliżymy jak to działa i dlaczego
okazało się tak skuteczne w przypadku danych sekwencyjnych. Następnie
wykorzystamy samouczenie się do stworzenia kodera Transformer, jednego z
podstawowych komponentów architektury Transformer i zastosujemy go do zadania
klasyfikacji recenzji filmów IMDB.

### Warstwy atencji

Czytając tę książkę, możesz jedynie przeglądać niektóre jej części, a inne
uważnie czytać, w zależności od tego, jakie są twoje cele lub zainteresowania.
Co by było, gdyby modele robiły to samo? Wykorzystamy prosty pomysł: nie
wszystkie informacje wejściowe widziane przez model są równie ważne dla danego
zadania, więc modele powinny "zwracać większą uwagę" na niektóre funkcje i
"zwracać mniejszą uwagę" na inne funkcje. Czy to brzmi znajomo? Z podobną
koncepcją spotkałeś się już dwukrotnie:

-   *Max pooling* w sieciach splotowych patrzy na pulę cech w regionie
    przestrzennym i wybiera tylko jedną cechę do zachowania. Jest to forma
    uwagi "wszystko albo nic" - zachowaj najważniejszą cechę i odrzuć resztę.
-   Normalizacja TF-IDF przypisuje tokenom współczynnik ważności w oparciu o
    to, ile informacji mogą przenosić różne tokeny. Ważne tokeny są wzmacniane,
    podczas gdy nieistotne tokeny są wygaszane. Jest to ciągła forma uwagi.

Istnieje wiele różnych form uwagi, które można sobie wyobrazić, ale wszystkie
zaczynają się od znalezienia wskaźnika uwagi dla zestawu cech, z wyższymi
wskaźnikami dla bardziej istotnych cech i niższymi dla mniej istotnych (patrz
@fig-attention1). Sposób obliczania tych wskaźników i to, co należy z nimi
zrobić, będzie się różnić w zależności od podejścia.

![Ogólna koncepcja atencji w uczeniu głębokim: Cechom wejściowym przypisywane
są wskaźniki uwagi, które można przekazać do następnej reprezentacji danych
wejściowych.](images/Zrzut%20ekranu%202024-02-10%20o%2013.57.40.png){#fig-attention1
fig-align="center" width="500"}

Co najważniejsze, ten rodzaj mechanizmu uwagi może być wykorzystywany do czegoś
więcej niż tylko podkreślania lub wymazywania pewnych cech. Można go
wykorzystać do uświadomienia kontekstu funkcji. Właśnie dowiedzieliśmy się o
osadzaniu słów, czyli przestrzeniach wektorowych, które przechwytują
"strukturę" relacji semantycznych między różnymi słowami. W przestrzeni
osadzania pojedyncze słowo ma stałą pozycję - stały zestaw relacji z każdym
innym słowem w przestrzeni. Ale nie do końca tak działa język - znaczenie słowa
jest zwykle zależne od kontekstu. Kiedy zaznaczamy datę w kalendarzu (ang.
*date*), nie mówimy o "randce". Kiedy mówimy "I'll see you soon", znaczenie
słowa "see" jest subtelnie inne niż "see" w "I see what you mean" lub "I’ll see
this project to its end". I oczywiście znaczenie zaimków takich jak "he", "it",
"you" i tak dalej jest całkowicie zależne od zdania i mogą zmieniać się
wielokrotnie w jednym zdaniu.

Oczywiście inteligentna przestrzeń osadzania zapewniłaby inną reprezentację
wektorową dla słowa w zależności od innych otaczających go słów. W tym miejscu
pojawia się samo-atencja. Celem samo-uwagi jest modulowanie reprezentacji
tokena poprzez wykorzystanie reprezentacji powiązanych tokenów w sekwencji. W
ten sposób powstają reprezentacje tokenów świadome kontekstu. Rozważmy
przykładowe zdanie: "The train left the station on time". Rozważmy teraz jedno
słowo w zdaniu: *station*. O jakiej stacji mówimy? Czy może to być stacja
radiowa? Może Międzynarodowa Stacja Kosmiczna? Rozważmy to algorytmicznie za
pomocą samo-atencji (patrz @fig-attention2)

![Samo-atencja. Wskaźniki uwagi są obliczane między "station" a każdym innym
słowem w sekwencji, a następnie są używane do ważenia sumy wektorów słów, które
stają się nowym wektorem
"station"](images/Zrzut%20ekranu%202024-02-10%20o%2014.08.20.png){#fig-attention2
fig-align="center" width="600"}

Krok 1 polega na obliczeniu wskaźników trafności między wektorem dla "station"
a każdym innym słowem w zdaniu. Są to nasze "wskaźniki uwagi". Zamierzamy
zastosować iloczyn skalarny pomiędzy dwoma wektorami słów jako miary siły ich
związku. Jest to bardzo wydajna obliczeniowo funkcja odległości i była już
standardowym sposobem powiązania ze sobą dwóch osadzeń słów na długo przed
transformerami. W praktyce wyniki te będą również przechodzić przez funkcję
skalowania i softmax, ale na razie jest to tylko szczegół implementacji.

Krok 2 polega na obliczeniu sumy wszystkich wektorów słów w zdaniu, ważonych
przez nasze wskaźniki atencji. Słowa blisko związane ze słowem "station" będą
miały większy udział w sumie (w tym samo słowo "station"), podczas gdy słowa
nieistotne nie wniosą prawie nic. Wynikowy wektor jest naszą nową reprezentacją
dla "station", reprezentacją, która uwzględnia otaczający kontekst. W
szczególności zawiera ona część wektora "train", wyjaśniając, że w
rzeczywistości jest to "train station".

Powtarzamy ten proces dla każdego słowa w zdaniu, tworząc nową sekwencję
wektorów kodujących zdanie. Zobaczmy to w pseudokodzie R:

```{r}
self_attention <- function(input_sequence) {
  c(sequence_len, embedding_size) %<-% dim(input_sequence)

  output <- array(0, dim(input_sequence))

  for (i in 1:sequence_len) {

    pivot_vector <- input_sequence[i, ]

    scores <- sapply(1:sequence_len, function(j)
      pivot_vector %*% input_sequence[j, ])

    scores <- softmax(scores / sqrt(embedding_size))

    broadcast_scores <- as.matrix(scores)[,rep(1, embedding_size)]

    new_pivot_representation <- colSums(input_sequence * broadcast_scores)

    output[i, ] <- new_pivot_representation
  }

  output
}

softmax <- function(x) {
   e <- exp(x - max(x))
   e / sum(e)
}
```

```{r}
sequence_length <- 20
embed_dim <- 256
inputs <- layer_input(c(sequence_length, embed_dim))
```

Oczywiście, w praktyce można użyć implementacji zwektoryzowanej. Keras ma
wbudowaną warstwę do obsługi tego - `layer_multi_head_attention()`. Oto jak
można jej użyć:

```{r}
num_heads <- 4
embed_dim <- 256

mha_layer <- layer_multi_head_attention(num_heads = num_heads,
                                        key_dim = embed_dim)
outputs <- mha_layer(inputs, inputs, inputs)
```

Czytając to, prawdopodobnie zastanawiasz się: dlaczego przekazujemy dane
wejściowe do warstwy trzy razy? Wydaje się to zbędne. Czym są te "wielokrotne
głowy"? Oba te pytania mają proste odpowiedzi.

### Uogólnienie samo-atencji

Do tej pory rozważaliśmy tylko jedną sekwencję wejściową. Jednak architektura
transformera została pierwotnie opracowana dla tłumaczenia maszynowego, gdzie
mamy do czynienia z dwiema sekwencjami wejściowymi: sekwencją źródłową, którą
aktualnie tłumaczysz (np. "How’s the weather today?") i sekwencją docelową, na
którą ją konwertujesz (np. "¿Qué tiempo hace hoy?"). Transformer jest modelem
dziłającym od sekwencji do sekwencji. Został zaprojektowany do konwersji jednej
sekwencji na inną.

Teraz cofnijmy się o krok. Mechanizm samo-uwagi, tak jak go przedstawiliśmy,
wykonuje następujące czynności, schematycznie:

![Mechanizm
samo-uwagi](images/Zrzut%20ekranu%202024-02-10%20o%2014.23.50.png){#fig-attention3
fig-align="center" width="500"}

Oznacza to, że "dla każdego tokena w wejściach (A) obliczamy, jak bardzo token
jest powiązany z każdym tokenem w wejściach (B) i używamy tych wyników do
ważenia sumy tokenów z wejść (C)". Co najważniejsze, nie wymgamy, aby A, B i C
odnosiły się do tej samej sekwencji wejściowej. W ogólnym przypadku można to
zrobić z trzema różnymi sekwencjami. Nazwiemy je "zapytaniem" (ang. *query*),
"kluczami" (ang. *key*) i "wartościami" (ang. *value*). Operacja ta zamienia
się w "dla każdego elementu w zapytaniu oblicz, jak bardzo element jest
powiązany z każdym kluczem i użyj tych wyników do ważenia sumy wartości":

```{r}
#| eval: false
outputs <- sum( values * pairwise_scores( query, keys ))
```

Terminologia ta pochodzi z wyszukiwarek i systemów rekomendacji (patrz
@fig-attention4). Wyobraźmy sobie, że wpisujemy zapytanie, aby pobrać zdjęcie
ze swojej kolekcji, "dogs on the beach". Wewnętrznie, każde z naszych zdjęć w
bazie danych jest opisane przez zestaw słów kluczowych - "cat", "dog", "party"
i tak dalej. Nazwiemy je "kluczami". Wyszukiwarka rozpocznie od porównania
zapytania z kluczami w bazie danych. "dog" daje dopasowanie 1 dla naszego
wyszukiwania, a "cat" daje dopasowanie 0. Następnie algorytm szereguje te
klucze według siły dopasowania - trafności - i zwraca zdjęcia powiązane z $N$
najlepszymi dopasowaniami, w kolejności trafności.

![Pobieranie obrazów z bazy danych. "Zapytanie" jest porównywane z zestawem
"kluczy", a wyniki dopasowania są wykorzystywane do uszeregowania "wartości"
(obrazów)](images/Zrzut%20ekranu%202024-02-10%20o%2014.30.59.png){#fig-attention4
fig-align="center" width="500"}

Chcąc wyrazić dokładnie wskaźniki atencji wzorem matematycznym, napisalibyśmy:

$$
\operatorname{AS}(Q,K,V)=\operatorname{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V,
$$

gdzie $Q,K,V$ oznaczają odpowiednio query, key i value, $d_k$ oznacza długość
reprezentacji tokenu w osadzeniu.

Koncepcyjnie, to właśnie te czynności wykonuje warstwa atencji w
transformerach. Mamy sekwencję referencyjną, która opisuje coś, czego szukamy:
zapytanie. Mamy zbiór wiedzy, z którego próbujemy wydobyć informacje: wartości.
Każda wartość ma przypisany klucz, który opisuje wartość w formacie, który
można łatwo porównać z zapytaniem. Wystarczy dopasować zapytanie do kluczy.
Następnie zwracana jest ważona suma wartości.

W praktyce klucze i wartości to często ta sama sekwencja. Na przykład w
tłumaczeniu maszynowym zapytanie byłoby sekwencją docelową, a sekwencja
źródłowa odgrywałaby rolę zarówno kluczy, jak i wartości: dla każdego elementu
celu (takiego jak "tiempo") chcemy wrócić do źródła ("How’s the weather
today?") i zidentyfikować różne bity, które są z nim powiązane ("tiempo" i
"weather" powinny mieć silne dopasowanie). Oczywiście, jeśli wykonujemy tylko
klasyfikację sekwencji, to zapytanie, klucze i wartości są takie same:
porównujemy sekwencję z samą sobą, aby wzbogacić każdy token o kontekst z całej
sekwencji.

To wyjaśnia, dlaczego musieliśmy przekazać dane wejściowe trzy razy do naszej
warstwy `layer_multi_head_attention()`. Ale po co atencja typu *multi-head*?

### Atencja typu *multi-head*

*Multi-head attention* to dodatkowe ulepszenie mechanizmu samo-uwagi,
wprowadzone w "Attention Is All You Need". Przydomek *multi-head* odnosi się do
faktu, że przestrzeń wyjściowa warstwy samo-uwagi jest podzielona na zestaw
niezależnych podprzestrzeni, uczonych oddzielnie: początkowe zapytanie, klucz i
wartość są wysyłane przez trzy niezależne zestawy gęstych projekcji, co
skutkuje trzema oddzielnymi wektorami. Każdy wektor jest przetwarzany przez
warstwę atencji, a trzy wyjścia są łączone z powrotem w jedną sekwencję
wyjściową. Każda taka podprzestrzeń nazywana jest "głową". Pełny obraz został
przedstawiony na @fig-attention5.

![Multi-head
attention](images/Zrzut%20ekranu%202024-02-10%20o%2014.41.55.png){#fig-attention5
fig-align="center" width="600"}

Obecność gęstych projekcji, których można uczyć, pozwala warstwie faktycznie
się czegoś nauczyć, w przeciwieństwie do bycia czysto bezstanową transformacją,
która wymagałaby dodatkowych warstw przed lub po niej, aby była użyteczna.
Ponadto posiadanie niezależnych "głów" pomaga warstwie uczyć się różnych grup
cech dla każdego tokena, gdzie cechy w jednej grupie są ze sobą skorelowane,
ale są w większości niezależne od cech w innej grupie.

Jest to zasadniczo podobne do tego, co robią konwolucje separowalne. W
konwolucji separowalnej przestrzeń wyjściowa konwolucji jest podzielona na
wiele podprzestrzeni (po jednej na kanał wejściowy), które są uczone
niezależnie. Artykuł "Attention Is All You Need" został napisany w czasie, gdy
wykazano, że idea faktoryzacji przestrzeni cech na niezależne podprzestrzenie
zapewnia ogromne korzyści dla komputerowych modeli wizyjnych, zarówno w
przypadku konwolucji separowalnych, jak i w przypadku blisko spokrewnionego
podejścia, konwolucji grupowych. *Multi-head attention* jest po prostu
zastosowaniem tego samego pomysłu do samo-atencji.

### Enkoder transformera

Jeśli dodanie dodatkowych gęstych projekcji jest tak przydatne, dlaczego nie
zastosujemy jednej lub dwóch do wyjścia mechanizmu uwagi? Nasz model zaczyna
realizować wiele funkcji, więc możemy chcieć dodać połączenia resztkowe, aby
upewnić się, że po drodze nie utracimy żadnych cennych informacji. Dodatkowo
aby przyspieszyć proces uczenia można dodać warstwy normalizacji.

Taki proces myślowy, który, rozwijał się w umysłach twórców architektury
Transformer. Dzielenie danych wyjściowych na wiele niezależnych przestrzeni,
dodawanie połączeń resztkowych, dodawanie warstw normalizacji - wszystko to są
standardowe wzorce architektury, które warto wykorzystać w każdym złożonym
modelu.

![Transformer-Encoder łączy `layer_multi_head_attention()` z gęstą projekcją i
dodaje normalizację oraz połączenia
resztkowe](images/Zrzut%20ekranu%202024-02-11%20o%2010.51.31.png){#fig-tr1
fig-align="center" width="400"}

Oryginalna architektura Transformer składa się z dwóch części: kodera
transformera, który przetwarza sekwencję źródłową, oraz dekodera transformera,
który wykorzystuje sekwencję źródłową do wygenerowania przetłumaczonej wersji.
Co najważniejsze, część kodera może być używana do klasyfikacji tekstu. Jest to
bardzo ogólny moduł, który przyjmuje sekwencję i uczy się przekształcać ją w
bardziej użyteczną reprezentację. Zaimplementujmy koder Transformer (taki jak
na @fig-tr1) i wypróbujemy go w zadaniu klasyfikacji sentymentu recenzji
filmowej.

```{r}
layer_transformer_encoder <- new_layer_class(
  classname = "TransformerEncoder",
  initialize = function(embed_dim, dense_dim, num_heads, ...) {
    super$initialize(...)
    self$embed_dim <- embed_dim # rozmiar tokenów wejściowych
    self$dense_dim <- dense_dim # liczba neuronów w sieci gęstej
    self$num_heads <- num_heads # liczba głów w warstwie atencji
    self$attention <-
      layer_multi_head_attention(num_heads = num_heads,
                                 key_dim = embed_dim)

    self$dense_proj <- keras_model_sequential() %>%
      layer_dense(dense_dim, activation = "relu") %>%
      layer_dense(embed_dim)

    self$layernorm_1 <- layer_layer_normalization()
    self$layernorm_2 <- layer_layer_normalization()
  },

  call = function(inputs, mask = NULL) { # określenie wywołania
    if (!is.null(mask)) # dostosowanie rozmiaru maski z 2D do 3D lub 4D
      mask <- mask[, tf$newaxis, ]

    inputs %>%
      { self$attention(., ., attention_mask = mask) + . } %>% # połączenie rezydualne
      self$layernorm_1() %>%
      { self$dense_proj(.) + . } %>% # połączenie projekcji gęstej z rezydualną
      self$layernorm_2()
  },

  get_config = function() { # implementacja serializacji potrzebna do zapisu modelu
    config <- super$get_config()
    for(name in c("embed_dim", "num_heads", "dense_dim"))
      config[[name]] <- self[[name]]
    config
  }
)
```

::: callout-tip
Podczas pisania niestandardowych warstw należy zaimplementować metodę
`get_config()`. Umożliwia to ponowne utworzenie warstwy z jej konfiguracji, co
jest przydatne podczas zapisywania i ładowania modelu. Metoda powinna zwracać
nazwaną listę R, która zawiera wartości argumentów konstruktora użytych do
utworzenia warstwy.

Wszystkie warstwy Keras mogą być serializowane i deserializowane w następujący
sposób:

```{r}
#| eval: false
config <- layer$get_config()
new_layer <- do.call(layer_<type>, config)
```

gdzie `layer_<type>` jest oryginalnym konstruktorem warstwy. Na przykład:

```{r}
#| eval: false
layer <- layer_dense(units = 10)
config <- layer$get_config()
new_layer <- do.call(layer_dense, config)
```

Można również uzyskać dostęp do rozpakowanego oryginalnego konstruktora warstwy
z dowolnej istniejącej warstwy bezpośrednio za pomocą specjalnego symbolu
`__class__` (choć rzadko trzeba to robić):

```{r}
#| eval: false
layer$`__class__`
new_layer <- layer$`__class__`$from_config(config)
```

Zdefiniowanie metody `get_config()` w niestandardowych klasach warstw umożliwia
określenie przepływu pracy. Na przykład:

```{r}
#| eval: false
layer <- layer_transformer_encoder(embed_dim = 256, dense_dim = 32,
                                   num_heads = 2)
config <- layer$get_config()

new_layer <- do.call(layer_transformer_encoder, config)
# -- lub --
new_layer <- layer$`__class__`$from_config(config)
```

Podczas zapisywania modelu zawierającego niestandardowe warstwy, zapisany plik
będzie zawierał te konfiguracje. Podczas ładowania modelu z pliku należy
dostarczyć niestandardowe klasy warstw do procesu ładowania, aby mógł on
zrozumieć obiekty konfiguracyjne:

```{r}
#| eval: false
model <- save_model_tf(model, filename)
model <- load_model_tf(filename,
                       custom_objects = list(layer_transformer_encoder))
```

Zwróćmy uwagę, że jeśli lista dostarczona do `custom_objects` jest nazwana, to
nazwy są dopasowywane do argumentu `classname`, który został podany podczas
konstruowania niestandardowego obiektu:

```{r}
#| eval: false
model <- load_model_tf(
  filename,
  custom_objects = list(TransformerEncoder = layer_transformer_encoder))
```
:::

Zauważyliśmy już pewnie, że warstwy normalizacji, których tu używamy, nie są
`layer_batch_normalization()`, jak te, których używaliśmy wcześniej w modelach
do obrazów. To dlatego, że `layer_batch_normalization()` nie działa dobrze w
przypadku danych sekwencyjnych. Zamiast tego używamy
`layer_layer_normalization()`, która normalizuje każdą sekwencję niezależnie od
innych sekwencji w partii. Tak to wygląda w pseudokodzie R:

```{r}
layer_normalization <- function(batch_of_sequences) {
  c(batch_size, sequence_length, embedding_dim) %<-% dim(batch_of_sequences)
  means <- variances <-
    array(0, dim = dim(batch_of_sequences))
  for (b in seq(batch_size))
    for (s in seq(sequence_length)) { # obliczanie statystyk po ostatniej osi osadzeń
      embedding <- batch_of_sequences[b, s, ]
      means[b, s, ] <- mean(embedding)
      variances[b, s, ] <- var(embedding)
    }
  (batch_of_sequences - means) / variances
}
```

Można to porównać z klasyczną `layer_batch_normalization()`:

```{r}
batch_normalization <- function(batch_of_images) {
  c(batch_size, height, width, channels) %<-% dim(batch_of_images)
  means <- variances <-
    array(0, dim = dim(batch_of_images))
  for (ch in seq(channels)) { # statystyki liczone są po partiach
    channel <- batch_of_images[, , , ch] # dla każdego kanału liczone oddzielnie
    means[, , , ch] <- mean(channel)
    variances[, , , ch] <- var(channel)
  }
  (batch_of_images - means) / variances
}
```

Funkcja `batch_normalization()` zbiera informacje z wielu próbek w celu
uzyskania statystyk średnich i wariancji cech, a funkcja
`layer_normalization()` gromadzi dane w każdej sekwencji osobno, co jest
bardziej właściwe dla danych sekwencyjnych.

Teraz wykorzystamy zbudowany transformer do naszego zadania:

```{r}
vocab_size <- 20000
embed_dim <- 256
num_heads <- 2
dense_dim <- 32

inputs <- layer_input(shape(NA), dtype = "int64")
outputs <- inputs %>%
  layer_embedding(vocab_size, embed_dim) %>%
  layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
  layer_global_average_pooling_1d() %>%
  layer_dropout(0.5) %>%
  layer_dense(1, activation = "sigmoid")
model <-  keras_model(inputs, outputs)
model %>% compile(optimizer = "rmsprop",
                  loss = "binary_crossentropy",
                  metrics = "accuracy")
model

callbacks = list(callback_model_checkpoint("models/transformer_encoder",
                                           save_best_only = TRUE))
```

```{r}
#| eval: false

model %>% fit(
  int_train_ds,
  validation_data = int_val_ds,
  epochs = 20,
  callbacks = callbacks
)
```

```{r}
#| eval: false
model <- load_model_tf("models/transformer_encoder",
                       custom_objects = list(layer_transformer_encoder))

sprintf("Test acc: %.3f", evaluate(model, int_test_ds)["accuracy"])
```

Osiągamy dopasowanie na poziomie 88,8% dla zbioru testowego. W tym momencie
powinniśmy zacząć odczuwać lekki niepokój. Coś tu nie gra. Co jest nie tak?

Ten rozdział rzekomo dotyczy "modeli sekwencji". Zaczęliśmy od podkreślenia
znaczenia kolejności słów. Powiedzieliśmy, że transformer to architektura
przetwarzania sekwencji, pierwotnie opracowana na potrzeby tłumaczenia
maszynowego. A jednak... koder transformera, który właśnie zobaczyliśmy w
akcji, wcale nie był modelem sekwencyjnym. Składa się on z gęstych warstw,
które przetwarzają tokeny sekwencji niezależnie od siebie, oraz warstwy uwagi,
która traktuje tokeny jako zestaw. Możesz zmienić kolejność tokenów w
sekwencji, a otrzymamy dokładnie takie same wyniki wzajemnej uwagi i dokładnie
takie same reprezentacje kontekstu. Gdybyśmy całkowicie wymieszali słowa w
każdej recenzji filmu, model by tego nie zauważył i nadal uzyskałbyś dokładnie
taką samą dokładność. Samoatencja jest mechanizmem przetwarzania zbiorów,
skoncentrowanym na relacjach między parami elementów sekwencji (patrz
@tbl-features) - jest ślepa na to, czy elementy te występują na początku, na
końcu czy w środku sekwencji. Dlaczego więc mówimy, że Transformer jest modelem
sekwencji? I w jaki sposób może on być dobry dla tłumaczenia maszynowego, jeśli
nie bierze pod uwagę kolejności słów?

Wskazaliśmy rozwiązanie wcześniej w tym rozdziale. Mimochodem wspominaliśmy, że
transformer jest podejściem hybrydowym, które jest technicznie niezależne od
kolejności, ale ręcznie wprowadza informacje o kolejności do przetwarzanych
reprezentacji. To jest brakujący składnik! Nazywa się to kodowaniem pozycyjnym.
Przyjrzyjmy się temu.

|                 | Świadomość kolejności słów | Świadomość kontekstu |
|-----------------|----------------------------|----------------------|
| Bag-of-unigrams | Nie                        | Nie                  |
| Bag-of-bigrams  | Bardzo ograniczona         | Nie                  |
| RNN             | Tak                        | Nie                  |
| Self-attention  | Nie                        | Tak                  |
| Transformer     | Tak                        | Tak                  |

: Cechy modeli wykorzystywanych do NLP {#tbl-features}

### Kodowanie pozycyjne

Idea kodowania pozycyjnego jest bardzo prosta. Aby zapewnić modelowi dostęp do
informacji o kolejności słów, zamierzamy dodać pozycję słowa w zdaniu do
każdego osadzenia słowa. Nasze wejściowe osadzenia słów będą miały dwa
komponenty: zwykły wektor słów, który reprezentuje słowo niezależnie od
konkretnego kontekstu, oraz wektor pozycji, który reprezentuje pozycję słowa w
bieżącym zdaniu.

Najprostszym schematem, jaki można wymyślić, byłoby połączenie pozycji słowa z
jego wektorem osadzania. Dodalibyśmy oś "pozycja" do wektora i wypełnili ją
wartością 0 dla pierwszego słowa w sekwencji, 1 dla drugiego i tak dalej. Może
to jednak nie być idealne rozwiązanie, ponieważ pozycje mogą być potencjalnie
bardzo dużymi liczbami całkowitymi, co zakłóci zakres wartości w wektorze
osadzania. Jak wiadomo, sieci neuronowe nie lubią bardzo dużych wartości
wejściowych ani dyskretnych rozkładów danych wejściowych.

W oryginalnym artykule "Attention Is All You Need" zastosowano interesującą
sztuczkę do kodowania pozycji słów: dodano do osadzania słów wektor zawierający
wartości z zakresu \[-1, 1\], które zmieniały się cyklicznie w zależności od
pozycji (wykorzystano do tego funkcje cosinusowe). Ta sztuczka pozwala
jednoznacznie scharakteryzować dowolną liczbę całkowitą w dużym zakresie za
pomocą wektora małych wartości. To sprytne, ale nie tego zamierzamy użyć w
naszym przypadku. Zrobimy coś prostszego i bardziej efektywnego: nauczymy się
wektorów osadzania pozycji w ten sam sposób, w jaki uczymy się osadzać indeksy
słów. Następnie dodamy nasze osadzenia pozycji do odpowiednich osadzeń słów,
aby uzyskać osadzenie słów uwzględniające pozycję. Technika ta nazywana jest
"osadzaniem pozycyjnym".

```{r}
layer_positional_embedding <- new_layer_class(
  classname = "PositionalEmbedding",

  initialize = function(sequence_length, input_dim, output_dim, ...) {
    super$initialize(...)
    self$token_embeddings <-
      layer_embedding(input_dim = input_dim,
                      output_dim = output_dim)
    self$position_embeddings <-
      layer_embedding(input_dim = sequence_length,
                      output_dim = output_dim)
    self$sequence_length <- sequence_length
    self$input_dim <- input_dim
    self$output_dim <- output_dim
  },

  call = function(inputs) {
    length <- tf$shape(inputs)[-1]
    positions <- tf$range(start = 0L, limit = length, delta = 1L)
    embedded_tokens <- self$token_embeddings(inputs)
    embedded_positions <- self$position_embeddings(positions)
    embedded_tokens + embedded_positions
  },

  compute_mask = function(inputs, mask = NULL) {
    inputs != 0
  },

  get_config = function() {
    config <- super$get_config()
    for(name in c("output_dim", "sequence_length", "input_dim"))
      config[[name]] <- self[[name]]
    config
  }
) 
```

Można teraz użyć funkcji `layer_positional_embedding()` tak jak zwykłej
`layer_embedding()`.

```{r}
vocab_size <- 20000
sequence_length <- 600
embed_dim <- 256
num_heads <- 2
dense_dim <- 32

inputs <- layer_input(shape(NULL), dtype = "int64")

outputs <- inputs %>%
  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
  layer_global_average_pooling_1d() %>%
  layer_dropout(0.5) %>%
  layer_dense(1, activation = "sigmoid")

model <-
  keras_model(inputs, outputs) %>%
  compile(optimizer = "rmsprop",
          loss = "binary_crossentropy",
          metrics = "accuracy")

model

callbacks <- list(
  callback_model_checkpoint("models/full_transformer_encoder",
                            save_best_only = TRUE)
)
```

```{r}
#| eval: false
model %>% fit(
  int_train_ds,
  validation_data = int_val_ds,
  epochs = 20,
  callbacks = callbacks
)
```

```{r}
#| eval: false
model <- load_model_tf(
  "models/full_transformer_encoder",
  custom_objects = list(layer_transformer_encoder,
                        layer_positional_embedding))

cat(sprintf(
  "Test acc: %.3f\n", evaluate(model, int_test_ds)["accuracy"]))
```

Osiągnęliśmy dokładność testową na poziomie 88,6% - to nieco gorzej niż w
przypadku transformera bez osadzeń pozycyjnych. Jest on jednak wciąż nieco
gorszy od podejścia opartego na *bag-of-words*.

## Wskazówki praktyczne dotyczące wyboru modelu

Czasami można usłyszeć, że metody *bag-of-words* są przestarzałe i że modele
sekwencji oparte na transformatach są najlepszym rozwiązaniem, niezależnie od
zadania lub zbioru danych. Zdecydowanie tak nie jest. Mały stos gęstych warstw
na szczycie *bag-of-bigrams* pozostaje całkowicie poprawnym i odpowiednim
podejściem w wielu przypadkach. W rzeczywistości, spośród różnych technik,
które wypróbowaliśmy na zbiorze danych IMDB w tym rozdziale, jak dotąd
najlepiej sprawdzał się *bag-of-bigrams*! Kiedy więc należy preferować jedno
podejście nad drugim?

W 2017 roku zespół pod kierownictwem Cholleta przeprowadził systematyczną
analizę wydajności różnych technik klasyfikacji tekstu w wielu różnych typach
zbiorów danych tekstowych i odkrył niezwykłą i zaskakującą zasadę podejmowania
decyzji, czy wybrać model *bag-of-words*, czy model sekwencji
(<http://mng.bz/AOzK>) - swego rodzaju złotą proporcję. Okazuje się, że
zabierając się do nowego zadania klasyfikacji tekstu, należy zwrócić szczególną
uwagę na stosunek liczby próbek w danych treningowych do średniej liczby słów
na próbkę (patrz @fig-rule). Jeśli stosunek ten jest niewielki - mniejszy niż
1500 - wówczas model *bag-of-bigrams* będzie działał lepiej (i jako bonus,
będzie znacznie szybszy do trenowania). Jeśli współczynnik ten jest wyższy niż
1500, należy wybrać model sekwencyjny. Innymi słowy, modele sekwencyjne
działają najlepiej, gdy dostępnych jest wiele danych szkoleniowych i gdy każda
próbka jest stosunkowo krótka.

![](images/Zrzut%20ekranu%202024-02-11%20o%2014.24.42.png){#fig-rule
fig-align="center" width="600"}

Tak więc, jeśli klasyfikujemy dokumenty o długości 1000 słów i mamy ich 100 000
(stosunek 100), powinniśmy wybrać model bigramowy. Jeśli klasyfikujemy tweety o
średniej długości 40 słów i mamy ich 50 000 (stosunek 1250), powinniśmy również
wybrać model bigramowy. Ale jeśli zwiększymy rozmiar zbioru danych do 500 000
tweetów (stosunek 12 500), wybierzmy wówczas transformery. A co z zadaniem
klasyfikacji recenzji filmów IMDB? Mieliśmy 20 000 próbek treningowych i
średnią liczbę słów wynoszącą 233, więc nasza zasada kciuka wskazuje na model
bigramowy, co potwierdza to, co znaleźliśmy w praktyce.

Intuicyjnie ma to sens: dane wejściowe modelu sekwencji reprezentują bogatszą i
bardziej złożoną przestrzeń, a zatem potrzeba więcej danych, aby ją odwzorować;
tymczasem zwykły zestaw terminów jest przestrzenią tak prostą, że można na niej
trenować regresję logistyczną przy użyciu zaledwie kilkuset lub tysięcy próbek.
Ponadto, im krótsza jest próbka, tym mniej model może sobie pozwolić na
odrzucenie jakichkolwiek zawartych w niej informacji - w szczególności
kolejność słów staje się ważniejsza, a odrzucenie jej może powodować
niejednoznaczność. Zdania "ten film jest bombą" i "ten film był bombą" mają
bardzo zbliżone reprezentacje unigramów, co może zmylić model *bag-of-words*,
ale model sekwencji może stwierdzić, który z nich jest negatywny, a który
pozytywny. Przy dłuższej próbce statystyki słów stałyby się bardziej
wiarygodne, a temat lub sentyment byłyby bardziej widoczne na podstawie samego
histogramu słów.

Należy pamiętać, że ta heurystyczna reguła została opracowana specjalnie dla
klasyfikacji tekstu. Niekoniecznie musi się ona sprawdzać w innych zadaniach
NLP - na przykład, jeśli chodzi o tłumaczenie maszynowe, Transformer wyróżnia
się szczególnie w przypadku bardzo długich sekwencji, w porównaniu do sieci
RNN. Nasza heurystyka jest również tylko praktyczną zasadą, a nie naukowym
prawem, więc spodziewaj się, że będzie działać przez większość czasu, ale
niekoniecznie za każdym razem.

## Tłumaczenia typu sequence-to-sequence

Posiadamy już wszystkie narzędzia, których będziemy potrzebować, aby poradzić
sobie z większością zadań przetwarzania języka naturalnego. Jednakże,
widzieliśmy te narzędzia w akcji tylko w jednym problemie: klasyfikacji tekstu.
Jest to niezwykle popularny przypadek użycia, ale NLP to znacznie więcej niż
klasyfikacja. W tym podrozdziale poszerzymy swoją wiedzę, poznając modele
*sequence-to-sequence*.

Model sekwencja-sekwencja przyjmuje jedną sekwencję jako dane wejściowe (często
zdanie lub akapit) i tłumaczy ją na drugą sekwencję. Jest to zadanie leżące u
podstaw wielu najbardziej udanych zastosowań NLP:

-   Tłumaczenie maszynowe - konwersja akapitu w języku źródłowym na jego
    odpowiednik w języku docelowym.
-   Podsumowanie tekstu - konwersja długiego dokumentu na krótszą wersję, która
    zachowuje najważniejsze informacje.
-   Odpowiadanie na pytania - konwersja pytania wejściowego na odpowiedź.
-   Chatboty - konwertowanie monitu dialogowego na odpowiedź na ten monit lub
    konwertowanie historii konwersacji na następną odpowiedź w konwersacji.
-   Generowanie tekstu - konwertowanie monitu tekstowego na akapit, który
    uzupełnia monit.
-   I tak dalej.

![Uczenie sekwencyjne. Sekwencja źródłowa jest przetwarzana przez koder, a
następnie przesyłana do dekodera. Dekoder analizuje dotychczasową sekwencję
docelową i przewiduje przesunięcie sekwencji docelowej o jeden krok w
przyszłość. Podczas wnioskowania generujemy jeden token docelowy na raz i
przekazujemy go z powrotem do
dekodera](images/Zrzut%20ekranu%202024-02-11%20o%2014.36.45.png){#fig-seq-to-seq
fig-align="center" width="600"}

Ogólny szablon modeli sekwencja-sekwencja został opisany na @fig-seq-to-seq.
Podczas szkolenia:

-   Model kodera zamienia sekwencję źródłową w reprezentację pośrednią.
-   Dekoder jest szkolony, aby przewidzieć następny token $i$ w sekwencji
    docelowej, patrząc zarówno na poprzednie tokeny (od 1 do $i - 1$), jak i
    zakodowaną sekwencję źródłową.

Podczas wnioskowania nie mamy dostępu do sekwencji docelowej - próbujemy
przewidzieć ją od zera. Będziemy musieli generować ją po jednym tokenie na raz:

1.  Otrzymujemy zakodowaną sekwencję źródłową z kodera.
2.  Dekoder zaczyna od patrzenia na zakodowaną sekwencję źródłową, a także
    początkowy token "seed" (taki jak ciąg "\[start\]") i używa go do
    przewidzenia pierwszego prawdziwego tokena w sekwencji.
3.  Przewidywana sekwencja jest przekazywana z powrotem do dekodera, który
    generuje następny token i tak dalej, aż do wygenerowania tokenu zatrzymania
    (takiego jak ciąg "\[end\]").

Zasadę działania takiego modelu przedstawimy na przykładzie.

### Przykład tłumaczenia maszynowego

Zademonstrujemy modelowanie sekwencja do sekwencji, w zadaniu tłumaczenia
maszynowego. Tłumaczenie maszynowe jest dokładnie tym, do czego transformer
został opracowany! Zaczniemy od rekurencyjnego modelu sekwencji, a następnie
wykorzystamy pełną architekturę transformera. Będziemy pracować z zestawem
danych tłumaczeń z angielskiego na hiszpański dostępnym na stronie
<http://www.manythings.org/anki/.> Pobierzmy go:

```{r}
#| eval: false
download.file("http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip",
              destfile = "data/spa-eng.zip")
zip::unzip("data/spa-eng.zip", exdir = "data/")
```

Plik tekstowy zawiera jeden przykład w wierszu: zdanie w języku angielskim, po
którym następuje znak tabulacji, a następnie odpowiadające mu zdanie w języku
hiszpańskim. Użyjmy `readr::read_tsv()`, ponieważ mamy wartości oddzielone
tabulatorami:

```{r}
text_file <- "data/spa-eng/spa.txt"
text_pairs <- text_file %>%
  readr::read_tsv(col_names = c("english", "spanish"),
                  col_types = c("cc")) %>%
  within(spanish %<>% paste("[start]", ., "[end]"))
```

Nasz przykładowy tekst wygląda teraz tak:

```{r}
str(text_pairs[sample(nrow(text_pairs), 1), ])
```

Przetasujmy je i podzielmy na zwykłe zestawy treningowe, walidacyjne i testowe:

```{r}
num_test_samples <- num_val_samples <-
  round(0.15 * nrow(text_pairs))
num_train_samples <- nrow(text_pairs) - num_val_samples - num_test_samples

pair_group <- sample(c(
  rep("train", num_train_samples),
  rep("test", num_test_samples),
  rep("val", num_val_samples)
))

train_pairs <- text_pairs[pair_group == "train", ]
test_pairs <- text_pairs[pair_group == "test", ]
val_pairs <- text_pairs[pair_group == "val", ]
```

Następnie przygotujmy dwie oddzielne warstwy `TextVectorization`: jedną dla
języka angielskiego i jedną dla hiszpańskiego. Będziemy musieli dostosować
sposób wstępnego przetwarzania ciągów znaków:

-   Musimy zachować wstawione przez nas tokeny "`[start]`" i "`[end]`".
    Domyślnie znaki `[` i `]` zostaną usunięte, ale chcemy je zachować, abyśmy
    mogli odróżnić słowo "start" od tokenu startowego "\[start\]".
-   Interpunkcja różni się w zależności od języka! W hiszpańskiej warstwie
    wektoryzacji tekstu, jeśli zamierzamy usunąć znaki interpunkcyjne, musimy
    również usunąć znak `¿`.

Zauważmy, że w przypadku modelu tłumaczenia innego niż *toy-model*,
traktowalibyśmy znaki interpunkcyjne jako oddzielne tokeny, zamiast je usuwać,
ponieważ chcielibyśmy być w stanie generować poprawnie interpunkcyjne zdania. W
naszym przypadku, dla uproszczenia, pozbędziemy się całej interpunkcji.

Przygotowujemy niestandardową funkcję standaryzacji ciągów znaków dla
hiszpańskiej warstwy `TextVectorization` - zachowuje ona `[` i `]`, ale usuwa
`¿`, `¡` i wszystkie inne znaki z klasy `[:punct:]`.

```{r}
punctuation_regex <- "[^[:^punct:][\\]]|[¡¿]"

library(tensorflow)
custom_standardization <- function(input_string) {
  input_string %>%
    tf$strings$lower() %>%
    tf$strings$regex_replace(punctuation_regex, "")
}

input_string <- as_tensor("[start] ¡corre! [end]")
custom_standardization(input_string)
```

::: callout-note
Wyrażenia regularne TensorFlow różnią się nieznacznie w stosunku do silnika
wyrażeń regularnych R. Więcej szczegółów można znaleźć pod adresem
<https://github.com/google/re2/wiki/Syntax.>
:::

```{r}
vocab_size <- 15000
sequence_length <- 20

source_vectorization <- layer_text_vectorization(
  max_tokens = vocab_size,
  output_mode = "int",
  output_sequence_length = sequence_length
)

target_vectorization <- layer_text_vectorization(
  max_tokens = vocab_size,
  output_mode = "int",
  output_sequence_length = sequence_length + 1,
  standardize = custom_standardization
)

adapt(source_vectorization, train_pairs$english)
adapt(target_vectorization, train_pairs$spanish)
```

Na koniec możemy przekształcić nasze dane w potok TF Dataset. Chcemy, aby
zwracał on parę (`inputs, target`), gdzie `inputs` jest nazwaną listą z dwoma
wpisami, angielskim zdaniem (wejście kodera) i hiszpańskim zdaniem (wejście
dekodera), a `target` jest hiszpańskim zdaniem przesuniętym o jeden krok do
przodu.

```{r}
format_pair <- function(pair) {
  eng <- source_vectorization(pair$english)
  spa <- target_vectorization(pair$spanish)

  inputs <- list(english = eng,
                 spanish = spa[NA:-2])
  targets <- spa[2:NA]
  list(inputs, targets)
}


batch_size <- 64

library(tfdatasets)
make_dataset <- function(pairs) {
    tensor_slices_dataset(pairs) %>%
    dataset_map(format_pair, num_parallel_calls = 4) %>%
    dataset_cache() %>%
    dataset_shuffle(2048) %>%
    dataset_batch(batch_size) %>%
    dataset_prefetch(16)
}
train_ds <-  make_dataset(train_pairs)
val_ds <- make_dataset(val_pairs)
```

Oto jak wyglądają dane wyjściowe naszego zestawu danych:

```{r}
c(inputs, targets) %<-% iter_next(as_iterator(train_ds))
str(inputs)
str(targets)
```

Dane są teraz gotowe - czas zbudować kilka modeli. Zaczniemy od rekurencyjnego
modelu sekwencja-sekwencja, zanim przejdziemy do transformera.

### Uczenie sekwencyjne z wykorzystaniem sieci RNN

Rekurencyjne sieci neuronowe zdominowały uczenie sekwencyjne w latach
2015-2017, zanim zostały wyprzedzone przez transformery. Były one podstawą
wielu rzeczywistych systemów tłumaczenia maszynowego. Google Translate około
2017 roku był zasilany przez stos siedmiu dużych warstw LSTM. Dziś również
warto zapoznać się z tym podejściem, ponieważ stanowi ono łatwy punkt wejścia
do zrozumienia modeli sekwencja-sekwencja.

Najprostszym, naiwnym sposobem wykorzystania RNN do przekształcenia sekwencji w
inną sekwencję jest zachowanie danych wyjściowych RNN w każdym kroku czasowym.
W języku `keras` wyglądałoby to następująco:

```{r}
inputs <- layer_input(shape = c(sequence_length), dtype = "int64")
outputs <- inputs %>%
  layer_embedding(input_dim = vocab_size, output_dim = 128) %>%
  layer_lstm(32, return_sequences = TRUE) %>%
  layer_dense(vocab_size, activation = "softmax")
model <- keras_model(inputs, outputs)
```

Podejście to ma jednak dwa główne problemy:

-   Sekwencja docelowa musi być zawsze tej samej długości co sekwencja
    źródłowa. W praktyce rzadko ma to miejsce. Z technicznego punktu widzenia
    nie jest to istotne, ponieważ zawsze można uzupełnić sekwencję źródłową lub
    docelową, aby ich długości się zgadzały.
-   Ze względu na charakter przetwarzania zadania przez sieć RNN krok po kroku,
    model będzie patrzył tylko na tokeny $1\ldots N$ w sekwencji źródłowej, aby
    przewidzieć token $N$ w sekwencji docelowej. To ograniczenie sprawia, że ta
    konfiguracja nie nadaje się do większości zadań, w szczególności do
    tłumaczenia. Rozważmy tłumaczenie "The weather is nice today" na
    francuski - byłoby to "Il fait beau aujourd'hui". Musiałbyś być w stanie
    przewidzieć "Il" z samego "The", "Il fait" z samego "The weather" i tak
    dalej, co jest po prostu niemożliwe.

Jeśli jesteśmy tłumaczami, zaczynamy od przeczytania całego zdania źródłowego,
zanim zaczniemy je tłumaczyć. Jest to szczególnie ważne, jeśli mamy do
czynienia z językami, które mają bardzo różną kolejność słów, jak angielski i
japoński. I dokładnie to robią standardowe modele sekwencja-sekwencja.

W prawidłowej konfiguracji sekwencja-sekwencja (patrz @fig-rnn1), najpierw
należy użyć RNN (kodera), aby przekształcić całą sekwencję źródłową w
pojedynczy wektor (lub zestaw wektorów). Może to być ostatnie wyjście RNN lub
alternatywnie jego końcowe wektory stanu wewnętrznego. Następnie należy użyć
tego wektora (lub wektorów) jako stanu początkowego innej sieci RNN (dekodera),
która przyjrzy się elementom $1\ldots N$ w sekwencji docelowej i spróbuje
przewidzieć krok $N+1$ w sekwencji docelowej.

![Model RNN sekwencja-sekwencja. Koder RNN jest używany do tworzenia wektora,
który koduje całą sekwencję źródłową, która jest używana jako stan początkowy
dla dekodera
RNN](images/Zrzut%20ekranu%202024-02-14%20o%2020.24.00.png){#fig-rnn1
fig-align="center" width="500"}

Zaimplementujmy to w Keras za pomocą koderów i dekoderów opartych na GRU. Wybór
GRU zamiast LSTM nieco upraszcza sprawę, ponieważ GRU ma tylko jeden wektor
stanu, podczas gdy LSTM ma ich wiele. Zacznijmy od kodera.

```{r}
embed_dim <- 256
latent_dim <- 1024

source <- layer_input(c(NA), dtype = "int64", name = "english")
encoded_source <- source %>%
  layer_embedding(vocab_size, embed_dim, mask_zero = TRUE) %>%
  bidirectional(layer_gru(units = latent_dim), merge_mode = "sum")
```

Następnie dodajmy dekoder - prostą warstwę GRU, która jako stan początkowy
przyjmuje zakodowane zdanie źródłowe. Na wierzchu dodajemy `layer_dense()`,
która tworzy dla każdego kroku wyjściowego rozkład prawdopodobieństwa w
hiszpańskim słownictwie.

```{r}
decoder_gru <- layer_gru(units = latent_dim, return_sequences = TRUE)

past_target <- layer_input(shape = c(NA), dtype = "int64", name = "spanish")
target_next_step <- past_target %>%
  layer_embedding(vocab_size, embed_dim, mask_zero = TRUE) %>%
  decoder_gru(initial_state = encoded_source) %>%
  layer_dropout(0.5) %>%
  layer_dense(vocab_size, activation = "softmax")
seq2seq_rnn <- keras_model(inputs = list(source, past_target),
                           outputs = target_next_step)
```

Podczas treningu dekoder przyjmuje jako dane wejściowe całą sekwencję docelową,
ale dzięki krokowej naturze RNN, patrzy tylko na tokeny $1\ldots N$ na wejściu,
aby przewidzieć token $N$ na wyjściu (co odpowiada następnemu tokenowi w
sekwencji, ponieważ wyjście ma być przesunięte o jeden krok). Oznacza to, że
używamy tylko informacji z przeszłości do przewidywania przyszłości, tak jak
powinniśmy; w przeciwnym razie oszukiwalibyśmy, a nasz model nie działałby w
czasie wnioskowania.

```{r}
#| eval: false
seq2seq_rnn %>% compile(optimizer = "rmsprop",
                        loss = "sparse_categorical_crossentropy",
                        metrics = "accuracy")

seq2seq_rnn %>% fit(train_ds, epochs = 15, validation_data = val_ds)
```

Wybraliśmy *accuracy* jako niezbyt fortunny sposób monitorowania wydajności
zestawu walidacyjnego podczas treningu. Średnio model przewiduje następne słowo
w hiszpańskim zdaniu poprawnie w 63,2% przypadków. Jednak w praktyce dokładność
następnego tokenu nie jest optymalną miarą dla modeli tłumaczenia maszynowego,
w szczególności dlatego, że zakłada, że prawidłowe tokeny docelowe od $0$ do
$N$ są już znane podczas przewidywania tokenu $N+1$. W rzeczywistości podczas
wnioskowania generujemy zdanie docelowe od zera i nie możemy polegać na tym, że
wcześniej wygenerowane tokeny są w 100% poprawne. Jeśli pracujemy nad
rzeczywistym systemem tłumaczenia maszynowego, prawdopodobnie użyjemy metryki
BLEU[^llm-6] do oceny swoich modeli - metryki, która analizuje całe
wygenerowane sekwencje i wydaje się dobrze korelować z ludzkim postrzeganiem
jakości tłumaczenia.

Na koniec użyjmy naszego modelu do wnioskowania. Wybierzemy kilka zdań z
zestawu testowego i sprawdzimy, jak nasz model je tłumaczy. Zaczniemy od tokenu
początkowego "`[start]`" i wprowadzimy go do modelu dekodera wraz z zakodowanym
angielskim zdaniem źródłowym. Pobierzemy predykcję następnego tokena i ponownie
wprowadzimy ją do dekodera, próbkując jeden nowy token docelowy w każdej
iteracji, aż dojdziemy do "`[end]`" lub osiągniemy maksymalną długość zdania.

```{r}
spa_vocab <- get_vocabulary(target_vectorization)
max_decoded_sentence_length <- 20

decode_sequence <- function(input_sentence) {
  tokenized_input_sentence <-
    source_vectorization(array(input_sentence, dim = c(1, 1)))
  decoded_sentence <- "[start]"
  for (i in seq(max_decoded_sentence_length)) {
    tokenized_target_sentence <-
      target_vectorization(array(decoded_sentence, dim = c(1, 1)))
    next_token_predictions <- seq2seq_rnn %>%
      predict(list(tokenized_input_sentence,
                   tokenized_target_sentence))
    sampled_token_index <- which.max(next_token_predictions[1, i, ])
    sampled_token <- spa_vocab[sampled_token_index]
    decoded_sentence <- paste(decoded_sentence, sampled_token)
    if (sampled_token == "[end]")
      break
  }
  decoded_sentence
}

for (i in seq(5)) {
    input_sentence <- sample(test_pairs$english, 1)
    print("-")
    print(input_sentence)
    print(decode_sequence(input_sentence))
}
```

`decode_sequence()` działa dobrze, choć być może nieco wolniej niż byśmy
chcieli. Jednym z łatwych sposobów na przyspieszenie działania takiego kodu
jest użycie funkcji `tf_function()`. Przepiszmy funkcję `decode_sentence()`
tak, aby była kompilowana przez `tf_function()`. Oznacza to, że zamiast używać
natywnych funkcji R, takich jak `seq()`, `predict()` i `which.max()`, użyjemy
odpowiedników TensorFlow, takich jak `tf$range()`, wywołując bezpośrednio
`model()` i `tf$argmax()`.

Ponieważ `tf$range()` i `tf$argmax()` zwracają wartość równą 0, ustawimy
lokalną opcję funkcji: `option(tensorflow.extract.style = "python")`. Zmieni to
zachowanie `[` dla tensorów, aby również startowały od 0.

```{r}
tf_decode_sequence <- tf_function(function(input_sentence) {

  withr::local_options(tensorflow.extract.style = "python")

  tokenized_input_sentence <- input_sentence %>%
    as_tensor(shape = c(1, 1)) %>%
    source_vectorization()

  spa_vocab <- as_tensor(spa_vocab)

  decoded_sentence <- as_tensor("[start]", shape = c(1, 1))

  for (i in tf$range(as.integer(max_decoded_sentence_length))) {

    tokenized_target_sentence <- decoded_sentence %>%
      target_vectorization()

    next_token_predictions <-
      seq2seq_rnn(list(tokenized_input_sentence,
                       tokenized_target_sentence))

    sampled_token_index <- tf$argmax(next_token_predictions[0, i, ])
    sampled_token <- spa_vocab[sampled_token_index]
    decoded_sentence <-
      tf$strings$join(c(decoded_sentence, sampled_token),
                      separator = " ")

    if (sampled_token == "[end]")
      break
  }

  decoded_sentence

})

for (i in seq(5)) {
    input_sentence <- sample(test_pairs$english, 1)
    cat("-\n")
    cat(input_sentence, "\n")
    cat(input_sentence %>% as_tensor() %>%
          tf_decode_sequence() %>% as.character(), "\n")
}
```

Nasza funkcja `tf_decode_sentence()` jest około 10 razy szybsza od wersji
natywnej.

Należy zauważyć, że ta konfiguracja wnioskowania, choć bardzo prosta, jest
raczej nieefektywna, ponieważ ponownie przetwarzamy całe zdanie źródłowe i całe
wygenerowane zdanie docelowe za każdym razem, gdy próbkujemy nowe słowo. W
praktycznym zastosowaniu koder i dekoder byłyby traktowane jako dwa oddzielne
modele, a dekoder wykonywałby tylko jeden krok w każdej iteracji próbkowania
tokenów, ponownie wykorzystując swój poprzedni stan wewnętrzny.

Oto wyniki naszego tłumaczenia. Nasz model działa przyzwoicie jak na model typu
*toy*, choć nadal popełnia wiele podstawowych błędów.

Istnieje wiele sposobów na ulepszenie "zabawkowego" modelu: moglibyśmy użyć
głębokiego stosu warstw rekurencyjnych zarówno dla kodera, jak i dekodera
(należy pamiętać, że w przypadku dekodera zarządzanie stanem jest nieco
bardziej skomplikowane). Moglibyśmy użyć LSTM zamiast GRU. I tak dalej. Poza
takimi poprawkami, podejście RNN do uczenia sekwencyjnego ma jednak kilka
fundamentalnych ograniczeń:

-   Reprezentacja sekwencji źródłowej musi być przechowywana w całości w
    wektorze (wektorach) stanu kodera, co nakłada znaczne ograniczenia na
    rozmiar i złożoność zdań, które można przetłumaczyć. To trochę tak, jakby
    człowiek tłumaczył zdanie w całości z pamięci, nie patrząc dwa razy na
    zdanie źródłowe podczas tworzenia tłumaczenia.

-   RNN mają problem z radzeniem sobie z bardzo długimi sekwencjami, ponieważ
    mają tendencję do stopniowego zapominania o przeszłości - do czasu
    osiągnięcia setnego tokena w dowolnej sekwencji, niewiele informacji
    pozostaje o początku sekwencji. Oznacza to, że modele oparte na RNN nie są
    w stanie utrzymać długoterminowego kontekstu, który może być niezbędny do
    tłumaczenia długich dokumentów.

Ograniczenia te sprawiły, że społeczność zajmująca się uczeniem maszynowym
przyjęła architekturę transformer do rozwiązywania problemów typu
sekwencja-sekwencja. Przyjrzyjmy się temu.

[^llm-6]: BLEU (ang. *BiLingual Evaluation Understudy*) to wskaźnik służący do
    automatycznej oceny tekstu przetłumaczonego maszynowo. Wynik BLEU to liczba
    od zera do jednego, która mierzy podobieństwo tekstu przetłumaczonego
    maszynowo do zestawu wysokiej jakości tłumaczeń referencyjnych. Wartość 0
    oznacza, że tekst przetłumaczony maszynowo nie pokrywa się z tłumaczeniem
    referencyjnym (niska jakość), podczas gdy wartość 1 oznacza, że tekst
    idealnie pokrywa się z tłumaczeniem referencyjnym (wysoka jakość).

### Uczenie się sekwencyjne z transformerem

Uczenie sekwencyjne to zadanie, w którym transformer naprawdę błyszczy. Atencja
neuronowa umożliwia modelom transformer właściwe przetwarzanie sekwencji, które
są znacznie dłuższe i bardziej złożone niż te, z którymi radzą sobie sieci RNN.

Jako człowiek tłumaczący z angielskiego na hiszpański, nie czytamy angielskiego
zdania po jednym słowie na raz, tylko zachowujemy jego znaczenie w pamięci, a
następnie generujemy hiszpańskie zdanie po jednym słowie na raz. Może to
zadziałać w przypadku zdania składającego się z pięciu słów, ale jest mało
prawdopodobne, by zadziałało w przypadku całego akapitu. Zamiast tego,
prawdopodobnie będziemy chcieli przeskakiwać między zdaniem źródłowym a
tłumaczeniem i zwracać uwagę na różne słowa w źródle podczas zapisywania
różnych części tłumaczenia.

Dokładnie to można osiągnąć za pomocą atencji neuronowej i transformerów. Znamy
już koder transformera, który wykorzystuje samo-atencję (w odmianie z wieloma
głowami) do tworzenia reprezentacji kontekstowych każdego tokena w sekwencji
wejściowej. W transformatorze sekwencja-sekwencja, koder transformera
naturalnie odgrywałby rolę kodera, który odczytuje sekwencję źródłową i tworzy
jej zakodowaną reprezentację. Jednak w przeciwieństwie do naszego poprzedniego
kodera RNN, koder transformer utrzymuje zakodowaną reprezentację w formacie
sekwencji: jest to sekwencja wektorów osadzania z uwzględnieniem kontekstu.

Druga część modelu to dekoder transformera. Podobnie jak dekoder RNN, odczytuje
on tokeny $1\ldots N$ w sekwencji docelowej i próbuje przewidzieć token
$N + 1$. Co najważniejsze, robiąc to, wykorzystuje atencję neuronową, aby
zidentyfikować, które tokeny w zakodowanym zdaniu źródłowym są najbardziej
powiązane z tokenem docelowym, który obecnie próbuje przewidzieć. Przypomnijmy
model atencji posiadał query, key i value. W dekoderze transformerowym
sekwencja docelowa służy jako query, które jest używane do zwracania większej
uwagi na różne części sekwencji źródłowej (sekwencja źródłowa odgrywa role
zarówno key, jak i value).

#### Dekoder transformerowy

@fig-trans1 przedstawia pełny transformator sekwencja do sekwencji. Przyjrzyjmy
się wewnętrznym elementom dekodera. Rozpoznamy, że wygląda on bardzo podobnie
do kodera transformera, z wyjątkiem tego, że dodatkowy blok atencji jest
wstawiony pomiędzy blok samo-atencji zastosowany do sekwencji docelowej i
gęstych warstw bloku wyjściowego.

![`TransformerDecoder` jest podobny do `TransformerEncoder`, z wyjątkiem tego,
że zawiera dodatkowy blok atencji, w którym key i value są sekwencją źródłową
zakodowaną przez `TransformerEncoder`. Razem, koder i dekoder tworzą
transformator
end-to-end](images/Zrzut%20ekranu%202024-02-14%20o%2020.56.12.png){#fig-trans1
fig-align="center" width="600"}

Zaimplementujmy to. Podobnie jak w przypadku `TransformerEncoder`, będziemy
tworzyć nową klasę warstw.

Metoda `call()` jest prostym odwzorowaniem połączeń diagramu z @fig-trans1.
Jest jednak dodatkowy szczegół, który musimy wziąć pod uwagę - przyczynowe
wypełnianie (ang. *casual padding*). Wypełnianie przyczynowe jest absolutnie
krytyczne dla pomyślnego uczenia transformatora sekwencja do sekwencji. W
przeciwieństwie do RNN, który patrzy na swoje dane wejściowe po jednym kroku na
raz, a zatem będzie miał dostęp tylko do kroków $1\ldots N$, aby wygenerować
krok wyjściowy $N$ (który jest tokenem $N+1$ w sekwencji docelowej), natomiast
`TransformerDecoder` jest niezależny od kolejności: patrzy na całą sekwencję
docelową naraz. Gdyby pozwolić mu na wykorzystanie całego wejścia, po prostu
nauczyłby się kopiować krok wejściowy $N+1$ do lokalizacji $N$ na wyjściu. W
ten sposób model osiągnąłby idealną dokładność treningu, ale oczywiście podczas
wnioskowania byłby całkowicie bezużyteczny, ponieważ kroki wejściowe poza $N$
nie są dostępne.

Rozwiązanie jest proste - zamaskujemy górną połowę macierzy atencji parami, aby
uniemożliwić modelowi zwracanie uwagi na informacje z przyszłości - informacje
tylko z tokenów $1\ldots N$ w sekwencji docelowej powinny być używane podczas
generowania tokena docelowego $N+1$. Aby to zrobić, dodamy metodę
`get_causal_attention_mask(inputs)` do naszego `TransformerDecoder`, aby pobrać
maskę uwagi, którą możemy przekazać do naszych warstw `MultiHeadAttention`.

```{r}
layer_transformer_decoder <- new_layer_class(
  classname = "TransformerDecoder",

  initialize = function(embed_dim, dense_dim, num_heads, ...) {
    super$initialize(...)
    self$embed_dim <- embed_dim
    self$dense_dim <- dense_dim
    self$num_heads <- num_heads
    self$attention_1 <- layer_multi_head_attention(num_heads = num_heads,
                                                   key_dim = embed_dim)
    self$attention_2 <- layer_multi_head_attention(num_heads = num_heads,
                                                   key_dim = embed_dim)
    self$dense_proj <- keras_model_sequential() %>%
      layer_dense(dense_dim, activation = "relu") %>%
      layer_dense(embed_dim)

    self$layernorm_1 <- layer_layer_normalization()
    self$layernorm_2 <- layer_layer_normalization()
    self$layernorm_3 <- layer_layer_normalization()
    self$supports_masking <- TRUE
  },

  get_config = function() {
    config <- super$get_config()
    for (name in c("embed_dim", "num_heads", "dense_dim"))
      config[[name]] <- self[[name]]
    config
  },

  get_causal_attention_mask = function(inputs) {
    c(batch_size, sequence_length, encoding_length) %<-%
      tf$unstack(tf$shape(inputs))

    x <- tf$range(sequence_length)
    i <- x[, tf$newaxis]
    j <- x[tf$newaxis, ]
    mask <- tf$cast(i >= j, "int32")

    tf$tile(mask[tf$newaxis, , ],
            tf$stack(c(batch_size, 1L, 1L)))
  },

  call = function(inputs, encoder_outputs, mask = NULL) {

    causal_mask <- self$get_causal_attention_mask(inputs)

    if (is.null(mask))
      mask <- causal_mask
    else
      mask %<>% { tf$minimum(tf$cast(.[, tf$newaxis, ], "int32"),
                             causal_mask) }

    inputs %>%
      { self$attention_1(query = ., value = ., key = .,
                         attention_mask = causal_mask) + . } %>%
      self$layernorm_1() %>%

      { self$attention_2(query = .,
                         value = encoder_outputs,
                         key = encoder_outputs,
                         attention_mask = mask) + . } %>%
      self$layernorm_2() %>%

      { self$dense_proj(.) + . } %>%
      self$layernorm_3()

  }
)
```

Będziemy trenować model transformator typu *end-to-end*. Mapuje on sekwencję
źródłową i docelową do sekwencji docelowej o jeden krok w przyszłości. W prosty
sposób łączy elementy, które zbudowaliśmy do tej pory: warstwy
`PositionalEmbedding`, `TransformerEncoder` i `TransformerDecoder`. Zwróćmy
uwagę, że zarówno `TransformerEncoder`, jak i `TransformerDecoder` są
niezmienne pod względem kształtu[^llm-7], więc możemy połączyć wiele z nich,
aby stworzyć bardziej wydajny koder lub dekoder. W naszym przykładzie będziemy
trzymać się pojedynczej instancji każdego z nich.

```{r}
embed_dim <- 256
dense_dim <- 2048
num_heads <- 8

encoder_inputs <- layer_input(shape(NA), dtype = "int64", name = "english")
encoder_outputs <- encoder_inputs %>%
  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  layer_transformer_encoder(embed_dim, dense_dim, num_heads)

transformer_decoder <-
  layer_transformer_decoder(NULL, embed_dim, dense_dim, num_heads)

decoder_inputs <-  layer_input(shape(NA), dtype = "int64", name = "spanish")
decoder_outputs <- decoder_inputs %>%
  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  transformer_decoder(., encoder_outputs) %>%
  layer_dropout(0.5) %>%
  layer_dense(vocab_size, activation="softmax")

transformer <- keras_model(list(encoder_inputs, decoder_inputs),
                           decoder_outputs)
```

```{r}
#| eval: false
transformer %>%
  compile(optimizer = "rmsprop",
          loss = "sparse_categorical_crossentropy",
          metrics = "accuracy")

transformer %>%
  fit(train_ds, epochs = 30, validation_data = val_ds)

save_model_tf(transformer, filepath = "models/end_to_end_transformer")
```

Osiągamy dokładność 65,3%, czyli istotnie lepiej niż z modelu opartego na GRU.

Na koniec spróbujmy użyć naszego modelu do przetłumaczenia nigdy wcześniej nie
widzianych angielskich zdań z zestawu testowego. Konfiguracja jest identyczna z
tą, której użyliśmy do modelu RNN typu sekwencja-sekwencja. Zastąpimy tylko
`seq2seq_rnn` transformatorem i usuniemy dodatkowego tokena, który
skonfigurowaliśmy do dodania w warstwie `target_vectorization()`.

```{r}
transformer <- load_model_tf(
  "models/end_to_end_transformer.keras",
  custom_objects = list(
    layer_positional_embedding,
    layer_transformer_decoder,
    layer_transformer_encoder
  )
)
```

```{r}
tf_decode_sequence <- tf_function(function(input_sentence) {
  withr::local_options(tensorflow.extract.style = "python")

  tokenized_input_sentence <- input_sentence %>%
    as_tensor(shape = c(1, 1)) %>%
    source_vectorization()
  spa_vocab <- as_tensor(spa_vocab)
  decoded_sentence <- as_tensor("[start]", shape = c(1, 1))

  for (i in tf$range(as.integer(max_decoded_sentence_length))) {

    tokenized_target_sentence <-
      target_vectorization(decoded_sentence)[,NA:-1]

    next_token_predictions <-
      transformer(list(tokenized_input_sentence,
                       tokenized_target_sentence))

    sampled_token_index <- tf$argmax(next_token_predictions[0, i, ])
    sampled_token <- spa_vocab[sampled_token_index]
    decoded_sentence <-
      tf$strings$join(c(decoded_sentence, sampled_token),
                      separator = " ")

    if (sampled_token == "[end]")
      break
  }

  decoded_sentence

})

for (i in seq(20)) {

    c(input_sentence, correct_translation) %<-%
      test_pairs[sample.int(nrow(test_pairs), 1), ]
    cat("-\n")
    cat(input_sentence, "\n")
    cat(correct_translation, "\n")
    cat(input_sentence %>% as_tensor() %>%
          tf_decode_sequence() %>% as.character(), "\n")
}
```

Na tym kończy się ten rozdział o przetwarzaniu języka naturalnego. W ramach
niego przeszliśmy od podstaw do w rozwiązania współcześnie używanego, czyli
Transformera, który może tłumaczyć zdania z angielskiego na hiszpański.

[^llm-7]: wejście i wyjście jest tych samych rozmiarów
