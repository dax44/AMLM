[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaawansowane metody uczenia maszynowego",
    "section": "",
    "text": "Wstęp\nKsiążka ta jest napisana na potrzeby prowadzenia zajęć na kierunku Inżynieria i analiza danych z przedmiotu Zaawansowane metody uczenia maszynowego. Jest swego rodzaju autorskim podejściem do tematu, przedstawiającym wybrane metody uczenia maszynowego, które rzadziej występują w opracowaniach na temat uczenia maszynowego.\nUczenie maszynowe stanowi obszar intensywnego rozwoju, który obejmuje szereg technik umożliwiających bardziej skomplikowane i wydajne modele predykcyjne. Wśród tych metod warto wyróżnić głębokie sieci neuronowe, zwłaszcza konwolucyjne sieci neuronowe (CNN) i rekurencyjne sieci neuronowe (RNN). CNN są wykorzystywane w zadaniach przetwarzania obrazów, gdzie potrafią efektywnie ekstrahować hierarchiczne cechy z danych wejściowych, natomiast RNN są efektywne w analizie sekwencji danych, takich jak język naturalny. Ponadto, metody uczenia maszynowego obejmują techniki transferu wiedzy, uczenie ze wzmocnieniem, generatywne modele, takie jak generatywne sieci przeciwdziedzinowe (GAN), czy też autokodery. Te nowoczesne podejścia umożliwiają modelom uczącym się wykonywanie bardziej złożonych zadań, a także adaptację do różnorodnych danych wejściowych, co sprawia, że są one stosowane w obszarach takich jak rozpoznawanie obrazów, przetwarzanie języka naturalnego, czy nawet w autonomicznych systemach decyzyjnych.\nPonadto, zaawansowane metody uczenia maszynowego obejmują także techniki regularyzacji, optymalizacji i inżynierię cech. Regularyzacja ma na celu zapobieganie przeuczeniu poprzez kontrolowanie złożoności modelu, natomiast optymalizacja skupia się na dostosowywaniu wag modelu w celu minimalizacji funkcji straty. Inżynieria cech polega na ręcznym lub automatycznym dostosowywaniu danych wejściowych w celu uzyskania lepszych wyników modelu. Dzięki tym zaawansowanym metodom, uczenie maszynowe staje się coraz bardziej potężnym narzędziem w analizie danych i podejmowaniu skomplikowanych decyzji w różnych dziedzinach.\nModele predykcyjne dla wielu wyjść, czyli tzw. multi-target regression and classification, stanowią kolejny istotny obszar w dziedzinie uczenia maszynowego. W przypadku multi-target regression, celem jest przewidywanie wielu wartości wyjściowych dla danego zestawu wejściowego, co często spotyka się w złożonych problemach predykcyjnych, takich jak prognozowanie wielu parametrów jednocześnie. Z kolei w przypadku multi-target classification, model ma za zadanie przypisanie jednego lub więcej klas dla każdego przykładu wejściowego. Te modele są powszechnie stosowane w różnych dziedzinach, takich jak bioinformatyka, finanse czy przemysł, gdzie jednoczesne przewidywanie wielu zmiennych jest kluczowe dla skutecznego rozwiązania problemu. Wdrożenie takich zaawansowanych modeli predykcyjnych wymaga starannej obróbki danych, odpowiedniego dostosowania architektury modelu oraz precyzyjnej oceny wyników, co sprawia, że są one istotnym narzędziem w obszarze analizy danych i podejmowania decyzji.\nModele językowe stanowią jeszcze jeden kluczowy obszar w dziedzinie uczenia maszynowego, skoncentrowany na zrozumieniu i generowaniu ludzkiego języka naturalnego. Głębokie sieci neuronowe, zwłaszcza rekurencyjne sieci neuronowe (RNN) i transformery, zostały skutecznie wykorzystane do tworzenia modeli językowych o zdolnościach przetwarzania i generowania tekstu na poziomie zbliżonym do ludzkiego. Te modele zdolne są do zrozumienia kontekstu, analizy gramatyki, a także generowania spójnych i sensownych odpowiedzi. Wykorzystywane są w różnorodnych zastosowaniach, takich jak tłumaczenie maszynowe, generowanie tekstu, czy analiza nastroju w tekście. Ponadto, pre-trenowane modele językowe, takie jak BERT czy GPT (Generative Pre-trained Transformer), zdobywają popularność, umożliwiając dostosowanie ich do różnych zadań poprzez fine-tuning. W miarę postępu badań i rozwoju w tej dziedzinie, modele językowe stają się coraz bardziej zaawansowane, co przyczynia się do doskonalenia komunikacji między maszynami a ludźmi oraz do rozwijania nowych możliwości w dziedzinie przetwarzania języka naturalnego.\nWspomniane powyżej metody i modele będą stanowić treść wykładów z wspomnianego na wstępie przedmiotu.",
    "crumbs": [
      "Wstęp"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "Witam w świecie zaawansowanych metod uczenia maszynowego 🤖, prezentowanej w niniejszej publikacji. Książka ta skupia się na trzech głównych obszarach, zaczynając od wielowymiarowych problemów predykcyjnych, przechodząc przez kompleksowe modele głębokich sieci neuronowych, a kończąc na zaawansowanych modelach językowych. Koncepcyjnie rozpoczniemy od omówienia multiple target regression and classification, gdzie przedstawimy skomplikowane zadania predykcyjne wymagające jednoczesnej prognozy wielu zmiennych. Przeanalizujemy praktyczne zastosowania tych modeli w obszarach, takich jak nauki społeczne, biologia i finanse.\nNastępnie poświęcimy uwagę głębokim sieciom neuronowym, głównemu filarowi nowoczesnej sztucznej inteligencji. Omówimy ewolucję od konwolucyjnych sieci neuronowych (CNN) do rekurencyjnych sieci neuronowych (RNN), zwracając uwagę na ich zdolność do efektywnego przetwarzania obrazów, sekwencji danych i rozwiązania bardziej złożonych problemów. W ramach tego obszaru, przyjrzymy się również technikom transferu wiedzy, uczenia ze wzmocnieniem oraz generatywnym modelom, takim jak generatywne sieci przeciwdziedzinowe (GAN), które poszerzają granice możliwości maszynowego uczenia się.\n\n\n\nTrzeci kluczowy obszar, który będzie przedmiotem analizy, to modele językowe. Rozważania rozpoczniemy od głębokich sieci neuronowych, a następnie skoncentrujemy się na transformatorach, które rewolucjonizują przetwarzanie języka naturalnego 👅. Przedstawimy praktyczne zastosowania tych modeli, zwłaszcza w tłumaczeniu maszynowym, generowaniu tekstu i analizie sentymentu. Ponadto, omówimy pre-trenowane modele językowe, takie jak BERT czy GPT, jako kluczowe narzędzia adaptacyjne, zdolne do fine-tuningu w zależności od konkretnego zadania.\nKażdy podejmowany temat będzie wzbogacony o implementację analizowanych metod w realnych scenariuszach. Omówimy kroki od obróbki danych, przez dostosowywanie architektury modelu, aż po ocenę wyników. W tym kontekście poruszymy także aspekty etyczne i wyzwania związane z zastosowaniem zaawansowanych modeli uczenia maszynowego.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html",
    "href": "multi_target_models.html",
    "title": "2  Modele z wieloma wyjściami",
    "section": "",
    "text": "2.1 Typy modeli z wieloma zmiennymi wynikowymi\nWśród nadzorowanych modeli uczenia maszynowego z wieloma zmiennymi wynikowymi można wymienić zarówno te dedykowane do klasyfikacji, jak i regresji. Modele te są znane jako modele z wieloma wyjściami (klasyfikacyjne) lub modele z wieloma wyjściami (regresyjne), w zależności od rodzaju problemu, który rozwiązują.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "href": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "title": "2  Modele z wieloma wyjściami",
    "section": "",
    "text": "Modele z wieloma wyjściami (klasyfikacyjne)\nW przypadku klasyfikacji, gdy mamy wiele kategorii (klas) jako zmienną wynikową, modele te są nazywane modelami z wieloma wyjściami. Przykłady obejmują algorytmy takie jak regresja logistyczna, metoda k najbliższych sąsiadów (k-NN) czy algorytmy drzew decyzyjnych, które zostały dostosowane do obsługi wielu klas.\nPrzykładowe zadanie: Załóżmy, że mamy zbiór danych dotyczący różnych rodzajów owoców (np. jabłek, pomarańczy, bananów) i chcemy stworzyć model, który jednocześnie przewiduje gatunek owocu oraz kolor owocu. Mamy więc dwie zmienne wynikowe: gatunek (klasyfikacja wieloklasowa) i kolor (klasyfikacja wieloklasowa).\nModele z wieloma wyjściami (regresyjne).\nW przypadku regresji, gdzie zmienną wynikową jest wektor wartości numerycznych, modele te są nazywane modelami z wieloma wyjściami. Przykłady obejmują algorytmy regresji liniowej lub nieliniowej, algorytmy oparte na drzewach decyzyjnych, czy też bardziej zaawansowane modele, takie jak sieci neuronowe.\nPrzykładowe zadanie: Zakładamy, że mamy zbiór danych zawierający informacje o pracownikach, takie jak doświadczenie zawodowe, poziom wykształcenia, liczba godzin pracy tygodniowo itp. Chcemy stworzyć model, który jednocześnie przewiduje zarobki pracowników oraz ich poziom satysfakcji zawodowej.\nModele wielozadaniowe.\nModele wielozadaniowe to rodzaj nadzorowanego uczenia maszynowego, w którym model jest trenowany jednocześnie do rozwiązania kilku zadań. Te zadania mogą obejmować zarówno klasyfikację, jak i regresję. Dzięki wspólnemu trenowaniu modelu na wielu zadaniach, można uzyskać korzyści w postaci wspólnego wykorzystywania wiedzy między zadaniami.\nPrzykładowe zadanie: Załóżmy, że mamy zbiór danych dotyczący zakupów klientów w sklepie internetowym. Dla każdego klienta mamy informacje o różnych aspektach zakupów, takich jak czas dostawy, łatwość obsługi strony, jakość produktów itp. Chcemy stworzyć model, który jednocześnie przewiduje dwie zmienne wynikowe: jakość obsługi klienta (skala jakościowa, np. “Niska”, “Średnia”, “Wysoka”) oraz całkowity wydatek klienta (zmienna ilościowa, np. kwota zakupów).\nModele hierarchiczne.\nW niektórych przypadkach, szczególnie gdy mamy hierarchię zmiennych wynikowych, modele te mogą być budowane w sposób hierarchiczny. Przykładowo, w problemie klasyfikacji obrazów z hierarchią kategorii (na przykład rozpoznawanie gatunków zwierząt), model może być zaprojektowany do rozpoznawania zarówno ogólnych, jak i bardziej szczegółowych kategorii.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#różnie-podejścia-do-modelowania-z-wieloma-wyjściami",
    "href": "multi_target_models.html#różnie-podejścia-do-modelowania-z-wieloma-wyjściami",
    "title": "2  Modele z wieloma wyjściami",
    "section": "2.2 Różnie podejścia do modelowania z wieloma wyjściami",
    "text": "2.2 Różnie podejścia do modelowania z wieloma wyjściami\nIstnieją dwa ogólne podejścia do rozwiązywania problemów wieloetykietowych: transformacja problemu i adaptacja algorytmu. Transformacja problemu polega na manipulowaniu zbiorem danych w taki sposób, że problem wieloetykietowy staje się jednym lub kilkoma problemami jednoetykietowymi (Tawiah i Sheng 2013). Adaptacja algorytmu polega na tym, że sam algorytm jest w stanie poradzić sobie bezpośrednio z problemem wieloetykietowym. Okazuje się, że wiele, choć nie wszystkie, metody adaptacji algorytmów metod adaptacji algorytmów w rzeczywistości wykorzystuje transformację problemu (Tsoumakas i Katakis 2007).\n\n2.2.1 Transformacja problemu\nTechniki te przewidują stworzenie indywidualnego modelu dla każdego celu, a następnie połączenie oddzielnych modeli w celu uzyskania ogólnej prognozy. Metody transformacji problemów okazały się lepsze od metod adaptacji algorytmów pod względem dokładności (Spyromitros-Xioufis i in. 2016). Co więcej, podstawowa zasada sprawia, że metody transformacji problemu są niezależne od algorytmu. W konsekwencji, można je łatwo dostosować do danego problemu poprzez zastosowanie odpowiednich bazowych metod uczących. Punkt ten ma również szczególne znaczenie dla modeli typu ensemble, które łączą oszacowania z wielu potencjalnie różnych algorytmów w ostateczną prognozę. Niedawno Spyromitros-Xioufis i in. (2016) zaproponowali rozszerzenie znanych metod transformacji klasyfikacji wieloetykietowej, aby poradzić sobie z problemem regresji wielowynikowej i modelować zależności między celami. W szczególności wprowadzili oni dwa nowe podejścia do regresji wielocelowej, składanie regresorów wielocelowych i łańcuchy regresorów, inspirowane popularnymi i skutecznymi podejściami do klasyfikacji wieloznaczeniowej.\nPodstawową koncepcją w metodach transformacji problemów jest wykorzystanie poprzednich modeli do nowego przewidywania poprzez rozszerzoną przestrzeń cech (Borchani i in. 2015). Stacked generalization to podejście do meta-uczenia, które wykorzystuje dane wyjściowe wcześniej wyuczonych modeli do uczenia się nowego modelu. W związku z tym początkowe dane wyjściowe modelu są traktowane jako nowe cechy i są układane w stos do początkowego wektora cech przed ponownym uczeniem. W oryginalnym sformułowaniu przewidziano tylko dwuetapową procedurę, tj. początkowe modele wyuczone z początkowego wektora cech odpowiadają odpowiednio modelom i danym poziomu 0, a powiększony wektor cech i ponownie wyuczony model są określane odpowiednio jako dane poziomu 1 i generalizator. Jednakże, rozsądnie rzecz biorąc, ten proces układania pojedynczego celu (ang. Single-target Stacking - STS) może być również przeprowadzany w wielu iteracjach. Aby wdrożyć tę zasadę dla problemów z wieloma celami, w których kodowane są również możliwe korelacje między zmiennymi docelowymi, wprowadzono koncepcję układania wielu celów (ang. Multi-target Stacking - MTS) (Borchani i in. 2015). Analogicznie do STS, szkolenie modelu MTS można uznać za procedurę dwuetapową. W pierwszym etapie uczone są niezależne modele dla każdej zmiennej docelowej. Następnie uczone są meta-modele dla każdej zmiennej docelowej z rozszerzonymi wektorami cech, które zawierają początkowe wektory cech, a także oszacowania poziomu 0 pozostałych zmiennych docelowych. Podobne pomysły były również stosowane w kontekście modeli zespołowych, tj. uczenia się kilku modeli poziomu 0 dla każdej zmiennej docelowej, które są łączone w procedurze uogólniania poziomu 1 dla wielu zmiennych docelowych (Santana i in. 2020).\n\n2.2.1.1 Single-target stacking\nMetoda ta jest stosowana przede wszystkim z zadaniach regresyjnych z wieloma wyjściami. Rozważmy zbiór danych \\(D = \\left\\{\\left(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(N)}, \\mathbf{y}^{(N)}\\right)\\right\\}\\), składający się z \\(N\\) obserwacji, które są realizacjami zmiennych losowych \\(X_1,\\ldots,X_m, Y_1,\\ldots,Y_d\\). Zatem każde wejście do modelu jest charakteryzowane przez \\(m\\) zmiennych \\(\\mathbf{x}{(l)}=\\left(x_1^{(l)},\\ldots, x_j^{(l)}, \\ldots, x_m^{(l)} \\right)\\) oraz \\(d\\) odpowiadających im wyjść \\(\\mathbf{y}{(l)}=\\left(y_1^{(l)},\\ldots, y_i^{(l)}, \\ldots, y_d^{(l)} \\right)\\), gdzie \\(l\\in\\{1,\\ldots,N\\}, j\\in\\{1,\\ldots,m\\}, i\\in\\{1,\\ldots,d\\}\\). Naszym celem w zadaniu regresyjnym (MTR - Multi-target Regression) jest nauczenie takiego modelu \\(h\\), który przekształca \\(\\mathbf{x}\\) w \\(\\mathbf{y}\\).\nW podejściu STS w pierwszym kroku budowanych jest \\(d\\) niezależnych modeli przewidujących pojedyncze wyjście. Po tej czynności meta-model jest trenowany na zbiorze \\(D_i'\\), który jest wzbogaconym zbiorem \\(D_i\\) o predykcje zmiennej \\(Y_i\\), czyli\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)} \\right)\\). W zależności czy rozpatrujemy algorytm STS niekumulatywny, czy kumulatywny, drugi krok iteracji wygląda nieco inaczej:\n\nniekumulatywny\n\\[\n\\bar{D}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i'^{(l)} \\right)\\)\nkumulatywny\n\\[\n\\bar{\\bar{D}}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)},\\hat{y}_i'^{(l)} \\right)\\).\n\n\n\n\nSingle-target stacking\n\n\n\n\n2.2.1.2 Multi-target stacking\nW przeciwieństwie do STS, MTS został zaprojektowany do dzielenia się wiedzą w skorelowanych zmiennych docelowych w ramach procedury łączenia w stosy. Podobnie, najpierw uczone są modele pojedynczego celu. Następnie tworzony jest zestaw meta-modeli, które zawierają model dla każdej zmiennej docelowej \\(Y_i,\\) \\(i \\in \\{1, \\ldots, d\\}\\). W ten sposób uwzględniane są szacunki dotyczące pozostałych zmiennych docelowych z pierwszego etapu, tj. model jest uczony z przekształconego zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_1^{(l)},\\ldots,\\hat{y}_d^{(l)} \\right)\\). W metodzie MTS istnieją również dwa sposoby składania kolejnych iteracji. Przebiegają one w podobny sposób jak w przypadku STS.\n\n\n\nMulti-target stacking\n\n\nIstnieje jeszcze trzecia metoda powszechnie stosowana do predykcji wielowyniowej zwana Regressor Chains lub Classifier Chains w zależności od celu zadania. Idę działania tej metody przedstawię na przykładzie modelu regresyjnego.\n\n\n2.2.1.3 Regressor Chains\nRC opierają się na idei dopasowywania modeli pojedynczego celu wzdłuż wybranej permutacji, tj. łańcucha. Najpierw losowana jest permutacja w odniesieniu do zmiennych docelowych. Proces ten można przeprowadzić w sposób losowy (Spyromitros-Xioufis i in. 2016) lub uporządkowany (Melki i in. 2017). Wybrana permutacja jest wykorzystywana do zbudowania oddzielnego modelu regresji dla zmiennych docelowych zgodnie z kolejnością permutacji. Aby wykorzystać tę strukturę do MTR, rzeczywiste wartości zmiennych docelowych są dostarczane do kolejnych modeli podczas uczenia się wzdłuż łańcucha. Na podstawie pełnego łańcucha lub wybranego zestawu \\(C = (Y_1,\\ldots,Y_d)\\), pierwszy model jest ograniczony do ustalenia predykcji dla \\(Y_1\\). Następnie, kolejno dla \\(Y_i\\) uczone są modele na podstawie zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, y_1^{(l)},\\ldots, y_{i-1}^{(l)} \\right)\\). Ten algorytm ma również dwie odmiany (niekumulatywną i kumulatywną) w zależności od kształtu kolejnych iteracji.\n\n\n\nRegressor chains\n\n\nPonieważ, jak można się spodziewać wyniki modelowania w znaczny sposób zależą od wylosowanej permutacji, to w metodzie zaproponowanej przez Melki i in. (2017) aby uniknąć tego efektu buduje się \\(k\\) modeli dla różnych permutacji i łączy się wyniki w podobny sposób jak w lasach losowych.\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nSłowo komentarza jeśli chodzi o dostępność tych metod w językach programowania. Niestety wspomniane metody w R nie są zaimplementowane w sposób, który pozwalałby na bezpieczne używanie przygotowanych rozwiązań. Istnieje kilka wzmianek (na dzień dzisiejszy, czyli początek 2024 roku) na ten temat. Twórcy dwóch głównych frameworków do uczenia maszynowego, czyli mlr3 oraz tidymodels przygotowują implementacje tych metod. Dodatkowo istnieje rozwiązanie w wersji eksperymentalnej mtr-toolkit, które pozwala na wykonanie modelowania z wieloma wyjściami, którym można się posiłkować.\nNiestety w przypadku Python-a nie jest dużo lepiej. Wprawdzie w pakiecie scikit-learn istnieją implementacje pozwalające na predykcje wielowyjściowe w obu typach zadań poprzez MultiOutputRegressor i MultiOutputClassifier, ale dokonują one predykcji naiwnej poprzez złożenie w listę wyników pojedynczych modeli dla każdej zmiennej. Nieco lepiej sprawa wygląda w przypadku metod łańcuchowych, ponieważ zarówno dla klasyfikacji, jak i regresji są metody to realizujące (ClassifierChain i RegressorChain).\n\n\n\n\n\n2.2.2 Adaptacja algorytmu\nProstota podejścia transformacji problemu sprawia, że jest ono odpowiednie dla problemów, w których jego wady mają niewielki lub żaden wpływ - jednak dla złożonych problemów podejście adaptacji algorytmu może okazać się bardziej efektywne. Dodatkowo, dowody empiryczne sugerują, że uczenie się powiązanych zadań jednocześnie, a nie niezależnie, może poprawić wyniki predykcyjne (Evgeniou i Pontil 2004). Z drugiej strony, jeśli zadania są bardzo odmienne, wydajność predykcyjna może ucierpieć, gdy zadania są uczone razem, a nie niezależnie (Faddoul i in. 2010). W związku z tym możemy wyciągnąć następujące wnioski:\n\njeśli zadania, których nasz predyktor ma się nauczyć, są powiązane, powinniśmy dążyć do znalezienia odpowiedniej metody adaptacji algorytmu;\njeśli zadania, których chcemy się nauczyć, nie są powiązane, powinniśmy zamiast tego dążyć do znalezienia odpowiedniej metody transformacji problemu.\n\nWreszcie, powinniśmy wziąć pod uwagę rozmiar problemu i zdać sobie sprawę, że gdy zadania są niepowiązane, istnieje potencjalny kompromis między efektywnością czasową a wydajnością predykcyjną przy wyborze metody transformacji problemu lub metody adaptacji algorytmu. W przypadku niepowiązanych ze sobą zadań, metody transformacji problemu mogą zwiększać skuteczność predykcyjną, ale zmniejszać wydajność czasową w przypadku dużych problemów i odwrotnie.\nNiestety tej metody nie da się zastosować do każdego typu modelu. Rodzina modeli, których adaptacja jest wykonana cały czas rośnie. Adaptacja modelu polega na przekształceniu go do postaci, w której da się wykonać predykcję dla wielu wyjść. Wśród modeli, których wersje native multi-target istnieją należy wymienić:\n\nregresja wieloraka (Izenman 1975)\nkNN\ndrzewo decyzyjne (Struyf i Džeroski 2006)\nlas losowy (Kocev i in. 2013)\nbagging (Kocev i in. 2013)\ngradient boosting (Zhang i Jung, b.d.; Faddoul i in. 2012)\nSVM (Xu, Guo, i Wang 2013; Vazquez i Walter 2003)\nno i oczywiście sieci neuronowe.\n\nNie sposób przedstawić w jaki sposób wprowadzone zostały zmiany we wszystkich algorytmach. Skupię się jednak na pokazaniu adaptacji drzew decyzyjnych do predykcji wielu wyjść jednocześnie, ponieważ jest to meta-model modeli takich jak lasy losowe, bagging czy boosting.\n\n2.2.2.1 Adaptacja klasyfikacyjnego drzewa decyzyjnego\nFaddoul i in. (2012) zaproponowali zmodyfikowaną wersję algorytmu drzewa decyzyjnego C4.5 (Quinlan 1993), która bezpośrednio obsługuje problemy klasyfikacji wielowyjściowej. Zmodyfikowana wersja (nazwana MT-DT) różni się od standardowej implementacji C4.5 w dwóch krytycznych aspektach: kryteriach podziału węzłów i procesie decyzyjnym. Faddoul i in. (2012) proponują trzy różne podejścia do łączenia wielu miar przyrostu informacji w jedną miarę: wspólny przyrost informacji, suma nieważona i maksymalny przyrost informacji. Wspólny przyrost informacyjny jest definiowany przy użyciu konkatenacji wszystkich poszczególnych zadań, tj. względnej różnicy w entropii mierzonej we wszystkich zadaniach decyzyjnych. Autorzy pokazują, że nieważona suma (Równanie 2.1) indywidualnych przyrostów informacyjnych wszystkich zadań jest równoważna wspólnemu przyrostowi informacyjnemu.\n\\[\nIG_U=\\sum_YIG_Y\n\\tag{2.1}\\]\nMaksymalny przyrost informacyjny, zgodnie z propozycją autorów jest definiowany po prostu jako maksymalny przyrost informacyjny wszystkich zadań:\n\\[\nIG_M=\\max_YIG_Y\n\\tag{2.2}\\]\nBadania eksperymentalne pokazały, że maksymalny przyrost informacyjny wykorzystany do budowania reguł podziału, charakteryzuje się wyższym poziomem dopasowania modeli, niż przy zastosowaniu \\(IG_U\\) i \\(IG_J\\).\nW przypadku klasyfikacji z jedną etykietą, algorytm indukcji drzewa decyzyjnego (taki jak C4.5) rekurencyjnie dzieli węzły, dodając (zazwyczaj dwa) elementy potomne, aż możliwe jest utworzenie liścia takiego, że znaczna większość (lub nawet wszystkie) jego przykładowych instancji należy do tej samej klasy. W przypadku wielu wyjść, indukcja drzewa niekoniecznie jest tak prosta. Rozważmy problem klasyfikacji wielowyjściowej z dwoma wyjściami binarnymi \\(\\nu_1\\) i \\(\\nu_2\\); możliwe jest, że po \\(t\\) podziałach, węzeł zawiera tylko wartości pozytywne dla \\(\\nu_1\\), ale mieszankę wartości pozytywnych i negatywnych dla \\(\\nu_2\\) - stąd, podczas konstruowania drzew decyzyjnych dla wielu jednoczesnych zadań, należy pamiętać, że proces decyzyjny dla pewnego zadania może wymagać krótszej ścieżki decyzyjnej niż inne zadania w ramach tego samego problemu wielowyjściowego. MT-DT radzi sobie z tym, sprawdzając w każdym węźle, czy możliwe jest utworzenie węzła terminalnego dla któregokolwiek z zadań - w powyższym przykładzie spowodowałoby to utworzenie drzewa, w którym wewnętrzny węzeł \\(t_1\\) jest oznaczony jako węzeł zatrzymania dla \\(\\nu_1\\), oznaczony klasą pozytywną. Ponieważ celem jest prognozowanie dla obu wyjść binarnych, \\(t_1\\) nie jest węzłem liścia - zamiast tego rekurencyjne dzielenie jest kontynuowane od \\(t_1\\), aż do znalezienia węzła \\(t_2\\) takiego, że \\(t_2\\) jest wystarczająco czysty w odniesieniu do \\(\\nu_2\\), aby można było utworzyć regułę klasyfikacji dla drugiego zadania binarnego. W tym momencie węzły decyzyjne (węzły wewnętrzne lub liście) zostały znalezione dla wszystkich wyników (\\(\\nu_1\\) i \\(\\nu_2\\)), a algorytm indukcji drzewa rekurencyjnego może zostać zakończony.\nNic dziwnego, że klasyfikacja przy użyciu już zbudowanego modelu MT-DT przebiega według tej samej formuły, co jego indukcja - podczas przechodzenia przez drzewo każdy węzeł jest sprawdzany w celu ustalenia, czy można podjąć decyzję dla któregokolwiek z aktualnie nierozstrzygniętych zadań. W przykładzie \\(\\nu_1\\), \\(\\nu_2\\), klasyfikacja zostanie dokonana dla \\(\\nu_1\\) w węźle \\(t_1\\), ponieważ jest on oznaczony jako węzeł zatrzymania dla \\(\\nu_1\\); następnie przejście jest kontynuowane do momentu napotkania \\(t_2\\) i klasyfikacja może zostać dokonana dla \\(\\nu_2\\). W tym momencie wszystkie wyjścia zostały sklasyfikowane, a przechodzenie może się zakończyć, zwracając dwie wartości w \\(t_1\\) i \\(t_2\\) jako klasyfikacje odpowiednio dla \\(\\nu_1\\) i \\(\\nu_2\\).\n\n\n2.2.2.2 Adaptacja regresyjnego drzewa decyzyjnego\nSegal (1992) zaproponował rozwiązanie dla drzew regresyjnych o wielu wyjściach (MRT), które są w stanie przewidywać wyniki dla wielu powiązanych zadań regresyjnych; te wielowyjściowe drzewa regresyjne są oparte na funkcji podziału najmniejszych kwadratów zaproponowanej w ramach CART (Breiman i in. 2017). W przypadku drzewa regresyjnego o jednej odpowiedzi celem jest minimalizacja następującej funkcji celu:\n\\[\n\\phi(t) = SS(t)-SS(t_L)-SS(t_R)\n\\]\ngdzie \\(SS(t)\\) jest zdefiniowana następująco\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))^2.\n\\]\nSegal (1992) dodał ważenie macierzą kowariancji do błędu kwadratowego, co prowadzi algorytm drzewa do tworzenia węzłów potomnych, które reprezentują jednorodne klastry w odniesieniu do zestawu odpowiedzi wyjściowych:\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))'V^{-1}(t)(y_i-\\bar{y}(t)),\n\\]\ngdzie \\(V(t)\\) oznacza macierz kowariancji w węźle \\(t\\).\n\n\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, i Pedro Larrañaga. 2015. „A Survey on Multi-Output Regression”. WIREs Data Mining and Knowledge Discovery 5 (5): 216–33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, i Charles J. Stone. 2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, i Massimiliano Pontil. 2004. „Regularized multi–task learning”. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, sierpień. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Rémi Gilleron, i Fabien Torre. 2012. „Learning multiple tasks with boosted decision trees”. W Proceedings of the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I, 681–96. ECMLPKDD’12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, i Remi Gilleron. 2010. „Boosting Multi-Task Weak Learners with Applications to Textual and Social Data”. W 2010 Ninth International Conference on Machine Learning and Applications, 367–72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nIzenman, Alan Julian. 1975. „Reduced-rank regression for the multivariate linear model”. Journal of multivariate analysis 5 (2): 248–64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, i Sašo Džeroski. 2013. „Tree ensembles for predicting structured outputs”. Pattern Recognition 46 (3): 817–33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, i Sebastián Ventura. 2017. „Multi-Target Support Vector Regression via Correlation Regressor Chains”. Information Sciences 415–416 (listopad): 53–69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Morgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello Mastelini, Fabio Luiz Melquiades, i Sylvio Barbon Jr. 2020. „Improved Prediction of Soil Properties with Multi-Target Stacked Generalisation on EDXRF Spectra”. arXiv preprint arXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. „Tree-Structured Methods for Longitudinal Data”. Journal of the American Statistical Association 87 (418): 407–18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves, i Ioannis Vlahavas. 2016. „Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs”. Machine Learning 104 (1): 55–98. https://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, i Sašo Džeroski. 2006. „Constraint Based Induction of Multi-objective Regression Trees”. W Knowledge Discovery in Inductive Databases, zredagowane przez Francesco Bonchi i Jean-François Boulicaut, 222–33. Lecture Notes w Computer Science. Springer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, i Victor Sheng. 2013. „Empirical Comparison of Multi-Label Classification Algorithms”. W Proceedings of the AAAI Conference on Artificial Intelligence, 27:1645–46.\n\n\nTsoumakas, Grigorios, i Ioannis Katakis. 2007. „Multi-Label Classification: An Overview”. International Journal of Data Warehousing and Mining (IJDWM) 3 (3): 1–13.\n\n\nVazquez, Emmanuel, i Eric Walter. 2003. „Multi-Output Suppport Vector Regression”. IFAC Proceedings Volumes, 13th IFAC Symposium on System Identification (SYSID 2003), Rotterdam, The Netherlands, 27-29 August, 2003, 36 (16): 1783–88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, i Laisheng Wang. 2013. „A Twin Multi-Class Classification Support Vector Machine”. Cognitive Computation 5 (4): 580–88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, i Cheolkon Jung. b.d. „GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputs”. https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Borchani, Hanen, Gherardo Varando, Concha Bielza, and Pedro Larrañaga.\n2015. “A Survey on Multi-Output Regression.” WIREs Data\nMining and Knowledge Discovery 5 (5): 216–33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, and Charles J. Stone.\n2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, and Massimiliano Pontil. 2004. “Regularized\nMulti–Task Learning.” Proceedings of the Tenth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\nAugust. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Rémi Gilleron, and Fabien\nTorre. 2012. “Learning Multiple Tasks with Boosted Decision\nTrees.” In Proceedings of the 2012th European Conference on\nMachine Learning and Knowledge Discovery in Databases - Volume Part\ni, 681–96. ECMLPKDD’12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, and Remi\nGilleron. 2010. “Boosting Multi-Task Weak Learners with\nApplications to Textual and Social Data.” In 2010 Ninth\nInternational Conference on Machine Learning and Applications,\n367–72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nIzenman, Alan Julian. 1975. “Reduced-Rank Regression for the\nMultivariate Linear Model.” Journal of Multivariate\nAnalysis 5 (2): 248–64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, and Sašo Džeroski. 2013.\n“Tree Ensembles for Predicting Structured Outputs.”\nPattern Recognition 46 (3): 817–33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, and Sebastián Ventura.\n2017. “Multi-Target Support Vector Regression via Correlation\nRegressor Chains.” Information Sciences 415–416\n(November): 53–69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning.\nMorgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello\nMastelini, Fabio Luiz Melquiades, and Sylvio Barbon Jr. 2020.\n“Improved Prediction of Soil Properties with Multi-Target Stacked\nGeneralisation on EDXRF Spectra.” arXiv Preprint\narXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. “Tree-Structured Methods for\nLongitudinal Data.” Journal of the American Statistical\nAssociation 87 (418): 407–18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves,\nand Ioannis Vlahavas. 2016. “Multi-Target Regression\nvia Input Space Expansion: Treating Targets as\nInputs.” Machine Learning 104 (1): 55–98.\nhttps://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, and Sašo Džeroski. 2006. “Constraint Based Induction\nof Multi-Objective Regression Trees.” In Knowledge Discovery\nin Inductive Databases, edited by Francesco Bonchi and\nJean-François Boulicaut, 222–33. Lecture Notes in Computer Science.\nSpringer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, and Victor Sheng. 2013. “Empirical Comparison of\nMulti-Label Classification Algorithms.” In Proceedings of the\nAAAI Conference on Artificial\nIntelligence, 27:1645–46.\n\n\nTsoumakas, Grigorios, and Ioannis Katakis. 2007. “Multi-Label\nClassification: An Overview.” International\nJournal of Data Warehousing and Mining (IJDWM) 3 (3): 1–13.\n\n\nVazquez, Emmanuel, and Eric Walter. 2003. “Multi-Output Suppport\nVector Regression.” IFAC Proceedings Volumes, 13th IFAC\nsymposium on system identification (SYSID 2003), rotterdam, the\nnetherlands, 27-29 august, 2003, 36 (16): 1783–88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, and Laisheng Wang. 2013. “A Twin Multi-Class\nClassification Support Vector Machine.” Cognitive\nComputation 5 (4): 580–88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, and Cheolkon Jung. n.d. “GBDT-MO: Gradient\nBoosted Decision Trees for Multiple Outputs.” https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "Literatura"
    ]
  }
]