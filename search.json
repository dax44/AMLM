[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaawansowane metody uczenia maszynowego",
    "section": "",
    "text": "Wstęp\nKsiążka ta jest napisana na potrzeby prowadzenia zajęć na kierunku Inżynieria i analiza danych z przedmiotu Zaawansowane metody uczenia maszynowego. Jest swego rodzaju autorskim podejściem do tematu, przedstawiającym wybrane metody uczenia maszynowego, które rzadziej występują w opracowaniach na temat uczenia maszynowego.\nUczenie maszynowe stanowi obszar intensywnego rozwoju, który obejmuje szereg technik umożliwiających bardziej skomplikowane i wydajne modele predykcyjne. Wśród tych metod warto wyróżnić głębokie sieci neuronowe, zwłaszcza konwolucyjne sieci neuronowe (CNN) i rekurencyjne sieci neuronowe (RNN). CNN są wykorzystywane w zadaniach przetwarzania obrazów, gdzie potrafią efektywnie ekstrahować hierarchiczne cechy z danych wejściowych, natomiast RNN są efektywne w analizie sekwencji danych, takich jak język naturalny. Ponadto, metody uczenia maszynowego obejmują techniki transferu wiedzy, uczenie ze wzmocnieniem, generatywne modele, takie jak generatywne sieci przeciwdziedzinowe (GAN), czy też autokodery. Te nowoczesne podejścia umożliwiają modelom uczącym się wykonywanie bardziej złożonych zadań, a także adaptację do różnorodnych danych wejściowych, co sprawia, że są one stosowane w obszarach takich jak rozpoznawanie obrazów, przetwarzanie języka naturalnego, czy nawet w autonomicznych systemach decyzyjnych.\nPonadto, zaawansowane metody uczenia maszynowego obejmują także techniki regularyzacji, optymalizacji i inżynierię cech. Regularyzacja ma na celu zapobieganie przeuczeniu poprzez kontrolowanie złożoności modelu, natomiast optymalizacja skupia się na dostosowywaniu wag modelu w celu minimalizacji funkcji straty. Inżynieria cech polega na ręcznym lub automatycznym dostosowywaniu danych wejściowych w celu uzyskania lepszych wyników modelu. Dzięki tym zaawansowanym metodom, uczenie maszynowe staje się coraz bardziej potężnym narzędziem w analizie danych i podejmowaniu skomplikowanych decyzji w różnych dziedzinach.\nModele predykcyjne dla wielu wyjść, czyli tzw. multi-target regression and classification, stanowią kolejny istotny obszar w dziedzinie uczenia maszynowego. W przypadku multi-target regression, celem jest przewidywanie wielu wartości wyjściowych dla danego zestawu wejściowego, co często spotyka się w złożonych problemach predykcyjnych, takich jak prognozowanie wielu parametrów jednocześnie. Z kolei w przypadku multi-target classification, model ma za zadanie przypisanie jednego lub więcej klas dla każdego przykładu wejściowego. Te modele są powszechnie stosowane w różnych dziedzinach, takich jak bioinformatyka, finanse czy przemysł, gdzie jednoczesne przewidywanie wielu zmiennych jest kluczowe dla skutecznego rozwiązania problemu. Wdrożenie takich zaawansowanych modeli predykcyjnych wymaga starannej obróbki danych, odpowiedniego dostosowania architektury modelu oraz precyzyjnej oceny wyników, co sprawia, że są one istotnym narzędziem w obszarze analizy danych i podejmowania decyzji.\nModele językowe stanowią jeszcze jeden kluczowy obszar w dziedzinie uczenia maszynowego, skoncentrowany na zrozumieniu i generowaniu ludzkiego języka naturalnego. Głębokie sieci neuronowe, zwłaszcza rekurencyjne sieci neuronowe (RNN) i transformery, zostały skutecznie wykorzystane do tworzenia modeli językowych o zdolnościach przetwarzania i generowania tekstu na poziomie zbliżonym do ludzkiego. Te modele zdolne są do zrozumienia kontekstu, analizy gramatyki, a także generowania spójnych i sensownych odpowiedzi. Wykorzystywane są w różnorodnych zastosowaniach, takich jak tłumaczenie maszynowe, generowanie tekstu, czy analiza nastroju w tekście. Ponadto, pre-trenowane modele językowe, takie jak BERT czy GPT (Generative Pre-trained Transformer), zdobywają popularność, umożliwiając dostosowanie ich do różnych zadań poprzez fine-tuning. W miarę postępu badań i rozwoju w tej dziedzinie, modele językowe stają się coraz bardziej zaawansowane, co przyczynia się do doskonalenia komunikacji między maszynami a ludźmi oraz do rozwijania nowych możliwości w dziedzinie przetwarzania języka naturalnego.\nWspomniane powyżej metody i modele będą stanowić treść wykładów z wspomnianego na wstępie przedmiotu.",
    "crumbs": [
      "Wstęp"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "Witam w świecie zaawansowanych metod uczenia maszynowego 🤖, prezentowanej w niniejszej publikacji. Książka ta skupia się na trzech głównych obszarach, zaczynając od wielowymiarowych problemów predykcyjnych, przechodząc przez kompleksowe modele głębokich sieci neuronowych, a kończąc na zaawansowanych modelach językowych. Koncepcyjnie rozpoczniemy od omówienia multiple target regression and classification, gdzie przedstawimy skomplikowane zadania predykcyjne wymagające jednoczesnej prognozy wielu zmiennych. Przeanalizujemy praktyczne zastosowania tych modeli w obszarach, takich jak nauki społeczne, biologia i finanse.\nNastępnie poświęcimy uwagę głębokim sieciom neuronowym, głównemu filarowi nowoczesnej sztucznej inteligencji. Omówimy ewolucję od konwolucyjnych sieci neuronowych (CNN) do rekurencyjnych sieci neuronowych (RNN), zwracając uwagę na ich zdolność do efektywnego przetwarzania obrazów, sekwencji danych i rozwiązania bardziej złożonych problemów. W ramach tego obszaru, przyjrzymy się również technikom transferu wiedzy, uczenia ze wzmocnieniem oraz generatywnym modelom, takim jak generatywne sieci przeciwdziedzinowe (GAN), które poszerzają granice możliwości maszynowego uczenia się.\n\n\n\nTrzeci kluczowy obszar, który będzie przedmiotem analizy, to modele językowe. Rozważania rozpoczniemy od głębokich sieci neuronowych, a następnie skoncentrujemy się na transformatorach, które rewolucjonizują przetwarzanie języka naturalnego 👅. Przedstawimy praktyczne zastosowania tych modeli, zwłaszcza w tłumaczeniu maszynowym, generowaniu tekstu i analizie sentymentu. Ponadto, omówimy pre-trenowane modele językowe, takie jak BERT czy GPT, jako kluczowe narzędzia adaptacyjne, zdolne do fine-tuningu w zależności od konkretnego zadania.\nKażdy podejmowany temat będzie wzbogacony o implementację analizowanych metod w realnych scenariuszach. Omówimy kroki od obróbki danych, przez dostosowywanie architektury modelu, aż po ocenę wyników. W tym kontekście poruszymy także aspekty etyczne i wyzwania związane z zastosowaniem zaawansowanych modeli uczenia maszynowego.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html",
    "href": "multi_target_models.html",
    "title": "2  Modele z wieloma wyjściami",
    "section": "",
    "text": "2.1 Typy modeli z wieloma zmiennymi wynikowymi\nWśród nadzorowanych modeli uczenia maszynowego z wieloma zmiennymi wynikowymi można wymienić zarówno te dedykowane do klasyfikacji, jak i regresji. Modele te są znane jako modele z wieloma wyjściami (klasyfikacyjne) lub modele z wieloma wyjściami (regresyjne), w zależności od rodzaju problemu, który rozwiązują.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "href": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "title": "2  Modele z wieloma wyjściami",
    "section": "",
    "text": "Modele z wieloma wyjściami (klasyfikacyjne)\nW przypadku klasyfikacji, gdy mamy wiele kategorii (klas) jako zmienną wynikową, modele te są nazywane modelami z wieloma wyjściami. Przykłady obejmują algorytmy takie jak regresja logistyczna, metoda k najbliższych sąsiadów (k-NN) czy algorytmy drzew decyzyjnych, które zostały dostosowane do obsługi wielu klas.\nPrzykładowe zadanie: Załóżmy, że mamy zbiór danych dotyczący różnych rodzajów owoców (np. jabłek, pomarańczy, bananów) i chcemy stworzyć model, który jednocześnie przewiduje gatunek owocu oraz kolor owocu. Mamy więc dwie zmienne wynikowe: gatunek (klasyfikacja wieloklasowa) i kolor (klasyfikacja wieloklasowa).\nModele z wieloma wyjściami (regresyjne).\nW przypadku regresji, gdzie zmienną wynikową jest wektor wartości numerycznych, modele te są nazywane modelami z wieloma wyjściami. Przykłady obejmują algorytmy regresji liniowej lub nieliniowej, algorytmy oparte na drzewach decyzyjnych, czy też bardziej zaawansowane modele, takie jak sieci neuronowe.\nPrzykładowe zadanie: Zakładamy, że mamy zbiór danych zawierający informacje o pracownikach, takie jak doświadczenie zawodowe, poziom wykształcenia, liczba godzin pracy tygodniowo itp. Chcemy stworzyć model, który jednocześnie przewiduje zarobki pracowników oraz ich poziom satysfakcji zawodowej.\nModele wielozadaniowe.\nModele wielozadaniowe to rodzaj nadzorowanego uczenia maszynowego, w którym model jest trenowany jednocześnie do rozwiązania kilku zadań. Te zadania mogą obejmować zarówno klasyfikację, jak i regresję. Dzięki wspólnemu trenowaniu modelu na wielu zadaniach, można uzyskać korzyści w postaci wspólnego wykorzystywania wiedzy między zadaniami.\nPrzykładowe zadanie: Załóżmy, że mamy zbiór danych dotyczący zakupów klientów w sklepie internetowym. Dla każdego klienta mamy informacje o różnych aspektach zakupów, takich jak czas dostawy, łatwość obsługi strony, jakość produktów itp. Chcemy stworzyć model, który jednocześnie przewiduje dwie zmienne wynikowe: jakość obsługi klienta (skala jakościowa, np. “Niska”, “Średnia”, “Wysoka”) oraz całkowity wydatek klienta (zmienna ilościowa, np. kwota zakupów).\nModele hierarchiczne.\nW niektórych przypadkach, szczególnie gdy mamy hierarchię zmiennych wynikowych, modele te mogą być budowane w sposób hierarchiczny. Przykładowo, w problemie klasyfikacji obrazów z hierarchią kategorii (na przykład rozpoznawanie gatunków zwierząt), model może być zaprojektowany do rozpoznawania zarówno ogólnych, jak i bardziej szczegółowych kategorii.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#różnie-podejścia-do-modelowania-z-wieloma-wyjściami",
    "href": "multi_target_models.html#różnie-podejścia-do-modelowania-z-wieloma-wyjściami",
    "title": "2  Modele z wieloma wyjściami",
    "section": "2.2 Różnie podejścia do modelowania z wieloma wyjściami",
    "text": "2.2 Różnie podejścia do modelowania z wieloma wyjściami\nIstnieją dwa ogólne podejścia do rozwiązywania problemów wieloetykietowych: transformacja problemu i adaptacja algorytmu. Transformacja problemu polega na manipulowaniu zbiorem danych w taki sposób, że problem wieloetykietowy staje się jednym lub kilkoma problemami jednoetykietowymi (Tawiah i Sheng 2013). Adaptacja algorytmu polega na tym, że sam algorytm jest w stanie poradzić sobie bezpośrednio z problemem wieloetykietowym. Okazuje się, że wiele, choć nie wszystkie, metody adaptacji algorytmów metod adaptacji algorytmów w rzeczywistości wykorzystuje transformację problemu (Tsoumakas i Katakis 2007).\n\n2.2.1 Transformacja problemu\nTechniki te przewidują stworzenie indywidualnego modelu dla każdego celu, a następnie połączenie oddzielnych modeli w celu uzyskania ogólnej prognozy. Metody transformacji problemów okazały się lepsze od metod adaptacji algorytmów pod względem dokładności (Spyromitros-Xioufis i in. 2016). Co więcej, podstawowa zasada sprawia, że metody transformacji problemu są niezależne od algorytmu. W konsekwencji, można je łatwo dostosować do danego problemu poprzez zastosowanie odpowiednich bazowych metod uczących. Punkt ten ma również szczególne znaczenie dla modeli typu ensemble, które łączą oszacowania z wielu potencjalnie różnych algorytmów w ostateczną prognozę. Niedawno Spyromitros-Xioufis i in. (2016) zaproponowali rozszerzenie znanych metod transformacji klasyfikacji wieloetykietowej, aby poradzić sobie z problemem regresji wielowynikowej i modelować zależności między celami. W szczególności wprowadzili oni dwa nowe podejścia do regresji wielocelowej, składanie regresorów wielocelowych i łańcuchy regresorów, inspirowane popularnymi i skutecznymi podejściami do klasyfikacji wieloznaczeniowej.\nPodstawową koncepcją w metodach transformacji problemów jest wykorzystanie poprzednich modeli do nowego przewidywania poprzez rozszerzoną przestrzeń cech (Borchani i in. 2015). Stacked generalization to podejście do meta-uczenia, które wykorzystuje dane wyjściowe wcześniej wyuczonych modeli do uczenia się nowego modelu. W związku z tym początkowe dane wyjściowe modelu są traktowane jako nowe cechy i są układane w stos do początkowego wektora cech przed ponownym uczeniem. W oryginalnym sformułowaniu przewidziano tylko dwuetapową procedurę, tj. początkowe modele wyuczone z początkowego wektora cech odpowiadają odpowiednio modelom i danym poziomu 0, a powiększony wektor cech i ponownie wyuczony model są określane odpowiednio jako dane poziomu 1 i generalizator. Jednakże, rozsądnie rzecz biorąc, ten proces układania pojedynczego celu (ang. Single-target Stacking - STS) może być również przeprowadzany w wielu iteracjach. Aby wdrożyć tę zasadę dla problemów z wieloma celami, w których kodowane są również możliwe korelacje między zmiennymi docelowymi, wprowadzono koncepcję układania wielu celów (ang. Multi-target Stacking - MTS) (Borchani i in. 2015). Analogicznie do STS, szkolenie modelu MTS można uznać za procedurę dwuetapową. W pierwszym etapie uczone są niezależne modele dla każdej zmiennej docelowej. Następnie uczone są meta-modele dla każdej zmiennej docelowej z rozszerzonymi wektorami cech, które zawierają początkowe wektory cech, a także oszacowania poziomu 0 pozostałych zmiennych docelowych. Podobne pomysły były również stosowane w kontekście modeli zespołowych, tj. uczenia się kilku modeli poziomu 0 dla każdej zmiennej docelowej, które są łączone w procedurze uogólniania poziomu 1 dla wielu zmiennych docelowych (Santana i in. 2020).\n\n2.2.1.1 Single-target stacking\nMetoda ta jest stosowana przede wszystkim z zadaniach regresyjnych z wieloma wyjściami. Rozważmy zbiór danych \\(D = \\left\\{\\left(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(N)}, \\mathbf{y}^{(N)}\\right)\\right\\}\\), składający się z \\(N\\) obserwacji, które są realizacjami zmiennych losowych \\(X_1,\\ldots,X_m, Y_1,\\ldots,Y_d\\). Zatem każde wejście do modelu jest charakteryzowane przez \\(m\\) zmiennych \\(\\mathbf{x}{(l)}=\\left(x_1^{(l)},\\ldots, x_j^{(l)}, \\ldots, x_m^{(l)} \\right)\\) oraz \\(d\\) odpowiadających im wyjść \\(\\mathbf{y}{(l)}=\\left(y_1^{(l)},\\ldots, y_i^{(l)}, \\ldots, y_d^{(l)} \\right)\\), gdzie \\(l\\in\\{1,\\ldots,N\\}, j\\in\\{1,\\ldots,m\\}, i\\in\\{1,\\ldots,d\\}\\). Naszym celem w zadaniu regresyjnym (MTR - Multi-target Regression) jest nauczenie takiego modelu \\(h\\), który przekształca \\(\\mathbf{x}\\) w \\(\\mathbf{y}\\).\nW podejściu STS w pierwszym kroku budowanych jest \\(d\\) niezależnych modeli przewidujących pojedyncze wyjście. Po tej czynności meta-model jest trenowany na zbiorze \\(D_i'\\), który jest wzbogaconym zbiorem \\(D_i\\) o predykcje zmiennej \\(Y_i\\), czyli\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)} \\right)\\). W zależności czy rozpatrujemy algorytm STS niekumulatywny, czy kumulatywny, drugi krok iteracji wygląda nieco inaczej:\n\nniekumulatywny\n\\[\n\\bar{D}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i'^{(l)} \\right)\\)\nkumulatywny\n\\[\n\\bar{\\bar{D}}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)},\\hat{y}_i'^{(l)} \\right)\\).\n\n\n\n\nSingle-target stacking\n\n\n\n\n2.2.1.2 Multi-target stacking\nW przeciwieństwie do STS, MTS został zaprojektowany do dzielenia się wiedzą w skorelowanych zmiennych docelowych w ramach procedury łączenia w stosy. Podobnie, najpierw uczone są modele pojedynczego celu. Następnie tworzony jest zestaw meta-modeli, które zawierają model dla każdej zmiennej docelowej \\(Y_i,\\) \\(i \\in \\{1, \\ldots, d\\}\\). W ten sposób uwzględniane są szacunki dotyczące pozostałych zmiennych docelowych z pierwszego etapu, tj. model jest uczony z przekształconego zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_1^{(l)},\\ldots,\\hat{y}_d^{(l)} \\right)\\). W metodzie MTS istnieją również dwa sposoby składania kolejnych iteracji. Przebiegają one w podobny sposób jak w przypadku STS.\n\n\n\nMulti-target stacking\n\n\nIstnieje jeszcze trzecia metoda powszechnie stosowana do predykcji wielowyniowej zwana Regressor Chains lub Classifier Chains w zależności od celu zadania. Idę działania tej metody przedstawię na przykładzie modelu regresyjnego.\n\n\n2.2.1.3 Regressor Chains\nRC opierają się na idei dopasowywania modeli pojedynczego celu wzdłuż wybranej permutacji, tj. łańcucha. Najpierw losowana jest permutacja w odniesieniu do zmiennych docelowych. Proces ten można przeprowadzić w sposób losowy (Spyromitros-Xioufis i in. 2016) lub uporządkowany (Melki i in. 2017). Wybrana permutacja jest wykorzystywana do zbudowania oddzielnego modelu regresji dla zmiennych docelowych zgodnie z kolejnością permutacji. Aby wykorzystać tę strukturę do MTR, rzeczywiste wartości zmiennych docelowych są dostarczane do kolejnych modeli podczas uczenia się wzdłuż łańcucha. Na podstawie pełnego łańcucha lub wybranego zestawu \\(C = (Y_1,\\ldots,Y_d)\\), pierwszy model jest ograniczony do ustalenia predykcji dla \\(Y_1\\). Następnie, kolejno dla \\(Y_i\\) uczone są modele na podstawie zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, y_1^{(l)},\\ldots, y_{i-1}^{(l)} \\right)\\). Ten algorytm ma również dwie odmiany (niekumulatywną i kumulatywną) w zależności od kształtu kolejnych iteracji.\n\n\n\nRegressor chains\n\n\nPonieważ, jak można się spodziewać wyniki modelowania w znaczny sposób zależą od wylosowanej permutacji, to w metodzie zaproponowanej przez Melki i in. (2017) aby uniknąć tego efektu buduje się \\(k\\) modeli dla różnych permutacji i łączy się wyniki w podobny sposób jak w lasach losowych.\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nSłowo komentarza jeśli chodzi o dostępność tych metod w językach programowania. Niestety wspomniane metody w R nie są zaimplementowane w sposób, który pozwalałby na bezpieczne używanie przygotowanych rozwiązań. Istnieje kilka wzmianek1 na ten temat. Twórcy dwóch głównych frameworków do uczenia maszynowego, czyli mlr3 oraz tidymodels przygotowują implementacje tych metod. Dodatkowo istnieje rozwiązanie w wersji eksperymentalnej mtr-toolkit, które pozwala na wykonanie modelowania z wieloma wyjściami, którym można się posiłkować.\nNiestety w przypadku Python-a nie jest dużo lepiej. Wprawdzie w pakiecie scikit-learn istnieją implementacje pozwalające na predykcje wielowyjściowe w obu typach zadań poprzez MultiOutputRegressor i MultiOutputClassifier, ale dokonują one predykcji naiwnej poprzez złożenie w listę wyników pojedynczych modeli dla każdej zmiennej. Nieco lepiej sprawa wygląda w przypadku metod łańcuchowych, ponieważ zarówno dla klasyfikacji, jak i regresji są metody to realizujące (ClassifierChain i RegressorChain).\n\n\n1 na dzień dzisiejszy, czyli początek 2024 roku\n\n\n2.2.2 Adaptacja algorytmu\nNiestety tej metody nie da się zastosować do każdego typu modelu.\n\n\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, i Pedro Larrañaga. 2015. „A Survey on Multi-Output Regression”. WIREs Data Mining and Knowledge Discovery 5 (5): 216–33. https://doi.org/10.1002/widm.1157.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, i Sebastián Ventura. 2017. „Multi-Target Support Vector Regression via Correlation Regressor Chains”. Information Sciences 415–416 (listopad): 53–69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello Mastelini, Fabio Luiz Melquiades, i Sylvio Barbon Jr. 2020. „Improved Prediction of Soil Properties with Multi-Target Stacked Generalisation on EDXRF Spectra”. arXiv preprint arXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves, i Ioannis Vlahavas. 2016. „Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs”. Machine Learning 104 (1): 55–98. https://doi.org/10.1007/s10994-016-5546-z.\n\n\nTawiah, Clifford, i Victor Sheng. 2013. „Empirical Comparison of Multi-Label Classification Algorithms”. W Proceedings of the AAAI Conference on Artificial Intelligence, 27:1645–46.\n\n\nTsoumakas, Grigorios, i Ioannis Katakis. 2007. „Multi-Label Classification: An Overview”. International Journal of Data Warehousing and Mining (IJDWM) 3 (3): 1–13.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Borchani, Hanen, Gherardo Varando, Concha Bielza, and Pedro Larrañaga.\n2015. “A Survey on Multi-Output Regression.” WIREs Data\nMining and Knowledge Discovery 5 (5): 216–33. https://doi.org/10.1002/widm.1157.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, and Sebastián Ventura.\n2017. “Multi-Target Support Vector Regression via Correlation\nRegressor Chains.” Information Sciences 415–416\n(November): 53–69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello\nMastelini, Fabio Luiz Melquiades, and Sylvio Barbon Jr. 2020.\n“Improved Prediction of Soil Properties with Multi-Target Stacked\nGeneralisation on EDXRF Spectra.” arXiv Preprint\narXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves,\nand Ioannis Vlahavas. 2016. “Multi-Target Regression\nvia Input Space Expansion: Treating Targets as\nInputs.” Machine Learning 104 (1): 55–98.\nhttps://doi.org/10.1007/s10994-016-5546-z.\n\n\nTawiah, Clifford, and Victor Sheng. 2013. “Empirical Comparison of\nMulti-Label Classification Algorithms.” In Proceedings of the\nAAAI Conference on Artificial\nIntelligence, 27:1645–46.\n\n\nTsoumakas, Grigorios, and Ioannis Katakis. 2007. “Multi-Label\nClassification: An Overview.” International\nJournal of Data Warehousing and Mining (IJDWM) 3 (3): 1–13.",
    "crumbs": [
      "Literatura"
    ]
  }
]