[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaawansowane metody uczenia maszynowego",
    "section": "",
    "text": "WstÄ™p\nKsiÄ…Å¼ka ta jest napisana na potrzeby prowadzenia zajÄ™Ä‡ na kierunku InÅ¼ynieria i analiza danych z przedmiotu Zaawansowane metody uczenia maszynowego. Jest swego rodzaju autorskim podejÅ›ciem do tematu, przedstawiajÄ…cym wybrane metody uczenia maszynowego, ktÃ³re rzadziej wystÄ™pujÄ… w opracowaniach na temat uczenia maszynowego.\nUczenie maszynowe stanowi obszar intensywnego rozwoju, ktÃ³ry obejmuje szereg technik umoÅ¼liwiajÄ…cych bardziej skomplikowane i wydajne modele predykcyjne. WÅ›rÃ³d tych metod warto wyrÃ³Å¼niÄ‡ gÅ‚Ä™bokie sieci neuronowe, zwÅ‚aszcza konwolucyjne sieci neuronowe (CNN) i rekurencyjne sieci neuronowe (RNN). CNN sÄ… wykorzystywane w zadaniach przetwarzania obrazÃ³w, gdzie potrafiÄ… efektywnie ekstrahowaÄ‡ hierarchiczne cechy z danych wejÅ›ciowych, natomiast RNN sÄ… efektywne w analizie sekwencji danych, takich jak jÄ™zyk naturalny. Ponadto, metody uczenia maszynowego obejmujÄ… techniki transferu wiedzy, uczenie ze wzmocnieniem, generatywne modele, takie jak generatywne sieci przeciwdziedzinowe (GAN), czy teÅ¼ autokodery. Te nowoczesne podejÅ›cia umoÅ¼liwiajÄ… modelom uczÄ…cym siÄ™ wykonywanie bardziej zÅ‚oÅ¼onych zadaÅ„, a takÅ¼e adaptacjÄ™ do rÃ³Å¼norodnych danych wejÅ›ciowych, co sprawia, Å¼e sÄ… one stosowane w obszarach takich jak rozpoznawanie obrazÃ³w, przetwarzanie jÄ™zyka naturalnego, czy nawet w autonomicznych systemach decyzyjnych.\nPonadto, zaawansowane metody uczenia maszynowego obejmujÄ… takÅ¼e techniki regularyzacji, optymalizacji i inÅ¼ynieriÄ™ cech. Regularyzacja ma na celu zapobieganie przeuczeniu poprzez kontrolowanie zÅ‚oÅ¼onoÅ›ci modelu, natomiast optymalizacja skupia siÄ™ na dostosowywaniu wag modelu w celu minimalizacji funkcji straty. InÅ¼ynieria cech polega na rÄ™cznym lub automatycznym dostosowywaniu danych wejÅ›ciowych w celu uzyskania lepszych wynikÃ³w modelu. DziÄ™ki tym zaawansowanym metodom, uczenie maszynowe staje siÄ™ coraz bardziej potÄ™Å¼nym narzÄ™dziem w analizie danych i podejmowaniu skomplikowanych decyzji w rÃ³Å¼nych dziedzinach.\nModele predykcyjne dla wielu wyjÅ›Ä‡, czyli tzw. multi-target regression and classification, stanowiÄ… kolejny istotny obszar w dziedzinie uczenia maszynowego. W przypadku multi-target regression, celem jest przewidywanie wielu wartoÅ›ci wyjÅ›ciowych dla danego zestawu wejÅ›ciowego, co czÄ™sto spotyka siÄ™ w zÅ‚oÅ¼onych problemach predykcyjnych, takich jak prognozowanie wielu parametrÃ³w jednoczeÅ›nie. Z kolei w przypadku multi-target classification, model ma za zadanie przypisanie jednego lub wiÄ™cej klas dla kaÅ¼dego przykÅ‚adu wejÅ›ciowego. Te modele sÄ… powszechnie stosowane w rÃ³Å¼nych dziedzinach, takich jak bioinformatyka, finanse czy przemysÅ‚, gdzie jednoczesne przewidywanie wielu zmiennych jest kluczowe dla skutecznego rozwiÄ…zania problemu. WdroÅ¼enie takich zaawansowanych modeli predykcyjnych wymaga starannej obrÃ³bki danych, odpowiedniego dostosowania architektury modelu oraz precyzyjnej oceny wynikÃ³w, co sprawia, Å¼e sÄ… one istotnym narzÄ™dziem w obszarze analizy danych i podejmowania decyzji.\nModele jÄ™zykowe stanowiÄ… jeszcze jeden kluczowy obszar w dziedzinie uczenia maszynowego, skoncentrowany na zrozumieniu i generowaniu ludzkiego jÄ™zyka naturalnego. GÅ‚Ä™bokie sieci neuronowe, zwÅ‚aszcza rekurencyjne sieci neuronowe (RNN) i transformery, zostaÅ‚y skutecznie wykorzystane do tworzenia modeli jÄ™zykowych o zdolnoÅ›ciach przetwarzania i generowania tekstu na poziomie zbliÅ¼onym do ludzkiego. Te modele zdolne sÄ… do zrozumienia kontekstu, analizy gramatyki, a takÅ¼e generowania spÃ³jnych i sensownych odpowiedzi. Wykorzystywane sÄ… w rÃ³Å¼norodnych zastosowaniach, takich jak tÅ‚umaczenie maszynowe, generowanie tekstu, czy analiza nastroju w tekÅ›cie. Ponadto, pre-trenowane modele jÄ™zykowe, takie jak BERT czy GPT (Generative Pre-trained Transformer), zdobywajÄ… popularnoÅ›Ä‡, umoÅ¼liwiajÄ…c dostosowanie ich do rÃ³Å¼nych zadaÅ„ poprzez fine-tuning. W miarÄ™ postÄ™pu badaÅ„ i rozwoju w tej dziedzinie, modele jÄ™zykowe stajÄ… siÄ™ coraz bardziej zaawansowane, co przyczynia siÄ™ do doskonalenia komunikacji miÄ™dzy maszynami a ludÅºmi oraz do rozwijania nowych moÅ¼liwoÅ›ci w dziedzinie przetwarzania jÄ™zyka naturalnego.\nWspomniane powyÅ¼ej metody i modele bÄ™dÄ… stanowiÄ‡ treÅ›Ä‡ wykÅ‚adÃ³w z wspomnianego na wstÄ™pie przedmiotu.",
    "crumbs": [
      "WstÄ™p"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1Â  Wprowadzenie",
    "section": "",
    "text": "Witam w Å›wiecie zaawansowanych metod uczenia maszynowego ğŸ¤–, prezentowanej w niniejszej publikacji. KsiÄ…Å¼ka ta skupia siÄ™ na trzech gÅ‚Ã³wnych obszarach, zaczynajÄ…c od wielowymiarowych problemÃ³w predykcyjnych, przechodzÄ…c przez kompleksowe modele gÅ‚Ä™bokich sieci neuronowych, a koÅ„czÄ…c na zaawansowanych modelach jÄ™zykowych. Koncepcyjnie rozpoczniemy od omÃ³wienia multiple target regression and classification, gdzie przedstawimy skomplikowane zadania predykcyjne wymagajÄ…ce jednoczesnej prognozy wielu zmiennych. Przeanalizujemy praktyczne zastosowania tych modeli w obszarach, takich jak nauki spoÅ‚eczne, biologia i finanse.\nNastÄ™pnie poÅ›wiÄ™cimy uwagÄ™ gÅ‚Ä™bokim sieciom neuronowym, gÅ‚Ã³wnemu filarowi nowoczesnej sztucznej inteligencji. OmÃ³wimy ewolucjÄ™ od konwolucyjnych sieci neuronowych (CNN) do rekurencyjnych sieci neuronowych (RNN), zwracajÄ…c uwagÄ™ na ich zdolnoÅ›Ä‡ do efektywnego przetwarzania obrazÃ³w, sekwencji danych i rozwiÄ…zania bardziej zÅ‚oÅ¼onych problemÃ³w. W ramach tego obszaru, przyjrzymy siÄ™ rÃ³wnieÅ¼ technikom transferu wiedzy, uczenia ze wzmocnieniem oraz generatywnym modelom, takim jak generatywne sieci przeciwdziedzinowe (GAN), ktÃ³re poszerzajÄ… granice moÅ¼liwoÅ›ci maszynowego uczenia siÄ™.\n\n\n\nTrzeci kluczowy obszar, ktÃ³ry bÄ™dzie przedmiotem analizy, to modele jÄ™zykowe. RozwaÅ¼ania rozpoczniemy od gÅ‚Ä™bokich sieci neuronowych, a nastÄ™pnie skoncentrujemy siÄ™ na transformatorach, ktÃ³re rewolucjonizujÄ… przetwarzanie jÄ™zyka naturalnego ğŸ‘…. Przedstawimy praktyczne zastosowania tych modeli, zwÅ‚aszcza w tÅ‚umaczeniu maszynowym, generowaniu tekstu i analizie sentymentu. Ponadto, omÃ³wimy pre-trenowane modele jÄ™zykowe, takie jak BERT czy GPT, jako kluczowe narzÄ™dzia adaptacyjne, zdolne do fine-tuningu w zaleÅ¼noÅ›ci od konkretnego zadania.\nKaÅ¼dy podejmowany temat bÄ™dzie wzbogacony o implementacjÄ™ analizowanych metod w realnych scenariuszach. OmÃ³wimy kroki od obrÃ³bki danych, przez dostosowywanie architektury modelu, aÅ¼ po ocenÄ™ wynikÃ³w. W tym kontekÅ›cie poruszymy takÅ¼e aspekty etyczne i wyzwania zwiÄ…zane z zastosowaniem zaawansowanych modeli uczenia maszynowego.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html",
    "href": "multi_target_models.html",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "",
    "text": "2.1 Typy modeli z wieloma zmiennymi wynikowymi\nWÅ›rÃ³d nadzorowanych modeli uczenia maszynowego z wieloma zmiennymi wynikowymi moÅ¼na wymieniÄ‡ zarÃ³wno te dedykowane do klasyfikacji, jak i regresji. Modele te sÄ… znane jako modele z wieloma wyjÅ›ciami (klasyfikacyjne) lub modele z wieloma wyjÅ›ciami (regresyjne), w zaleÅ¼noÅ›ci od rodzaju problemu, ktÃ³ry rozwiÄ…zujÄ….",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "href": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "",
    "text": "Modele z wieloma wyjÅ›ciami (klasyfikacyjne)\nW przypadku klasyfikacji, gdy mamy wiele kategorii (klas) jako zmiennÄ… wynikowÄ…, modele te sÄ… nazywane modelami z wieloma wyjÅ›ciami. PrzykÅ‚ady obejmujÄ… algorytmy takie jak regresja logistyczna, metoda k najbliÅ¼szych sÄ…siadÃ³w (k-NN) czy algorytmy drzew decyzyjnych, ktÃ³re zostaÅ‚y dostosowane do obsÅ‚ugi wielu klas.\nPrzykÅ‚adowe zadanie: ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych dotyczÄ…cy rÃ³Å¼nych rodzajÃ³w owocÃ³w (np. jabÅ‚ek, pomaraÅ„czy, bananÃ³w) i chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje gatunek owocu oraz kolor owocu. Mamy wiÄ™c dwie zmienne wynikowe: gatunek (klasyfikacja wieloklasowa) i kolor (klasyfikacja wieloklasowa).\nModele z wieloma wyjÅ›ciami (regresyjne).\nW przypadku regresji, gdzie zmiennÄ… wynikowÄ… jest wektor wartoÅ›ci numerycznych, modele te sÄ… nazywane modelami z wieloma wyjÅ›ciami. PrzykÅ‚ady obejmujÄ… algorytmy regresji liniowej lub nieliniowej, algorytmy oparte na drzewach decyzyjnych, czy teÅ¼ bardziej zaawansowane modele, takie jak sieci neuronowe.\nPrzykÅ‚adowe zadanie: ZakÅ‚adamy, Å¼e mamy zbiÃ³r danych zawierajÄ…cy informacje o pracownikach, takie jak doÅ›wiadczenie zawodowe, poziom wyksztaÅ‚cenia, liczba godzin pracy tygodniowo itp. Chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje zarobki pracownikÃ³w oraz ich poziom satysfakcji zawodowej.\nModele wielozadaniowe.\nModele wielozadaniowe to rodzaj nadzorowanego uczenia maszynowego, w ktÃ³rym model jest trenowany jednoczeÅ›nie do rozwiÄ…zania kilku zadaÅ„. Te zadania mogÄ… obejmowaÄ‡ zarÃ³wno klasyfikacjÄ™, jak i regresjÄ™. DziÄ™ki wspÃ³lnemu trenowaniu modelu na wielu zadaniach, moÅ¼na uzyskaÄ‡ korzyÅ›ci w postaci wspÃ³lnego wykorzystywania wiedzy miÄ™dzy zadaniami.\nPrzykÅ‚adowe zadanie: ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych dotyczÄ…cy zakupÃ³w klientÃ³w w sklepie internetowym. Dla kaÅ¼dego klienta mamy informacje o rÃ³Å¼nych aspektach zakupÃ³w, takich jak czas dostawy, Å‚atwoÅ›Ä‡ obsÅ‚ugi strony, jakoÅ›Ä‡ produktÃ³w itp. Chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje dwie zmienne wynikowe: jakoÅ›Ä‡ obsÅ‚ugi klienta (skala jakoÅ›ciowa, np. â€œNiskaâ€, â€œÅšredniaâ€, â€œWysokaâ€) oraz caÅ‚kowity wydatek klienta (zmienna iloÅ›ciowa, np. kwota zakupÃ³w).\nModele hierarchiczne.\nW niektÃ³rych przypadkach, szczegÃ³lnie gdy mamy hierarchiÄ™ zmiennych wynikowych, modele te mogÄ… byÄ‡ budowane w sposÃ³b hierarchiczny. PrzykÅ‚adowo, w problemie klasyfikacji obrazÃ³w z hierarchiÄ… kategorii (na przykÅ‚ad rozpoznawanie gatunkÃ³w zwierzÄ…t), model moÅ¼e byÄ‡ zaprojektowany do rozpoznawania zarÃ³wno ogÃ³lnych, jak i bardziej szczegÃ³Å‚owych kategorii.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#rÃ³Å¼nie-podejÅ›cia-do-modelowania-z-wieloma-wyjÅ›ciami",
    "href": "multi_target_models.html#rÃ³Å¼nie-podejÅ›cia-do-modelowania-z-wieloma-wyjÅ›ciami",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "2.2 RÃ³Å¼nie podejÅ›cia do modelowania z wieloma wyjÅ›ciami",
    "text": "2.2 RÃ³Å¼nie podejÅ›cia do modelowania z wieloma wyjÅ›ciami\nIstniejÄ… dwa ogÃ³lne podejÅ›cia do rozwiÄ…zywania problemÃ³w wieloetykietowych: transformacja problemu i adaptacja algorytmu. Transformacja problemu polega na manipulowaniu zbiorem danych w taki sposÃ³b, Å¼e problem wieloetykietowy staje siÄ™ jednym lub kilkoma problemami jednoetykietowymi (Tawiah i Sheng 2013). Adaptacja algorytmu polega na tym, Å¼e sam algorytm jest w stanie poradziÄ‡ sobie bezpoÅ›rednio z problemem wieloetykietowym. Okazuje siÄ™, Å¼e wiele, choÄ‡ nie wszystkie, metody adaptacji algorytmÃ³w metod adaptacji algorytmÃ³w w rzeczywistoÅ›ci wykorzystuje transformacjÄ™ problemu (Tsoumakas i Katakis 2007).\n\n2.2.1 Transformacja problemu\nTechniki te przewidujÄ… stworzenie indywidualnego modelu dla kaÅ¼dego celu, a nastÄ™pnie poÅ‚Ä…czenie oddzielnych modeli w celu uzyskania ogÃ³lnej prognozy. Metody transformacji problemÃ³w okazaÅ‚y siÄ™ lepsze od metod adaptacji algorytmÃ³w pod wzglÄ™dem dokÅ‚adnoÅ›ci (Spyromitros-Xioufis i in. 2016). Co wiÄ™cej, podstawowa zasada sprawia, Å¼e metody transformacji problemu sÄ… niezaleÅ¼ne od algorytmu. W konsekwencji, moÅ¼na je Å‚atwo dostosowaÄ‡ do danego problemu poprzez zastosowanie odpowiednich bazowych metod uczÄ…cych. Punkt ten ma rÃ³wnieÅ¼ szczegÃ³lne znaczenie dla modeli typu ensemble, ktÃ³re Å‚Ä…czÄ… oszacowania z wielu potencjalnie rÃ³Å¼nych algorytmÃ³w w ostatecznÄ… prognozÄ™. Niedawno Spyromitros-Xioufis i in. (2016) zaproponowali rozszerzenie znanych metod transformacji klasyfikacji wieloetykietowej, aby poradziÄ‡ sobie z problemem regresji wielowynikowej i modelowaÄ‡ zaleÅ¼noÅ›ci miÄ™dzy celami. W szczegÃ³lnoÅ›ci wprowadzili oni dwa nowe podejÅ›cia do regresji wielocelowej, skÅ‚adanie regresorÃ³w wielocelowych i Å‚aÅ„cuchy regresorÃ³w, inspirowane popularnymi i skutecznymi podejÅ›ciami do klasyfikacji wieloznaczeniowej.\nPodstawowÄ… koncepcjÄ… w metodach transformacji problemÃ³w jest wykorzystanie poprzednich modeli do nowego przewidywania poprzez rozszerzonÄ… przestrzeÅ„ cech (Borchani i in. 2015). Stacked generalization to podejÅ›cie do meta-uczenia, ktÃ³re wykorzystuje dane wyjÅ›ciowe wczeÅ›niej wyuczonych modeli do uczenia siÄ™ nowego modelu. W zwiÄ…zku z tym poczÄ…tkowe dane wyjÅ›ciowe modelu sÄ… traktowane jako nowe cechy i sÄ… ukÅ‚adane w stos do poczÄ…tkowego wektora cech przed ponownym uczeniem. W oryginalnym sformuÅ‚owaniu przewidziano tylko dwuetapowÄ… procedurÄ™, tj. poczÄ…tkowe modele wyuczone z poczÄ…tkowego wektora cech odpowiadajÄ… odpowiednio modelom i danym poziomu 0, a powiÄ™kszony wektor cech i ponownie wyuczony model sÄ… okreÅ›lane odpowiednio jako dane poziomu 1 i generalizator. JednakÅ¼e, rozsÄ…dnie rzecz biorÄ…c, ten proces ukÅ‚adania pojedynczego celu (ang. Single-target Stacking - STS) moÅ¼e byÄ‡ rÃ³wnieÅ¼ przeprowadzany w wielu iteracjach. Aby wdroÅ¼yÄ‡ tÄ™ zasadÄ™ dla problemÃ³w z wieloma celami, w ktÃ³rych kodowane sÄ… rÃ³wnieÅ¼ moÅ¼liwe korelacje miÄ™dzy zmiennymi docelowymi, wprowadzono koncepcjÄ™ ukÅ‚adania wielu celÃ³w (ang. Multi-target Stacking - MTS) (Borchani i in. 2015). Analogicznie do STS, szkolenie modelu MTS moÅ¼na uznaÄ‡ za procedurÄ™ dwuetapowÄ…. W pierwszym etapie uczone sÄ… niezaleÅ¼ne modele dla kaÅ¼dej zmiennej docelowej. NastÄ™pnie uczone sÄ… meta-modele dla kaÅ¼dej zmiennej docelowej z rozszerzonymi wektorami cech, ktÃ³re zawierajÄ… poczÄ…tkowe wektory cech, a takÅ¼e oszacowania poziomu 0 pozostaÅ‚ych zmiennych docelowych. Podobne pomysÅ‚y byÅ‚y rÃ³wnieÅ¼ stosowane w kontekÅ›cie modeli zespoÅ‚owych, tj. uczenia siÄ™ kilku modeli poziomu 0 dla kaÅ¼dej zmiennej docelowej, ktÃ³re sÄ… Å‚Ä…czone w procedurze uogÃ³lniania poziomu 1 dla wielu zmiennych docelowych (Santana i in. 2020).\n\n2.2.1.1 Single-target stacking\nMetoda ta jest stosowana przede wszystkim z zadaniach regresyjnych z wieloma wyjÅ›ciami. RozwaÅ¼my zbiÃ³r danych \\(D = \\left\\{\\left(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(N)}, \\mathbf{y}^{(N)}\\right)\\right\\}\\), skÅ‚adajÄ…cy siÄ™ z \\(N\\) obserwacji, ktÃ³re sÄ… realizacjami zmiennych losowych \\(X_1,\\ldots,X_m, Y_1,\\ldots,Y_d\\). Zatem kaÅ¼de wejÅ›cie do modelu jest charakteryzowane przez \\(m\\) zmiennych \\(\\mathbf{x}{(l)}=\\left(x_1^{(l)},\\ldots, x_j^{(l)}, \\ldots, x_m^{(l)} \\right)\\) oraz \\(d\\) odpowiadajÄ…cych im wyjÅ›Ä‡ \\(\\mathbf{y}{(l)}=\\left(y_1^{(l)},\\ldots, y_i^{(l)}, \\ldots, y_d^{(l)} \\right)\\), gdzie \\(l\\in\\{1,\\ldots,N\\}, j\\in\\{1,\\ldots,m\\}, i\\in\\{1,\\ldots,d\\}\\). Naszym celem w zadaniu regresyjnym (MTR - Multi-target Regression) jest nauczenie takiego modelu \\(h\\), ktÃ³ry przeksztaÅ‚ca \\(\\mathbf{x}\\) w \\(\\mathbf{y}\\).\nW podejÅ›ciu STS w pierwszym kroku budowanych jest \\(d\\) niezaleÅ¼nych modeli przewidujÄ…cych pojedyncze wyjÅ›cie. Po tej czynnoÅ›ci meta-model jest trenowany na zbiorze \\(D_i'\\), ktÃ³ry jest wzbogaconym zbiorem \\(D_i\\) o predykcje zmiennej \\(Y_i\\), czyli\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)} \\right)\\). W zaleÅ¼noÅ›ci czy rozpatrujemy algorytm STS niekumulatywny, czy kumulatywny, drugi krok iteracji wyglÄ…da nieco inaczej:\n\nniekumulatywny\n\\[\n\\bar{D}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i'^{(l)} \\right)\\)\nkumulatywny\n\\[\n\\bar{\\bar{D}}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)},\\hat{y}_i'^{(l)} \\right)\\).\n\n\n\n\nSingle-target stacking\n\n\n\n\n2.2.1.2 Multi-target stacking\nW przeciwieÅ„stwie do STS, MTS zostaÅ‚ zaprojektowany do dzielenia siÄ™ wiedzÄ… w skorelowanych zmiennych docelowych w ramach procedury Å‚Ä…czenia w stosy. Podobnie, najpierw uczone sÄ… modele pojedynczego celu. NastÄ™pnie tworzony jest zestaw meta-modeli, ktÃ³re zawierajÄ… model dla kaÅ¼dej zmiennej docelowej \\(Y_i,\\) \\(i \\in \\{1, \\ldots, d\\}\\). W ten sposÃ³b uwzglÄ™dniane sÄ… szacunki dotyczÄ…ce pozostaÅ‚ych zmiennych docelowych z pierwszego etapu, tj. model jest uczony z przeksztaÅ‚conego zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_1^{(l)},\\ldots,\\hat{y}_d^{(l)} \\right)\\). W metodzie MTS istniejÄ… rÃ³wnieÅ¼ dwa sposoby skÅ‚adania kolejnych iteracji. PrzebiegajÄ… one w podobny sposÃ³b jak w przypadku STS.\n\n\n\nMulti-target stacking\n\n\nIstnieje jeszcze trzecia metoda powszechnie stosowana do predykcji wielowyniowej zwana Regressor Chains lub Classifier Chains w zaleÅ¼noÅ›ci od celu zadania. IdÄ™ dziaÅ‚ania tej metody przedstawiÄ™ na przykÅ‚adzie modelu regresyjnego.\n\n\n2.2.1.3 Regressor Chains\nRC opierajÄ… siÄ™ na idei dopasowywania modeli pojedynczego celu wzdÅ‚uÅ¼ wybranej permutacji, tj. Å‚aÅ„cucha. Najpierw losowana jest permutacja w odniesieniu do zmiennych docelowych. Proces ten moÅ¼na przeprowadziÄ‡ w sposÃ³b losowy (Spyromitros-Xioufis i in. 2016) lub uporzÄ…dkowany (Melki i in. 2017). Wybrana permutacja jest wykorzystywana do zbudowania oddzielnego modelu regresji dla zmiennych docelowych zgodnie z kolejnoÅ›ciÄ… permutacji. Aby wykorzystaÄ‡ tÄ™ strukturÄ™ do MTR, rzeczywiste wartoÅ›ci zmiennych docelowych sÄ… dostarczane do kolejnych modeli podczas uczenia siÄ™ wzdÅ‚uÅ¼ Å‚aÅ„cucha. Na podstawie peÅ‚nego Å‚aÅ„cucha lub wybranego zestawu \\(C = (Y_1,\\ldots,Y_d)\\), pierwszy model jest ograniczony do ustalenia predykcji dla \\(Y_1\\). NastÄ™pnie, kolejno dla \\(Y_i\\) uczone sÄ… modele na podstawie zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, y_1^{(l)},\\ldots, y_{i-1}^{(l)} \\right)\\). Ten algorytm ma rÃ³wnieÅ¼ dwie odmiany (niekumulatywnÄ… i kumulatywnÄ…) w zaleÅ¼noÅ›ci od ksztaÅ‚tu kolejnych iteracji.\n\n\n\nRegressor chains\n\n\nPoniewaÅ¼, jak moÅ¼na siÄ™ spodziewaÄ‡ wyniki modelowania w znaczny sposÃ³b zaleÅ¼Ä… od wylosowanej permutacji, to w metodzie zaproponowanej przez Melki i in. (2017) aby uniknÄ…Ä‡ tego efektu buduje siÄ™ \\(k\\) modeli dla rÃ³Å¼nych permutacji i Å‚Ä…czy siÄ™ wyniki w podobny sposÃ³b jak w lasach losowych.\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nSÅ‚owo komentarza jeÅ›li chodzi o dostÄ™pnoÅ›Ä‡ tych metod w jÄ™zykach programowania. Niestety wspomniane metody w R nie sÄ… zaimplementowane w sposÃ³b, ktÃ³ry pozwalaÅ‚by na bezpieczne uÅ¼ywanie przygotowanych rozwiÄ…zaÅ„. Istnieje kilka wzmianek (na dzieÅ„ dzisiejszy, czyli poczÄ…tek 2024 roku) na ten temat. TwÃ³rcy dwÃ³ch gÅ‚Ã³wnych frameworkÃ³w do uczenia maszynowego, czyli mlr3 oraz tidymodels przygotowujÄ… implementacje tych metod. Dodatkowo istnieje rozwiÄ…zanie w wersji eksperymentalnej mtr-toolkit, ktÃ³re pozwala na wykonanie modelowania z wieloma wyjÅ›ciami, ktÃ³rym moÅ¼na siÄ™ posiÅ‚kowaÄ‡. Na potrzeby klasyfikacji istnieje rÃ³wnieÅ¼ pakiet mldr i ultim, ktÃ³re pozwalajÄ… na uczenie modeli klasyfikacyjnymi z wieloma wyjÅ›ciami.\nNiestety w przypadku Python-a nie jest duÅ¼o lepiej. Wprawdzie w pakiecie scikit-learn istniejÄ… implementacje pozwalajÄ…ce na predykcje wielowyjÅ›ciowe w obu typach zadaÅ„ poprzez MultiOutputRegressor i MultiOutputClassifier, ale dokonujÄ… one predykcji naiwnej poprzez zÅ‚oÅ¼enie w listÄ™ wynikÃ³w pojedynczych modeli dla kaÅ¼dej zmiennej. Nieco lepiej sprawa wyglÄ…da w przypadku metod Å‚aÅ„cuchowych, poniewaÅ¼ zarÃ³wno dla klasyfikacji, jak i regresji sÄ… metody to realizujÄ…ce (ClassifierChain i RegressorChain).\n\n\n\n\n\n2.2.2 Adaptacja algorytmu\nProstota podejÅ›cia transformacji problemu sprawia, Å¼e jest ono odpowiednie dla problemÃ³w, w ktÃ³rych jego wady majÄ… niewielki lub Å¼aden wpÅ‚yw - jednak dla zÅ‚oÅ¼onych problemÃ³w podejÅ›cie adaptacji algorytmu moÅ¼e okazaÄ‡ siÄ™ bardziej efektywne. Dodatkowo, dowody empiryczne sugerujÄ…, Å¼e uczenie siÄ™ powiÄ…zanych zadaÅ„ jednoczeÅ›nie, a nie niezaleÅ¼nie, moÅ¼e poprawiÄ‡ wyniki predykcyjne (Evgeniou i Pontil 2004). Z drugiej strony, jeÅ›li zadania sÄ… bardzo odmienne, wydajnoÅ›Ä‡ predykcyjna moÅ¼e ucierpieÄ‡, gdy zadania sÄ… uczone razem, a nie niezaleÅ¼nie (Faddoul i in. 2010). W zwiÄ…zku z tym moÅ¼emy wyciÄ…gnÄ…Ä‡ nastÄ™pujÄ…ce wnioski:\n\njeÅ›li zadania, ktÃ³rych nasz predyktor ma siÄ™ nauczyÄ‡, sÄ… powiÄ…zane, powinniÅ›my dÄ…Å¼yÄ‡ do znalezienia odpowiedniej metody adaptacji algorytmu;\njeÅ›li zadania, ktÃ³rych chcemy siÄ™ nauczyÄ‡, nie sÄ… powiÄ…zane, powinniÅ›my zamiast tego dÄ…Å¼yÄ‡ do znalezienia odpowiedniej metody transformacji problemu.\n\nWreszcie, powinniÅ›my wziÄ…Ä‡ pod uwagÄ™ rozmiar problemu i zdaÄ‡ sobie sprawÄ™, Å¼e gdy zadania sÄ… niepowiÄ…zane, istnieje potencjalny kompromis miÄ™dzy efektywnoÅ›ciÄ… czasowÄ… a wydajnoÅ›ciÄ… predykcyjnÄ… przy wyborze metody transformacji problemu lub metody adaptacji algorytmu. W przypadku niepowiÄ…zanych ze sobÄ… zadaÅ„, metody transformacji problemu mogÄ… zwiÄ™kszaÄ‡ skutecznoÅ›Ä‡ predykcyjnÄ…, ale zmniejszaÄ‡ wydajnoÅ›Ä‡ czasowÄ… w przypadku duÅ¼ych problemÃ³w i odwrotnie.\nNiestety tej metody nie da siÄ™ zastosowaÄ‡ do kaÅ¼dego typu modelu. Rodzina modeli, ktÃ³rych adaptacja jest wykonana caÅ‚y czas roÅ›nie. Adaptacja modelu polega na przeksztaÅ‚ceniu go do postaci, w ktÃ³rej da siÄ™ wykonaÄ‡ predykcjÄ™ dla wielu wyjÅ›Ä‡. WÅ›rÃ³d modeli, ktÃ³rych wersje native multi-target istniejÄ… naleÅ¼y wymieniÄ‡:\n\nregresja wieloraka (Izenman 1975)\nkNN\ndrzewo decyzyjne (Struyf i DÅ¾eroski 2006)\nlas losowy (Kocev i in. 2013)\nbagging (Kocev i in. 2013)\ngradient boosting (Zhang i Jung, b.d.; Faddoul i in. 2012)\nSVM (Xu, Guo, i Wang 2013; Vazquez i Walter 2003)\nno i oczywiÅ›cie sieci neuronowe.\n\nNie sposÃ³b przedstawiÄ‡ w jaki sposÃ³b wprowadzone zostaÅ‚y zmiany we wszystkich algorytmach. SkupiÄ™ siÄ™ jednak na pokazaniu adaptacji drzew decyzyjnych do predykcji wielu wyjÅ›Ä‡ jednoczeÅ›nie, poniewaÅ¼ jest to meta-model modeli takich jak lasy losowe, bagging czy boosting.\n\n2.2.2.1 Adaptacja klasyfikacyjnego drzewa decyzyjnego\nFaddoul i in. (2012) zaproponowali zmodyfikowanÄ… wersjÄ™ algorytmu drzewa decyzyjnego C4.5 (Quinlan 1993), ktÃ³ra bezpoÅ›rednio obsÅ‚uguje problemy klasyfikacji wielowyjÅ›ciowej. Zmodyfikowana wersja (nazwana MT-DT) rÃ³Å¼ni siÄ™ od standardowej implementacji C4.5 w dwÃ³ch krytycznych aspektach: kryteriach podziaÅ‚u wÄ™zÅ‚Ã³w i procesie decyzyjnym. Faddoul i in. (2012) proponujÄ… trzy rÃ³Å¼ne podejÅ›cia do Å‚Ä…czenia wielu miar przyrostu informacji w jednÄ… miarÄ™: wspÃ³lny przyrost informacji, suma niewaÅ¼ona i maksymalny przyrost informacji. WspÃ³lny przyrost informacyjny jest definiowany przy uÅ¼yciu konkatenacji wszystkich poszczegÃ³lnych zadaÅ„, tj. wzglÄ™dnej rÃ³Å¼nicy w entropii mierzonej we wszystkich zadaniach decyzyjnych. Autorzy pokazujÄ…, Å¼e niewaÅ¼ona suma (RÃ³wnanieÂ 2.1) indywidualnych przyrostÃ³w informacyjnych wszystkich zadaÅ„ jest rÃ³wnowaÅ¼na wspÃ³lnemu przyrostowi informacyjnemu.\n\\[\nIG_U=\\sum_YIG_Y\n\\tag{2.1}\\]\nMaksymalny przyrost informacyjny, zgodnie z propozycjÄ… autorÃ³w jest definiowany po prostu jako maksymalny przyrost informacyjny wszystkich zadaÅ„:\n\\[\nIG_M=\\max_YIG_Y\n\\tag{2.2}\\]\nBadania eksperymentalne pokazaÅ‚y, Å¼e maksymalny przyrost informacyjny wykorzystany do budowania reguÅ‚ podziaÅ‚u, charakteryzuje siÄ™ wyÅ¼szym poziomem dopasowania modeli, niÅ¼ przy zastosowaniu \\(IG_U\\) i \\(IG_J\\).\nW przypadku klasyfikacji z jednÄ… etykietÄ…, algorytm indukcji drzewa decyzyjnego (taki jak C4.5) rekurencyjnie dzieli wÄ™zÅ‚y, dodajÄ…c (zazwyczaj dwa) elementy potomne, aÅ¼ moÅ¼liwe jest utworzenie liÅ›cia takiego, Å¼e znaczna wiÄ™kszoÅ›Ä‡ (lub nawet wszystkie) jego przykÅ‚adowych instancji naleÅ¼y do tej samej klasy. W przypadku wielu wyjÅ›Ä‡, indukcja drzewa niekoniecznie jest tak prosta. RozwaÅ¼my problem klasyfikacji wielowyjÅ›ciowej z dwoma wyjÅ›ciami binarnymi \\(\\nu_1\\) i \\(\\nu_2\\); moÅ¼liwe jest, Å¼e po \\(t\\) podziaÅ‚ach, wÄ™zeÅ‚ zawiera tylko wartoÅ›ci pozytywne dla \\(\\nu_1\\), ale mieszankÄ™ wartoÅ›ci pozytywnych i negatywnych dla \\(\\nu_2\\) - stÄ…d, podczas konstruowania drzew decyzyjnych dla wielu jednoczesnych zadaÅ„, naleÅ¼y pamiÄ™taÄ‡, Å¼e proces decyzyjny dla pewnego zadania moÅ¼e wymagaÄ‡ krÃ³tszej Å›cieÅ¼ki decyzyjnej niÅ¼ inne zadania w ramach tego samego problemu wielowyjÅ›ciowego. MT-DT radzi sobie z tym, sprawdzajÄ…c w kaÅ¼dym wÄ™Åºle, czy moÅ¼liwe jest utworzenie wÄ™zÅ‚a terminalnego dla ktÃ³regokolwiek z zadaÅ„ - w powyÅ¼szym przykÅ‚adzie spowodowaÅ‚oby to utworzenie drzewa, w ktÃ³rym wewnÄ™trzny wÄ™zeÅ‚ \\(t_1\\) jest oznaczony jako wÄ™zeÅ‚ zatrzymania dla \\(\\nu_1\\), oznaczony klasÄ… pozytywnÄ…. PoniewaÅ¼ celem jest prognozowanie dla obu wyjÅ›Ä‡ binarnych, \\(t_1\\) nie jest wÄ™zÅ‚em liÅ›cia - zamiast tego rekurencyjne dzielenie jest kontynuowane od \\(t_1\\), aÅ¼ do znalezienia wÄ™zÅ‚a \\(t_2\\) takiego, Å¼e \\(t_2\\) jest wystarczajÄ…co czysty w odniesieniu do \\(\\nu_2\\), aby moÅ¼na byÅ‚o utworzyÄ‡ reguÅ‚Ä™ klasyfikacji dla drugiego zadania binarnego. W tym momencie wÄ™zÅ‚y decyzyjne (wÄ™zÅ‚y wewnÄ™trzne lub liÅ›cie) zostaÅ‚y znalezione dla wszystkich wynikÃ³w (\\(\\nu_1\\) i \\(\\nu_2\\)), a algorytm indukcji drzewa rekurencyjnego moÅ¼e zostaÄ‡ zakoÅ„czony.\nNic dziwnego, Å¼e klasyfikacja przy uÅ¼yciu juÅ¼ zbudowanego modelu MT-DT przebiega wedÅ‚ug tej samej formuÅ‚y, co jego indukcja - podczas przechodzenia przez drzewo kaÅ¼dy wÄ™zeÅ‚ jest sprawdzany w celu ustalenia, czy moÅ¼na podjÄ…Ä‡ decyzjÄ™ dla ktÃ³regokolwiek z aktualnie nierozstrzygniÄ™tych zadaÅ„. W przykÅ‚adzie \\(\\nu_1\\), \\(\\nu_2\\), klasyfikacja zostanie dokonana dla \\(\\nu_1\\) w wÄ™Åºle \\(t_1\\), poniewaÅ¼ jest on oznaczony jako wÄ™zeÅ‚ zatrzymania dla \\(\\nu_1\\); nastÄ™pnie przejÅ›cie jest kontynuowane do momentu napotkania \\(t_2\\) i klasyfikacja moÅ¼e zostaÄ‡ dokonana dla \\(\\nu_2\\). W tym momencie wszystkie wyjÅ›cia zostaÅ‚y sklasyfikowane, a przechodzenie moÅ¼e siÄ™ zakoÅ„czyÄ‡, zwracajÄ…c dwie wartoÅ›ci w \\(t_1\\) i \\(t_2\\) jako klasyfikacje odpowiednio dla \\(\\nu_1\\) i \\(\\nu_2\\).\n\n\n2.2.2.2 Adaptacja regresyjnego drzewa decyzyjnego\nSegal (1992) zaproponowaÅ‚ rozwiÄ…zanie dla drzew regresyjnych o wielu wyjÅ›ciach (MRT), ktÃ³re sÄ… w stanie przewidywaÄ‡ wyniki dla wielu powiÄ…zanych zadaÅ„ regresyjnych; te wielowyjÅ›ciowe drzewa regresyjne sÄ… oparte na funkcji podziaÅ‚u najmniejszych kwadratÃ³w zaproponowanej w ramach CART (Breiman i in. 2017). W przypadku drzewa regresyjnego o jednej odpowiedzi celem jest minimalizacja nastÄ™pujÄ…cej funkcji celu:\n\\[\n\\phi(t) = SS(t)-SS(t_L)-SS(t_R)\n\\]\ngdzie \\(SS(t)\\) jest zdefiniowana nastÄ™pujÄ…co\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))^2.\n\\]\nSegal (1992) dodaÅ‚ waÅ¼enie macierzÄ… kowariancji do bÅ‚Ä™du kwadratowego, co prowadzi algorytm drzewa do tworzenia wÄ™zÅ‚Ã³w potomnych, ktÃ³re reprezentujÄ… jednorodne klastry w odniesieniu do zestawu odpowiedzi wyjÅ›ciowych:\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))'V^{-1}(t)(y_i-\\bar{y}(t)),\n\\]\ngdzie \\(V(t)\\) oznacza macierz kowariancji w wÄ™Åºle \\(t\\).\n\n\n2.2.2.3 Adaptacja drzew decyzyjnych do realizacji obu zadaÅ„\nJak wspomniano wczeÅ›niej, jednÄ… z kluczowych motywacji do podejmowania prÃ³b rozwiÄ…zywania problemÃ³w rozpoznawania wzorcÃ³w z wieloma wyjÅ›ciami przy uÅ¼yciu metod adaptacji algorytmÃ³w jest oczekiwanie, Å¼e pojedynczy model wytrenowany na zestawie powiÄ…zanych zadaÅ„ wykaÅ¼e poprawÄ™ wydajnoÅ›ci predykcyjnej w porÃ³wnaniu do zestawu indywidualnych modeli, z ktÃ³rych kaÅ¼dy zostaÅ‚ wytrenowany na pojedynczym zadaniu. Rodzi to pytanie: co jeÅ›li problem wielowynikowy zawiera zarÃ³wno zadania klasyfikacji, jak i regresji? JeÅ›li zadania sÄ… niepowiÄ…zane, rozwiÄ…zanie takiego wspÃ³lnego problemu klasyfikacyjno-regresyjnego nie musi byÄ‡ trudniejsze niÅ¼ szkolenie zestawu klasyfikatorÃ³w i regresorÃ³w dla poszczegÃ³lnych zadaÅ„; jeÅ›li jednak zadania sÄ… powiÄ…zane, oczekujemy, Å¼e metoda adaptacji algorytmu zapewni lepsze wyniki pod wzglÄ™dem wydajnoÅ›ci predykcyjnej.\nGlocker i in. (2012) zaproponowaÅ‚ algorytm indukcji drzewa, ktÃ³ry jednoczeÅ›nie rozwiÄ…zuje jedno zadanie klasyfikacji i jedno zadanie regresji. Podobnie jak MT-DT i MRT, wspÃ³lne drzewo klasyfikacyjno-regresyjne (JCRT) rozwiÄ…zuje wiele jednoczesnych zadaÅ„ predykcji poprzez modyfikacjÄ™ funkcji podziaÅ‚u wÄ™zÅ‚a w kroku indukcyjnym i oznaczenie wÄ™zÅ‚Ã³w koÅ„cowych odpowiednimi wartoÅ›ciami dla kaÅ¼dego zadania. Ze wzglÄ™du na charakter wspÃ³lnych problemÃ³w klasyfikacyjno-regresyjnych, zmodyfikowana funkcja podziaÅ‚u jest wymagana do jednoczesnego uwzglÄ™dnienia bÅ‚Ä™du zarÃ³wno czÄ™Å›ci klasyfikacyjnej, jak i regresyjnej. Funkcja podziaÅ‚u zaproponowana przez Glocker i in. (2012) wykorzystuje funkcjÄ™ entropii skÅ‚adajÄ…cÄ… siÄ™ z trzech czÄ™Å›ci:\n\npo pierwsze, entropia Shannona jest obliczana dla czÄ™Å›ci klasyfikacji;\npo drugie, waÅ¼ona entropia rÃ³Å¼nicowa jest obliczana dla czÄ™Å›ci regresji;\npo trzecie, ze wzglÄ™du na fakt, Å¼e entropia Shannona i entropia rÃ³Å¼nicowa istniejÄ… w rÃ³Å¼nych zakresach, stosuje siÄ™ krok normalizacji w celu poÅ‚Ä…czenia dwÃ³ch entropii. Entropia Shannona jest obliczana tak, jak opisano wczeÅ›niej:\n\n\\[\nH_c(t) = \\sum_{c\\in C}p(c|x)\\log p(c|x).\n\\]\nMiara entropii rÃ³Å¼niczkowej stosowana przez Glocker i in. (2012) dla regresyjnej czÄ™Å›ci problemu jest obliczana w podobny sposÃ³b, z dwiema kluczowymi rÃ³Å¼nicami:\n\nzamiast sumowania prawdopodobieÅ„stw wartoÅ›ci nominalnych, entropia jest definiowana przez rÃ³Å¼niczkÄ™ funkcji prawdopodobieÅ„stwa wyjÅ›cia o wartoÅ›ci rzeczywistej;\ndodatkowo funkcja prawdopodobieÅ„stwa jest waÅ¼ona w klasach:\n\n\\[\nH_{r|c}(t) = \\sum_{c\\in C}p(c|x)\\int_{r\\in \\mathbb{R}^n}-p(r|c,x)\\log p(r|c,x)dr.\n\\]\nNastÄ™pnie dokonywana jest normalizacja ze wzglÄ™du na oba zadania, gdzie punktem odniesienie jest entropia w korzeniu:\n\\[\nH(t) = \\frac12\\left(\\frac{H_c(t)}{H_c(t_0)}+\\frac{H_{r|c}(t)}{H_{r|c}(t_0)}\\right).\n\\]\n\n\n\n\n\n\nAdnotacja\n\n\n\nJedyne implementacje, ktÃ³re znalazÅ‚em dla obu jÄ™zykÃ³w programowania (R i Python) dotyczyÅ‚y lasÃ³w losowych. W R pakiet nazywa siÄ™ randomForestSRC, a w Pythonie morfist. PozwalajÄ… one zarÃ³wno na wykonywanie wielowyjÅ›ciowych zadaÅ„ klasyfikacyjnych i regresyjnych, jak rÃ³wnieÅ¼ zadaÅ„ mieszanych. OczywiÅ›cie wspomniane wyÅ¼ej typy zadaÅ„ moÅ¼na realizowaÄ‡ przy uÅ¼yciu sieci neuronowych w obu jÄ™zykach programowania.\n\n\n\n\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, i Pedro LarraÃ±aga. 2015. â€A Survey on Multi-Output Regressionâ€. WIREs Data Mining and Knowledge Discovery 5 (5): 216â€“33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, i Charles J. Stone. 2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, i Massimiliano Pontil. 2004. â€Regularized multiâ€“task learningâ€. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, sierpieÅ„. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, RÃ©mi Gilleron, i Fabien Torre. 2012. â€Learning multiple tasks with boosted decision treesâ€. W Proceedings of the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I, 681â€“96. ECMLPKDDâ€™12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, i Remi Gilleron. 2010. â€Boosting Multi-Task Weak Learners with Applications to Textual and Social Dataâ€. W 2010 Ninth International Conference on Machine Learning and Applications, 367â€“72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nGlocker, Ben, Olivier Pauly, Ender Konukoglu, i Antonio Criminisi. 2012. â€Joint Classification-Regression Forests for Spatially Structured Multi-object Segmentationâ€. W Computer Vision â€“ ECCV 2012, zredagowane przez Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, i Cordelia Schmid, 7575:870â€“81. Springer Berlin Heidelberg. http://link.springer.com/10.1007/978-3-642-33765-9_62.\n\n\nIzenman, Alan Julian. 1975. â€Reduced-rank regression for the multivariate linear modelâ€. Journal of multivariate analysis 5 (2): 248â€“64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, i SaÅ¡o DÅ¾eroski. 2013. â€Tree ensembles for predicting structured outputsâ€. Pattern Recognition 46 (3): 817â€“33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, i SebastiÃ¡n Ventura. 2017. â€Multi-Target Support Vector Regression via Correlation Regressor Chainsâ€. Information Sciences 415â€“416 (listopad): 53â€“69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Morgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello Mastelini, Fabio Luiz Melquiades, i Sylvio Barbon Jr. 2020. â€Improved Prediction of Soil Properties with Multi-Target Stacked Generalisation on EDXRF Spectraâ€. arXiv preprint arXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. â€Tree-Structured Methods for Longitudinal Dataâ€. Journal of the American Statistical Association 87 (418): 407â€“18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves, i Ioannis Vlahavas. 2016. â€Multi-Target Regression via Input Space Expansion: Treating Targets as Inputsâ€. Machine Learning 104 (1): 55â€“98. https://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, i SaÅ¡o DÅ¾eroski. 2006. â€Constraint Based Induction of Multi-objective Regression Treesâ€. W Knowledge Discovery in Inductive Databases, zredagowane przez Francesco Bonchi i Jean-FranÃ§ois Boulicaut, 222â€“33. Lecture Notes w Computer Science. Springer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, i Victor Sheng. 2013. â€Empirical Comparison of Multi-Label Classification Algorithmsâ€. W Proceedings of the AAAI Conference on Artificial Intelligence, 27:1645â€“46.\n\n\nTsoumakas, Grigorios, i Ioannis Katakis. 2007. â€Multi-Label Classification: An Overviewâ€. International Journal of Data Warehousing and Mining (IJDWM) 3 (3): 1â€“13.\n\n\nVazquez, Emmanuel, i Eric Walter. 2003. â€Multi-Output Suppport Vector Regressionâ€. IFAC Proceedings Volumes, 13th IFAC Symposium on System Identification (SYSID 2003), Rotterdam, The Netherlands, 27-29 August, 2003, 36 (16): 1783â€“88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, i Laisheng Wang. 2013. â€A Twin Multi-Class Classification Support Vector Machineâ€. Cognitive Computation 5 (4): 580â€“88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, i Cheolkon Jung. b.d. â€GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputsâ€. https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "3Â  PrzykÅ‚ady - metody klasyczne",
    "section": "",
    "text": "3.1 PrzykÅ‚ad 1\nNajpierw sformuÅ‚ujemy problem badawczy wymagajÄ…cy zastosowania modeli z wieloma wyjÅ›ciami.\nKod\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_regression\nKod\n# generowanie danych do zadania\nX, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)\nKod\n# Å‚Ä…czenie ich w ramki danych\nX_df = pd.DataFrame(i for i in X)\nX_df.columns = [\"X\"+ str(i) for i in range(1,11)]\ny_df = pd.DataFrame(i for i in y)\ny_df.columns = [\"y\"+str(i) for i in range(1,4)]\n\ndf = pd.concat([X_df,y_df], axis=1)\ndf.head()\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nX9\nX10\ny1\ny2\ny3\n\n\n\n\n0\n-0.547331\n0.426522\n-1.693585\n-0.740282\n0.277445\n-1.910679\n-0.320635\n1.449172\n-0.469619\n0.371273\n-92.924393\n-116.819352\n-32.117311\n\n\n1\n2.314630\n-0.936016\n-1.833034\n-0.394507\n-0.902981\n-1.089381\n1.005442\n-0.351289\n1.218223\n0.350258\n88.577211\n182.003040\n67.579488\n\n\n2\n-0.025423\n0.684189\n1.208964\n1.325672\n0.328946\n-0.354083\n-0.566556\n0.671359\n-0.560768\n0.327379\n115.652472\n63.462207\n43.519697\n\n\n3\n-0.044533\n0.603030\n-1.495716\n-0.507870\n-0.268485\n-0.140194\n-0.246658\n-0.758946\n2.567979\n1.808345\n-12.092227\n65.181041\n96.138770\n\n\n4\n2.083679\n0.318852\n-0.080982\n-1.284608\n0.281687\n0.792470\n-0.560598\n-1.368963\n0.718059\n-1.741815\n-18.573726\n-103.219393\n11.559200\nKod\ndf_X_boxplot = pd.melt(X_df)\nsns.boxplot(data = df_X_boxplot, x = \"variable\", y = \"value\")\nplt.show()\nKod\ndf_y_boxplot = pd.melt(y_df)\nsns.boxplot(data = df_y_boxplot, x = \"variable\", y = \"value\")\nplt.show()\nZarÃ³wno zmienne X, jak i y majÄ… zbliÅ¼one rozkÅ‚ady.\nKod\nsns.set_theme(style=\"white\")\ng = sns.PairGrid(y_df, diag_sharey=False)\ng.map_upper(sns.scatterplot, s=7)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.histplot)\nplt.show()\nJak widaÄ‡ z powyÅ¼szego wykresu zmienne y1,y2,y3 wykazujÄ… pewne wzajemne zaleÅ¼noÅ›ci, dlatego budowa oddzielnych modeli dla kaÅ¼dej ze zmiennych powinna dawaÄ‡ gorsze predykcje niÅ¼ modele wiÄ…Å¼Ä…ce wszystkie zmiennej w jednym modelu. MoÅ¼na teÅ¼ zauwaÅ¼yÄ‡, Å¼e rozkÅ‚ady sÄ… zbliÅ¼one do normalnego.\nKod\ncor = df.corr()\nplt.figure(figsize = (12,10))\nsns.heatmap(cor, \n            xticklabels=cor.columns.values,\n            yticklabels=cor.columns.values,\n            annot = True, fmt = '.2f')\nplt.show()\nJak widaÄ‡ z powyÅ¼szej macierzy korelacji, przynajmniej 8 spoÅ›rÃ³d 10 zmiennych X koreluje istotnie ze zmiennymi y.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>PrzykÅ‚ady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#przykÅ‚ad-1",
    "href": "examples.html#przykÅ‚ad-1",
    "title": "3Â  PrzykÅ‚ady - metody klasyczne",
    "section": "",
    "text": "PrzykÅ‚ad 3.1 Dane do zadania wygenerujemy za pomocÄ… funkcji make_regression() z pakietu sklern.dataset1. Wygenerujemy 700 obserwacji z 10 predyktorami i 3 zmiennymi zaleÅ¼nymi.\n1Â Nie znam R-owego odpowiednika",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>PrzykÅ‚ady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#rozwiÄ…zanie",
    "href": "examples.html#rozwiÄ…zanie",
    "title": "3Â  PrzykÅ‚ady - metody klasyczne",
    "section": "3.2 RozwiÄ…zanie",
    "text": "3.2 RozwiÄ…zanie\nNa potrzeby porÃ³wnania rÃ³Å¼nych rozwiÄ…zaÅ„ zbudujemy nastÄ™pujÄ…ce konfiguracje modeli:\n\ntrzy lasy losowe dla kaÅ¼dej zmiennej y niezaleÅ¼nie;\nmodel lasu losowego z wykorzystaniem reguÅ‚y Regression Chains jako transformacji modelu;\nlas losowy dla trzech zmiennych wynikowych jednoczeÅ›nie (adaptacja modelu)\n\nRozwiÄ…zania te porÃ³wnamy pod wzglÄ™dem dopasowania.\n\n3.2.1 NiezaleÅ¼ne lasy losowe\nFunkcja MultiOutputRegressor naÅ‚oÅ¼ona na model lasu losowego takie rozwiÄ…zanie tworzy.\n\n\nKod\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)\n\n\n\n\nKod\nrf_meta = RandomForestRegressor(random_state=4)\nrf_indep = MultiOutputRegressor(rf_meta)\nprint(rf_indep)\n\n\nMultiOutputRegressor(estimator=RandomForestRegressor(random_state=4))\n\n\n\n\nKod\nrf_indep.fit(X_train, y_train)\nr2_indep= rf_indep.score(X_test, y_test)\npred_indep = rf_indep.predict(X_test)\nrmse_indep = mean_squared_error(y_test, pred_indep, squared = False)\n\nprint(f'R2 on test samples: {r2_indep:.2f}')\nprint(f'RMSE on test samples: {rmse_indep:.1f}')\n\n\nR2 on test samples: 0.82\nRMSE on test samples: 71.1\n\n\n\n\n3.2.2 Regressor Chains RF\n\n\nKod\nrf_chains = RegressorChain(rf_meta)\nprint(rf_chains)\n\n\nRegressorChain(base_estimator=RandomForestRegressor(random_state=4))\n\n\n\n\nKod\nrf_chains.fit(X_train, y_train)\nr2_chains= rf_chains.score(X_test, y_test)\npred_chains = rf_chains.predict(X_test)\nrmse_chains = mean_squared_error(y_test, pred_chains, squared = False)\n\nprint(f'R2 on test samples: {r2_chains:.2f}')\nprint(f'RMSE on test samples: {rmse_chains:.1f}')\n\n\nR2 on test samples: 0.80\nRMSE on test samples: 72.5\n\n\n\n\n3.2.3 Adaptacja modelu RF\n\n\nKod\nprint(rf_meta)\n\n\nRandomForestRegressor(random_state=4)\n\n\n\n\nKod\nrf_meta.fit(X_train, y_train)\nr2_meta= rf_meta.score(X_test, y_test)\npred_meta = rf_meta.predict(X_test)\nrmse_meta = mean_squared_error(y_test, pred_meta, squared = False)\n\nprint(f'R2 on test samples: {r2_meta:.2f}')\nprint(f'RMSE on test samples: {rmse_meta:.1f}')\n\n\nR2 on test samples: 0.77\nRMSE on test samples: 78.0\n\n\n\n\n3.2.4 Podsumowanie\nBiorÄ…c pod uwagÄ™ miary dopasowania najlepiej poradziÅ‚ sobie z tym zadaniem model skÅ‚adajÄ…cy siÄ™ z trzech niezaleÅ¼nych modeli RF, potem model RF w wersji Regressor Chains, a najgorzej (o dziwo) radzi sobie z predykcjÄ… model korzystajÄ…cy adaptacji drzew decyzyjnych do wersji wielowyjÅ›ciowej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>PrzykÅ‚ady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#przykÅ‚ad-2",
    "href": "examples.html#przykÅ‚ad-2",
    "title": "3Â  PrzykÅ‚ady - metody klasyczne",
    "section": "3.3 PrzykÅ‚ad 2",
    "text": "3.3 PrzykÅ‚ad 2\nTym razem sformuÅ‚ujemy problem z wieloma wyjÅ›ciami ale klasyfikacyjny i rÃ³wnieÅ¼ dopasujemy trzy wersje modelu lasu losowego:\n\nlasy losowe dla kaÅ¼dej zmiennej y niezaleÅ¼nie;\nmodel lasu losowego z wykorzystaniem reguÅ‚y Classifier Chains jako transformacji modelu\nlas losowy dla wszystkich zmiennych wynikowych jednoczeÅ›nie (adaptacja modelu).\n\nAnalizowany problem bÄ™dzie prawdziwy i bÄ™dzie dotyczyÅ‚ klasyfikacji enzymÃ³w na podstawie cech charakterystycznych substratÃ³w. Pierwszych 31 zmiennych stanowi zmienne objaÅ›niajÄ…ce, a 6 pozostaÅ‚ych zmienne kodujÄ…ce przynaleÅ¼noÅ›Ä‡ do danej grupy enzymÃ³w.\n\n\nKod\ndt = pd.read_csv(\"data/original.csv\", index_col = \"id\")\ndt.head()\n\n\n\n\n\n\n\n\n\nBertzCT\nChi1\nChi1n\nChi1v\nChi2n\nChi2v\nChi3v\nChi4n\nEState_VSA1\nEState_VSA2\n...\nSlogP_VSA3\nVSA_EState9\nfr_COO\nfr_COO2\nEC1\nEC2\nEC3\nEC4\nEC5\nEC6\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC00009\n49.783540\n2.000000\n0.782574\n2.347723\n0.513277\n1.539831\n0.000000\n0.000000\n7.822697\n0.000000\n...\n4.565048\n16.923611\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00013\n147.355172\n3.707107\n1.530297\n4.590890\n1.062804\n3.678309\n1.914534\n0.138556\n15.645394\n0.000000\n...\n13.440728\n20.899028\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00014\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.150546\n...\n0.000000\n0.000000\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00017\n172.720106\n4.947265\n2.081214\n2.081214\n1.157830\n1.157830\n0.489278\n0.180980\n12.514062\n12.451936\n...\n9.589074\n35.105740\n1\n1\n0\n1\n1\n0\n0\n0\n\n\nC00022\n72.039100\n2.642734\n1.381855\n1.381855\n0.861339\n0.861339\n0.301176\n0.000000\n11.752550\n0.000000\n...\n9.589074\n25.333333\n1\n1\n1\n1\n1\n1\n0\n1\n\n\n\n\n5 rows Ã— 37 columns\n\n\n\n\n\nKod\ndt.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1039 entries, C00009 to C22220\nData columns (total 37 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   BertzCT            1039 non-null   float64\n 1   Chi1               1039 non-null   float64\n 2   Chi1n              1039 non-null   float64\n 3   Chi1v              1039 non-null   float64\n 4   Chi2n              1039 non-null   float64\n 5   Chi2v              1039 non-null   float64\n 6   Chi3v              1039 non-null   float64\n 7   Chi4n              1039 non-null   float64\n 8   EState_VSA1        1039 non-null   float64\n 9   EState_VSA2        1039 non-null   float64\n 10  ExactMolWt         1039 non-null   float64\n 11  FpDensityMorgan1   1039 non-null   float64\n 12  FpDensityMorgan2   1039 non-null   float64\n 13  FpDensityMorgan3   1039 non-null   float64\n 14  HallKierAlpha      1039 non-null   float64\n 15  HeavyAtomMolWt     1039 non-null   float64\n 16  Kappa3             1039 non-null   float64\n 17  MaxAbsEStateIndex  1039 non-null   float64\n 18  MinEStateIndex     1039 non-null   float64\n 19  NumHeteroatoms     1039 non-null   int64  \n 20  PEOE_VSA10         1039 non-null   float64\n 21  PEOE_VSA14         1039 non-null   float64\n 22  PEOE_VSA6          1039 non-null   float64\n 23  PEOE_VSA7          1039 non-null   float64\n 24  PEOE_VSA8          1039 non-null   float64\n 25  SMR_VSA10          1039 non-null   float64\n 26  SMR_VSA5           1039 non-null   float64\n 27  SlogP_VSA3         1039 non-null   float64\n 28  VSA_EState9        1039 non-null   float64\n 29  fr_COO             1039 non-null   int64  \n 30  fr_COO2            1039 non-null   int64  \n 31  EC1                1039 non-null   int64  \n 32  EC2                1039 non-null   int64  \n 33  EC3                1039 non-null   int64  \n 34  EC4                1039 non-null   int64  \n 35  EC5                1039 non-null   int64  \n 36  EC6                1039 non-null   int64  \ndtypes: float64(28), int64(9)\nmemory usage: 308.5+ KB\n\n\nSprawdzimy na ile niezbalansowane sÄ… poszczegÃ³lne klasy wynikowe.\n\n\nKod\ndt_deps = dt.loc[:,'EC1':'EC6']\ndt_deps = dt_deps.melt()\ndt_deps = dt_deps[dt_deps.value == 1]\nsns.countplot(data = dt_deps, x = 'variable')\n\n\n\n\n\n\n\n\n\nNiestety nie wszystkie klasy wystÄ™pujÄ… jednakowo czÄ™sto i moÅ¼e pojawiÄ‡ siÄ™ zjawisko, Å¼e kombinacja enzymÃ³w bÄ™dzie wystÄ™powaÅ‚a bardzo rzadko (np. raz). Jak widaÄ‡ z poniÅ¼szej tabeli faktycznie tak siÄ™ dzieje. To nie pozwala przeprowadziÄ‡ uczenia. Dlatego musimy poÅ‚Ä…czyÄ‡ pewne klasy enzymÃ³w aby uniemoÅ¼liwiÄ‡ takÄ… sytuacjÄ™.\n\n\nKod\ndt.loc[:,'EC1':'EC6'].value_counts()\n\n\nEC1  EC2  EC3  EC4  EC5  EC6\n1    0    0    0    0    0      178\n0    1    0    0    0    0      136\n1    1    0    0    0    0      112\n0    1    1    0    0    0       90\n     0    1    0    0    0       69\n1    1    0    1    0    0       38\n          1    0    0    0       37\n     0    0    1    0    0       33\n0    0    0    1    0    0       30\n     1    0    1    0    0       27\n               0    1    0       21\n1    0    1    0    0    0       20\n     1    0    0    0    1       19\n               1    0    1       15\n0    0    0    0    1    0       15\n1    1    0    0    1    0       14\n     0    0    0    1    0       12\n0    0    1    1    0    0       12\n          0    0    0    1       11\n1    1    0    1    1    1       11\n0    1    0    0    0    1       10\n          1    1    0    0        9\n1    0    0    1    1    0        8\n     1    1    1    0    1        8\n     0    0    1    0    1        7\n     1    1    0    0    1        7\n     0    0    0    0    1        7\n0    1    1    0    1    0        7\n                    0    1        7\n1    1    1    1    1    0        6\n          0    1    1    0        6\n0    1    0    1    0    1        6\n1    1    1    0    1    0        5\n0    0    0    1    1    0        5\n     1    1    1    1    0        4\n1    1    0    0    1    1        4\n          1    1    0    0        4\n0    0    1    0    0    1        4\n                    1    0        3\n1    1    1    1    1    1        3\n     0    1    1    0    0        3\n               0    0    1        3\n0    1    0    1    1    0        2\n                         1        2\n     0    0    1    0    1        2\n1    0    0    0    1    1        1\n0    0    1    1    0    1        1\n               0    1    1        1\n1    0    1    1    0    1        1\n               0    1    0        1\n0    1    1    1    0    1        1\n     0    1    1    1    0        1\nName: count, dtype: int64\n\n\nUsuniemy zatem takie kombinacje, ktÃ³re wystÄ™pujÄ… bardzo rzadko w danych.\n\n\nKod\ncombinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()\nidx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()\nbad_combinations = combinations[idx&lt;10]\nidx = []\nfor i in range(len(dt)):\n  idx.append(True)\n  for j in range(len(bad_combinations)):\n    if all(dt.iloc[i, 31:] == bad_combinations[j]):\n      idx[i]=False\n\ndt = dt.iloc[idx,:]\n\n\n\n\nKod\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier, ClassifierChain\ny = dt.iloc[:,31:].to_numpy()\nX = dt.iloc[:,:31].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n\n\n\n3.3.1 NiezaleÅ¼ne lasy losowe\n\n\nKod\nrf_meta = RandomForestClassifier(random_state=4)\nrf_indep = MultiOutputClassifier(rf_meta)\nprint(rf_indep)\n\nrf_indep.fit(X_train, y_train)\nacc_indep= rf_indep.score(X_test, y_test)\npred_indep = rf_indep.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_indep:.3f}')\n\n\nMultiOutputClassifier(estimator=RandomForestClassifier(random_state=4))\nAccuracy on test samples: 0.179\n\n\n\n\n3.3.2 Classifier Chains\n\n\nKod\nrf_chains = ClassifierChain(rf_meta)\nprint(rf_chains)\n\nrf_chains.fit(X_train, y_train)\nacc_chains= rf_chains.score(X_test, y_test)\npred_chains = rf_chains.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_chains:.3f}')\n\n\nClassifierChain(base_estimator=RandomForestClassifier(random_state=4))\nAccuracy on test samples: 0.190\n\n\n\n\n3.3.3 Adaptacja modelu RF\n\n\nKod\nrf_meta.fit(X_train, y_train)\nacc_meta= rf_meta.score(X_test, y_test)\npred_meta = rf_meta.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_meta:.3f}')\n\n\nAccuracy on test samples: 0.201\n\n\n\n\n3.3.4 Podsumowanie\nRÃ³wnieÅ¼ i tym razem nie widaÄ‡ wyraÅºnych rÃ³Å¼nic pomiÄ™dzy jakoÅ›ciÄ… dopasowania modeli. Model adaptowanego lasu losowego poradziÅ‚ sobie z zadanie najlepiej (19% poprawnoÅ›ci trafieÅ„), Classifier Chains daÅ‚ 18,7%, a niezaleÅ¼ne lasy losowe 17,2%.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>PrzykÅ‚ady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples2.html",
    "href": "examples2.html",
    "title": "4Â  PrzykÅ‚ady NN",
    "section": "",
    "text": "4.1 PrzykÅ‚ad 1\nRozwiÄ…zanie przykÅ‚adu z poprzedniego rozdziaÅ‚u moÅ¼na dokonaÄ‡ z duÅ¼o lepszÄ… precyzjÄ… wykorzystujÄ…c sieci neuronowe. W tym przykÅ‚adzie po raz kolejny wygenerujemy dane do uczenia1, a nastÄ™pnie wytrenujemy sieÄ‡ (dosyÄ‡ pÅ‚ytkÄ…) MLP.\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, ReLU, LeakyReLU, ELU\nfrom keras.callbacks import EarlyStopping\nimport keras\nKod\n# generowanie danych do zadania\nX, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)\n\nn_neurons = [10,20,50]\nModel bÄ™dzie bardzo prosty, skÅ‚adajÄ…cy siÄ™ z tylko jednej warstwy ukrytej.\nKod\ndef get_model(n_inputs, n_outputs, n_neurons):\n  model = Sequential()\n  model.add(Dense(int(n_neurons), input_dim=n_inputs, activation='relu'))\n  model.add(Dense(n_outputs, activation='linear'))\n  model.compile(loss='mse', optimizer='adam')\n  return model\nW tym przykÅ‚adzie chciaÅ‚em rÃ³wnieÅ¼ pokazaÄ‡ jak wykonywaÄ‡ trenowanie sieci z uÅ¼yciem sprawdzianu krzyÅ¼owego, ktÃ³ry pomoÅ¼e nam ustaliÄ‡ optymalnÄ… liczÄ™ neuronÃ³w w warstwie ukrytej.\nKod\n# ocena dopasowania modelu z wykorzystaniem CV\ndef evaluate_model(X, y, n_neurons):\n    results = list()\n    n_inputs, n_outputs = X.shape[1], y.shape[1]\n    # definicja CV\n    cv = RepeatedKFold(n_splits=5, random_state=1)\n    # pÄ™tla po foldach\n    for train_ix, test_ix in cv.split(X):\n        # przygotowanie danych\n        X_tr, X_te = X[train_ix], X[test_ix]\n        y_tr, y_te = y[train_ix], y[test_ix]\n        # okreÅ›lenie modelu\n        model = get_model(n_inputs, n_outputs, n_neurons)\n        # dopasowanie modelu\n        model.fit(X_tr, y_tr, verbose=0, epochs=100)\n        # ocena dopasowania na foldzie testowym\n        mae = model.evaluate(X_te, y_te, verbose=0)\n        results.append(mae)\n    return results\nKod\nresults = []\nfor i in n_neurons:\n  # dopasuj i oceÅ„ model na zbiorze uczÄ…cym\n  results.append(np.mean(evaluate_model(X_train, y_train, i)))\nKod\nresults = np.load(\"./data/mlp_eval.npz\")\nresults = results['arr_0'].tolist()\nfor i in range(len(n_neurons)):\n  print(f\"Dla {n_neurons[i]} neuronÃ³w MAE: {results[i]:.0f}\")\n\n\nDla 10 neuronÃ³w MAE: 16409\nDla 20 neuronÃ³w MAE: 9806\nDla 50 neuronÃ³w MAE: 3779\nNajlepszy rezultat osiÄ…gamy dla 50 neuronÃ³w i taki parametr dobierzemy w ostatecznym modelu.\nKod\nkeras.utils.set_random_seed(44)\nmy_callbacks = [\n    EarlyStopping(patience=2)\n]\n\nmodel = get_model(X_train.shape[1], y_train.shape[1], 50)\nhistory = model.fit(X_train, y_train, \n                    verbose=0, epochs=1000, \n                    validation_split=0.2, callbacks=my_callbacks)\nProces uczenia przebiegaÅ‚ prawidÅ‚owo i osiÄ…gniÄ™to niski poziom funkcji straty.\nKod\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()\nWyniki dopasowania znacznie przekraczajÄ… wyniki uzyskane metodami z poprzedniego rozdziaÅ‚u.\nKod\ny_pred = model.predict(X_test, verbose=0)\nrmse_mlp = mean_squared_error(y_test, y_pred, squared=False)\nr2_mlp = r2_score(y_test, y_pred)\nprint(f\"R2 on test sample: {r2_mlp:.2f}\")\nprint(f\"RMSE on test sample: {rmse_mlp:.1f}\")\n\n\nR2 on test sample: 1.00\nRMSE on test sample: 0.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>PrzykÅ‚ady NN</span>"
    ]
  },
  {
    "objectID": "examples2.html#przykÅ‚ad-1",
    "href": "examples2.html#przykÅ‚ad-1",
    "title": "4Â  PrzykÅ‚ady NN",
    "section": "",
    "text": "1Â te same co z przykÅ‚adu regresyjnego z poprzedniego wykÅ‚adu",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>PrzykÅ‚ady NN</span>"
    ]
  },
  {
    "objectID": "examples2.html#przykÅ‚ad-2",
    "href": "examples2.html#przykÅ‚ad-2",
    "title": "4Â  PrzykÅ‚ady NN",
    "section": "4.2 PrzykÅ‚ad 2",
    "text": "4.2 PrzykÅ‚ad 2\nW tym przykÅ‚adzie jeszcze raz rozpatrzymy zadanie klasyfikacyjne z wieloma wyjÅ›ciami z poprzedniego rozdziaÅ‚u. Przeprowadzimy czynnoÅ›ci preprocessingu podobne jak w poprzednim rozdziale, dodajÄ…c jeszcze standaryzacjÄ™, ktÃ³ra dla sieci neuronowych jest bardzo waÅ¼na.\n\n\nKod\nfrom sklearn.preprocessing import StandardScaler\ndt = pd.read_csv(\"./data/original.csv\", index_col = \"id\")\ncombinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()\nidx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()\nbad_combinations = combinations[idx&lt;10]\nidx = []\nfor i in range(len(dt)):\n  idx.append(True)\n  for j in range(len(bad_combinations)):\n    if all(dt.iloc[i, 31:] == bad_combinations[j]):\n      idx[i]=False\n\ndt = dt.iloc[idx,:]\n\ny = dt.iloc[:,31:].to_numpy()\nX = dt.iloc[:,:31].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n\nscaler = StandardScaler().fit(X_train)\nX_test = scaler.transform(X_test)\nX_train = scaler.transform(X_train)\n\n\nNastÄ™pnie przygotujemy model sieci neuronowej, ktÃ³ra pozwoli na wÅ‚aÅ›ciwe klasyfikacje obiektÃ³w.\n\n\nKod\nmodel = Sequential()\nmodel.add(Dense(15, input_dim=X_train.shape[1]))\nmodel.add(ReLU())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(y_train.shape[1], activation='sigmoid'))\n\nopt = keras.optimizers.Nadam(0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=opt)\nmodel.summary()\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 15)                480       \n                                                                 \n re_lu (ReLU)                (None, 15)                0         \n                                                                 \n dropout (Dropout)           (None, 15)                0         \n                                                                 \n dense_3 (Dense)             (None, 6)                 96        \n                                                                 \n=================================================================\nTotal params: 576 (2.25 KB)\nTrainable params: 576 (2.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nKod\nkeras.utils.set_random_seed(44)\nmy_callbacks = [\n    EarlyStopping(patience=10)\n]\nhistory = model.fit(X_train, y_train, epochs=1000,  validation_split=0.4, verbose=0,\ncallbacks=my_callbacks)\n\n\n\n\nKod\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nKod\nfrom sklearn.metrics import accuracy_score\ny_pred = model.predict(X_test, verbose=0)\ny_class = y_pred.round()\nacc = accuracy_score(y_test, y_class)\nprint(f\"ACC on test sample: {acc:.3f}\")\n\n\nACC on test sample: 0.198\n\n\n\n4.2.1 Podsumowanie\nModel prostej sieci neuronowej nie poprawiÅ‚ znaczÄ…co jakoÅ›ci dopasowania w stosunku do modelu lasu losowego adaptowanego do zadania z wieloma wyjÅ›ciami.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>PrzykÅ‚ady NN</span>"
    ]
  },
  {
    "objectID": "rnn.html",
    "href": "rnn.html",
    "title": "\n5Â  DNN dla danych sekwencyjnych\n",
    "section": "",
    "text": "5.1 Rodzaje szeregÃ³w czasowych\nSzereg czasowy moÅ¼e byÄ‡ dowolnym zestawem danych uzyskanym poprzez pomiary w regularnych odstÄ™pach czasu, jak np. dzienna cena akcji, godzinowe zuÅ¼ycie energii elektrycznej w mieÅ›cie lub tygodniowa sprzedaÅ¼ w sklepie. Szeregi czasowe sÄ… obecne wszÄ™dzie, niezaleÅ¼nie od tego, czy patrzymy na zjawiska naturalne (takie jak aktywnoÅ›Ä‡ sejsmiczna, ewolucja populacji ryb w rzece czy pogoda w danym miejscu), czy na wzorce aktywnoÅ›ci ludzkiej (takie jak odwiedzajÄ…cy stronÄ™ internetowÄ…, PKB kraju czy transakcje kartami kredytowymi). W przeciwieÅ„stwie do typÃ³w danych, z ktÃ³rymi mieliÅ›my do tej pory do czynienia, praca z szeregami czasowymi wymaga zrozumienia dynamiki systemu - jego cykli okresowych, trendÃ³w w czasie, regularnego reÅ¼imu i nagÅ‚ych skokÃ³w.\nZdecydowanie najbardziej powszechnym zadaniem zwiÄ…zanym z szeregami czasowymi jest prognozowanie: przewidywanie, co stanie siÄ™ w nastÄ™pnej serii; prognozowanie zuÅ¼ycia energii elektrycznej z kilkugodzinnym wyprzedzeniem, aby moÅ¼na byÅ‚o przewidzieÄ‡ popyt; prognozowanie przychodÃ³w z kilkumiesiÄ™cznym wyprzedzeniem, aby moÅ¼na byÅ‚o zaplanowaÄ‡ budÅ¼et; prognozowanie pogody z kilkudniowym wyprzedzeniem, aby moÅ¼na byÅ‚o zaplanowaÄ‡ wyjazd. Prognozowanie jest tym, na czym skupia siÄ™ ten rozdziaÅ‚. Ale w rzeczywistoÅ›ci istnieje wiele innych rzeczy, ktÃ³re moÅ¼na zrobiÄ‡ z szeregami czasowymi:\nPodczas pracy z szeregami czasowymi, spotkamy siÄ™ z szerokÄ… gamÄ… technik reprezentacji danych specyficznych dla danej dziedziny. Na przykÅ‚ad, transformata Fouriera, ktÃ³ra polega na wyraÅ¼eniu serii wartoÅ›ci w kategoriach superpozycji fal o rÃ³Å¼nych czÄ™stotliwoÅ›ciach. Transformata Fouriera moÅ¼e byÄ‡ bardzo cenna podczas wstÄ™pnego przetwarzania wszelkich danych, ktÃ³re charakteryzujÄ… siÄ™ przede wszystkim cyklami i oscylacjami (jak dÅºwiÄ™k, drgania ramy wieÅ¼owca czy fale mÃ³zgowe). W kontekÅ›cie gÅ‚Ä™bokiego uczenia, analiza Fouriera (lub powiÄ…zana analiza czÄ™stotliwoÅ›ci Mel) i inne reprezentacje specyficzne dla danej domeny mogÄ… byÄ‡ przydatne jako forma inÅ¼ynierii cech, sposÃ³b na przygotowanie danych przed trenowaniem modelu na nich, aby uÅ‚atwiÄ‡ pracÄ™ modelu.\nMoÅ¼emy wyrÃ³Å¼niÄ‡ trzy rodzaje zadaÅ„ z wykorzystaniem szeregÃ³w czasowych:\nOprÃ³cz wspomnianego podziaÅ‚u rodzajÃ³w zadaÅ„ z wykorzystaniem szeregÃ³w czasowych, moÅ¼emy je rÃ³wnieÅ¼ podzieliÄ‡ na typy zadaÅ„ ze wzglÄ™du na postaÄ‡ wejÅ›Ä‡ i wyjÅ›Ä‡ z modelu sieci neuronowej:",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#rodzaje-szeregÃ³w-czasowych",
    "href": "rnn.html#rodzaje-szeregÃ³w-czasowych",
    "title": "\n5Â  DNN dla danych sekwencyjnych\n",
    "section": "",
    "text": "klasyfikacja - przypisanie jednej lub wiÄ™cej etykiet kategorycznych do szeregu czasowego. Na przykÅ‚ad, biorÄ…c pod uwagÄ™ szereg czasowy aktywnoÅ›ci osoby odwiedzajÄ…cej stronÄ™ internetowÄ…, naleÅ¼y sklasyfikowaÄ‡, czy osoba ta jest botem czy czÅ‚owiekiem.\nwykrywanie zdarzeÅ„ - identyfikacja wystÄ…pienia okreÅ›lonego, oczekiwanego zdarzenia w ciÄ…gÅ‚ym strumieniu danych. SzczegÃ³lnie uÅ¼ytecznym zastosowaniem jest â€œwykrywanie sÅ‚Ã³w kluczyâ€, gdzie model monitoruje strumieÅ„ audio i wykrywa wypowiedzi takie jak â€œOK, Googleâ€ lub â€œHej, Alexaâ€.\nwykrywanie anomalii - wykrywanie wszelkich nietypowych zdarzeÅ„ w ciÄ…gÅ‚ym strumieniu danych. Nietypowa aktywnoÅ›Ä‡ w sieci firmowej, ktÃ³re moÅ¼e byÄ‡ rozpoznane jako atak. Nietypowe odczyty na linii produkcyjnej, po to, aby czÅ‚owiek siÄ™ temu przyjrzaÅ‚. Wykrywanie anomalii odbywa siÄ™ zazwyczaj poprzez uczenie bez nadzoru, poniewaÅ¼ czÄ™sto nie wiadomo, jakiego rodzaju anomalii siÄ™ szuka, wiÄ™c nie moÅ¼na trenowaÄ‡ na konkretnych przykÅ‚adach anomalii.\n\n\n\n\njednowymiarowe szeregi czasowe (ang. univariate time series) - problem sekwencyjny, w ktÃ³rym na podstawie historycznych wartoÅ›ci szeregu czasowego przewidujemy przyszÅ‚e wartoÅ›ci. PrzykÅ‚adowo na podstawie temperatur z kilku poprzednich dni, chcemy przewidywaÄ‡ temperaturÄ™ jutrzejszÄ….\nwielowymiarowe szeregi czasowe (ang. multivariate time series) - problem sekwencyjny, w ktÃ³rym na podstawie historycznych wartoÅ›ci kilku zmiennych (w tym historycznych danych na temat zmiennej wyjÅ›ciowej) przewidujemy przyszÅ‚e wartoÅ›ci pewnej zmiennej wyjÅ›ciowej. PrzykÅ‚adowo jeÅ›li chcemy przewidywaÄ‡ zanieczyszczenie powietrza1 na podstawie wczeÅ›niejszych odczytÃ³w zanieczyszczenia oraz wczeÅ›niejszych informacji o sile i kierunku wiatru, temperaturze, punkcie rosy itp.\nwieloetapowe szeregi czasowe (ang. multistage time series) - problem sekwencyjny, w ktÃ³rym na postawie historycznych wartoÅ›ci szeregu czasowego przewidujemy kilka (wiÄ™cej niÅ¼ jednÄ…) wartoÅ›Ä‡ przyszÅ‚Ä… tego szeregu czasowego. PrzykÅ‚adowo jeÅ›li na podstawie historycznych wielkoÅ›ci opadÃ³w chcemy przewidzieÄ‡ wielkoÅ›Ä‡ opadÃ³w w najbliÅ¼szych trzech dniach.\n\n1Â wyraÅ¼one w jednostkach pm2.5\n\njeden do jednego - gdy istnieje jedno wejÅ›cie i jedno wyjÅ›cie. Typowym przykÅ‚adem problemu sekwencji jeden do jednego jest przypadek, gdy mamy obraz i chcemy przewidzieÄ‡ jednÄ… etykietÄ™ dla tego obrazu.\nwiele do jednego - problemach sekwencyjnych wiele do jednego, mamy sekwencjÄ™ danych jako wejÅ›cie i musimy przewidzieÄ‡ pojedyncze wyjÅ›cie. Klasyfikacja tekstu jest doskonaÅ‚ym przykÅ‚adem problemÃ³w sekwencji many-to-one, gdzie mamy sekwencjÄ™ wejÅ›ciowÄ… sÅ‚Ã³w i chcemy przewidzieÄ‡ pojedynczy tag wyjÅ›ciowy.\njeden do wielu - w problemach sekwencji jeden do wielu, mamy pojedyncze wejÅ›cie i sekwencjÄ™ wyjÅ›Ä‡. Typowym przykÅ‚adem jest obraz i odpowiadajÄ…cy mu opis sceny.\nwiele do wielu - problemy sekwencji wiele do wielu obejmujÄ… sekwencjÄ™ wejÅ›Ä‡ i sekwencjÄ™ wyjÅ›Ä‡. Na przykÅ‚ad, ceny akcji z 7 dni jako dane wejÅ›ciowe i ceny akcji z kolejnych 7 dni jako dane wyjÅ›ciowe. Chatboty sÄ… rÃ³wnieÅ¼ przykÅ‚adem problemÃ³w sekwencji wiele do wielu, gdzie sekwencja tekstowa jest wejÅ›ciem, a inna sekwencja tekstowa jest wyjÅ›ciem.\n\n\n\nRÃ³Å¼ne typy zadaÅ„ sekwencyjnych realizowanych przez sieci neuronowe",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#zastosowanie-sieci-rekurencyjnych-w-szeregach-czasowych",
    "href": "rnn.html#zastosowanie-sieci-rekurencyjnych-w-szeregach-czasowych",
    "title": "\n5Â  DNN dla danych sekwencyjnych\n",
    "section": "\n5.2 Zastosowanie sieci rekurencyjnych w szeregach czasowych",
    "text": "5.2 Zastosowanie sieci rekurencyjnych w szeregach czasowych\nW tym podrozdziale omÃ³wimy trzy zaawansowane techniki poprawy wydajnoÅ›ci i siÅ‚y generalizacji rekurencyjnych sieci neuronowych. Zademonstrujemy wszystkie trzy koncepcje na problemie prognozowania pogody, gdzie mamy dostÄ™p do szeregu obserwacji pochodzÄ…cych z czujnikÃ³w zainstalowanych na dachu budynku, takich jak temperatura, ciÅ›nienie powietrza i wilgotnoÅ›Ä‡, ktÃ³re uÅ¼yjemy do przewidywania, jaka bÄ™dzie temperatura 24 godziny po ostatniej obserwacji w bazie danych. Jest to doÅ›Ä‡ trudny problem, ktÃ³ry ilustruje wiele typowych trudnoÅ›ci napotykanych podczas pracy z szeregami czasowymi.\nOmÃ³wimy nastÄ™pujÄ…ce techniki:\n\n\nRecurrent dropout - to specyficzny, wbudowany sposÃ³b uÅ¼ycia dropoutu do walki z nadmiernym dopasowaniem w warstwach rekurencyjnych.\nSkÅ‚adanie warstw rekurencyjnych - zwiÄ™ksza to moc reprezentacyjnÄ… sieci (kosztem wiÄ™kszego obciÄ…Å¼enia obliczeniowego).\nDwukierunkowe warstwy rekurencyjne - prezentujÄ… one tÄ™ samÄ… informacjÄ™ sieci rekurencyjnej na rÃ³Å¼ne sposoby, zwiÄ™kszajÄ…c dokÅ‚adnoÅ›Ä‡ i Å‚agodzÄ…c problemy zwiÄ…zane z zapominaniem.\n\n\nPrzykÅ‚ad 5.1 W analizowanym zestawie danych 14 rÃ³Å¼nych wielkoÅ›ci (takich jak temperatura powietrza, ciÅ›nienie atmosferyczne, wilgotnoÅ›Ä‡, kierunek wiatru i tak dalej) byÅ‚o rejestrowanych co 10 minut, przez kilka lat. Oryginalne dane siÄ™gajÄ… 2003 roku, ale ten przykÅ‚ad jest ograniczony do danych z lat 2009-2016.\nNa poczÄ…tek pobierzemy dane z serwera https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip i rozpakujemy.\n\nKodurl &lt;-\n \"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\"\ndownload.file(url, destfile = basename(url))\nzip::unzip(zipfile = \"jena_climate_2009_2016.csv.zip\",\n          files = \"jena_climate_2009_2016.csv\")\n\n\n\nKodfull_df &lt;- readr::read_csv(\"jena_climate_2009_2016.csv\")\nfull_df\n\n# A tibble: 420,451 Ã— 15\n   `Date Time`         `p (mbar)` `T (degC)` `Tpot (K)` `Tdew (degC)` `rh (%)`\n   &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 01.01.2009 00:10:00       997.      -8.02       265.         -8.9      93.3\n 2 01.01.2009 00:20:00       997.      -8.41       265.         -9.28     93.4\n 3 01.01.2009 00:30:00       997.      -8.51       265.         -9.31     93.9\n 4 01.01.2009 00:40:00       997.      -8.31       265.         -9.07     94.2\n 5 01.01.2009 00:50:00       997.      -8.27       265.         -9.04     94.1\n 6 01.01.2009 01:00:00       996.      -8.05       265.         -8.78     94.4\n 7 01.01.2009 01:10:00       996.      -7.62       266.         -8.3      94.8\n 8 01.01.2009 01:20:00       996.      -7.62       266.         -8.36     94.4\n 9 01.01.2009 01:30:00       996.      -7.91       266.         -8.73     93.8\n10 01.01.2009 01:40:00       997.      -8.43       265.         -9.34     93.1\n# â„¹ 420,441 more rows\n# â„¹ 9 more variables: `VPmax (mbar)` &lt;dbl&gt;, `VPact (mbar)` &lt;dbl&gt;,\n#   `VPdef (mbar)` &lt;dbl&gt;, `sh (g/kg)` &lt;dbl&gt;, `H2OC (mmol/mol)` &lt;dbl&gt;,\n#   `rho (g/m**3)` &lt;dbl&gt;, `wv (m/s)` &lt;dbl&gt;, `max. wv (m/s)` &lt;dbl&gt;,\n#   `wd (deg)` &lt;dbl&gt;\n\n\nChoÄ‡ nie jest to konieczne w tym zadaniu, to przeksztaÅ‚cimy datÄ™ (bÄ™dÄ…cÄ… pierwsza kolumnÄ… w zestawie danych) z typu character do typu DateTime.\n\nKodfull_df$`Date Time` %&lt;&gt;%\n as.POSIXct(tz = \"Etc/GMT+1\", format = \"%d.%m.%Y %H:%M:%S\")\n\n\nZwrÃ³Ä‡ uwagÄ™, Å¼e przekazujemy tz = \"Etc/GMT+1\" zamiast tz = \"Europe/Berlin\", poniewaÅ¼ znaczniki czasu w zbiorze danych nie dostosowujÄ… siÄ™ do czasu letniego Å›rodkowoeuropejskiego (znanego rÃ³wnieÅ¼ jako czas letni), ale zawsze sÄ… wedÅ‚ug czasu Å›rodkowoeuropejskiego.\n\n\n\n\n\n\nWskazÃ³wka\n\n\n\nPowyÅ¼ej po raz pierwszy uÅ¼yliÅ›my potoku przypisania x %&lt;&gt;% fn(), ktÃ³ry jest skrÃ³tem od x &lt;- x %&gt;% fn(). Jest to przydatne, poniewaÅ¼ pozwala pisaÄ‡ bardziej czytelny kod i uniknÄ…Ä‡ wielokrotnego powtarzania tej samej nazwy zmiennej. Przypisanie jest udostÄ™pniane przez wywoÅ‚anie library(keras).\n\n\n\nKodplot(`T (degC)` ~ `Date Time`, data = full_df, pch = 20, cex = .3)\n\n\n\n\n\n\nRys.Â 5.1: Przebieg szeregu czasowego temperatury\n\n\n\n\nRys.Â 5.1 przedstawia wykres temperatury (w stopniach Celsjusza) w czasie. Na tym wykresie wyraÅºnie widaÄ‡ rocznÄ… okresowoÅ›Ä‡ temperatury - dane obejmujÄ… 8 lat.\n\nKodplot(`T (degC)` ~ `Date Time`, data = full_df[1:1440, ], type = 'l')\n\n\n\n\n\n\nRys.Â 5.2: Wycinek przebiegu szeregu czasowego temperatury\n\n\n\n\nRys.Â 5.2 przedstawia wÄ™Å¼szy wykres danych temperatury z pierwszych 10 dni. PoniewaÅ¼ dane sÄ… rejestrowane co 10 minut, otrzymujemy 24 Ã— 6 = 144 punkty danych dziennie. ZauwaÅ¼ teÅ¼, Å¼e ten 10-dniowy okres musi pochodziÄ‡ z doÅ›Ä‡ zimnego zimowego miesiÄ…ca. GdybyÅ› prÃ³bowaÅ‚ przewidzieÄ‡ Å›redniÄ… temperaturÄ™ na nastÄ™pny miesiÄ…c biorÄ…c pod uwagÄ™ kilka miesiÄ™cy danych z przeszÅ‚oÅ›ci, problem byÅ‚by Å‚atwy, ze wzglÄ™du na okresowoÅ›Ä‡ danych w skali roku. Ale patrzÄ…c na dane w skali dni, temperatura wyglÄ…da o wiele bardziej chaotycznie. Czy ten szereg czasowy jest przewidywalny w skali dziennej? Przekonajmy siÄ™.\nWe wszystkich naszych eksperymentach wykorzystamy pierwsze 50% danych do szkolenia, kolejne 25% do walidacji, a ostatnie 25% do testowania. Podczas pracy z danymi szeregÃ³w czasowych waÅ¼ne jest, aby uÅ¼ywaÄ‡ danych walidacyjnych i testowych, ktÃ³re sÄ… nowsze niÅ¼ dane treningowe, poniewaÅ¼ prÃ³bujemy przewidzieÄ‡ przyszÅ‚oÅ›Ä‡ na podstawie przeszÅ‚oÅ›ci, a nie odwrotnie, a podziaÅ‚y walidacyjne / testowe powinny to odzwierciedlaÄ‡.\n\nKodnum_train_samples &lt;- round(nrow(full_df) * .5)\nnum_val_samples &lt;- round(nrow(full_df) * 0.25)\nnum_test_samples &lt;- nrow(full_df) - num_train_samples - num_val_samples\n\ntrain_df &lt;- full_df[seq(num_train_samples), ]\n\nval_df &lt;- full_df[seq(from = nrow(train_df) + 1,\n                      length.out = num_val_samples), ]\n\ntest_df &lt;- full_df[seq(to = nrow(full_df),\n                       length.out = num_test_samples), ]\n\ncat(\"num_train_samples:\", nrow(train_df), \"\\n\")\n\nnum_train_samples: 210226 \n\nKodcat(\"num_val_samples:\", nrow(val_df), \"\\n\")\n\nnum_val_samples: 105113 \n\nKodcat(\"num_test_samples:\", nrow(test_df), \"\\n\")\n\nnum_test_samples: 105112 \n\n\nDokÅ‚adne sformuÅ‚owanie problemu bÄ™dzie nastÄ™pujÄ…ce: biorÄ…c pod uwagÄ™ dane obejmujÄ…ce poprzednie piÄ™Ä‡ dni i prÃ³bkowane raz na godzinÄ™, czy moÅ¼emy przewidzieÄ‡ temperaturÄ™ w ciÄ…gu 24 godzin?\nPo pierwsze, wstÄ™pnie przetworzymy dane do formatu, ktÃ³ry moÅ¼e przyjÄ…Ä‡ sieÄ‡ neuronowa. Dane sÄ… juÅ¼ numeryczne, wiÄ™c nie trzeba ich wektoryzowaÄ‡. Jednak kaÅ¼dy szereg czasowy w danych ma innÄ… skalÄ™ (np. ciÅ›nienie atmosferyczne, mierzone w mbar, wynosi okoÅ‚o 1000, podczas gdy H2OC, mierzone w milimolach na mol, wynosi okoÅ‚o 3). Znormalizujemy kaÅ¼dÄ… seriÄ™ czasowÄ… (kolumnÄ™) niezaleÅ¼nie, tak aby wszystkie przyjmowaÅ‚y maÅ‚e wartoÅ›ci w podobnej skali. UÅ¼yjemy pierwszych 210 226 krokÃ³w czasowych jako danych treningowych, wiÄ™c obliczymy Å›redniÄ… i odchylenie standardowe tylko dla tej czÄ™Å›ci danych.\n\nKodinput_data_colnames &lt;- names(full_df) %&gt;%\n  setdiff(c(\"Date Time\"))\n\nnormalization_values &lt;-\n  zip_lists(mean = lapply(train_df[input_data_colnames], mean),\n            sd = lapply(train_df[input_data_colnames], sd))\n\nstr(normalization_values)\n\nList of 14\n $ p (mbar)       :List of 2\n  ..$ mean: num 989\n  ..$ sd  : num 8.51\n $ T (degC)       :List of 2\n  ..$ mean: num 8.83\n  ..$ sd  : num 8.77\n $ Tpot (K)       :List of 2\n  ..$ mean: num 283\n  ..$ sd  : num 8.87\n $ Tdew (degC)    :List of 2\n  ..$ mean: num 4.31\n  ..$ sd  : num 7.08\n $ rh (%)         :List of 2\n  ..$ mean: num 75.9\n  ..$ sd  : num 16.6\n $ VPmax (mbar)   :List of 2\n  ..$ mean: num 13.1\n  ..$ sd  : num 7.6\n $ VPact (mbar)   :List of 2\n  ..$ mean: num 9.19\n  ..$ sd  : num 4.15\n $ VPdef (mbar)   :List of 2\n  ..$ mean: num 3.95\n  ..$ sd  : num 4.77\n $ sh (g/kg)      :List of 2\n  ..$ mean: num 5.81\n  ..$ sd  : num 2.63\n $ H2OC (mmol/mol):List of 2\n  ..$ mean: num 9.3\n  ..$ sd  : num 4.2\n $ rho (g/m**3)   :List of 2\n  ..$ mean: num 1218\n  ..$ sd  : num 42\n $ wv (m/s)       :List of 2\n  ..$ mean: num 2.15\n  ..$ sd  : num 1.53\n $ max. wv (m/s)  :List of 2\n  ..$ mean: num 3.56\n  ..$ sd  : num 2.32\n $ wd (deg)       :List of 2\n  ..$ mean: num 176\n  ..$ sd  : num 85.9\n\nKodnormalize_input_data &lt;- function(df) {\n  normalize &lt;- function(x, center, scale)\n    (x - center) / scale\n\n  for(col_nm in input_data_colnames) {\n    col_nv &lt;- normalization_values[[col_nm]]\n    df[[col_nm]] %&lt;&gt;% normalize(., col_nv$mean, col_nv$sd)\n  }\n\n  df\n}\n\n\nNastÄ™pnie stwÃ³rzmy obiekt TF Dataset, ktÃ³ry zawiera partie danych z ostatnich piÄ™ciu dni wraz z temperaturÄ… docelowÄ… 24 godziny w przyszÅ‚oÅ›ci. PoniewaÅ¼ prÃ³bki w zestawie danych sÄ… wysoce nadmiarowe (prÃ³bka N i prÃ³bka N + 1 bÄ™dÄ… miaÅ‚y wiÄ™kszoÅ›Ä‡ wspÃ³lnych krokÃ³w czasowych), marnotrawstwem byÅ‚oby jawne przydzielanie pamiÄ™ci dla kaÅ¼dej prÃ³bki. Zamiast tego bÄ™dziemy generowaÄ‡ prÃ³bki w locie, zachowujÄ…c w pamiÄ™ci tylko oryginalne tablice danych i nic wiÄ™cej.\nMoglibyÅ›my z Å‚atwoÅ›ciÄ… napisaÄ‡ funkcjÄ™ w R, aby to zrobiÄ‡, ale istnieje wbudowane narzÄ™dzie w keras, ktÃ³re wÅ‚aÅ›nie to robi - (timeseries_dataset_from_array()) - wiÄ™c moÅ¼emy zaoszczÄ™dziÄ‡ sobie trochÄ™ pracy, korzystajÄ…c z niego. OgÃ³lnie rzecz biorÄ…c, moÅ¼na go uÅ¼ywaÄ‡ do wszelkiego rodzaju zadaÅ„ zwiÄ…zanych z prognozowaniem szeregÃ³w czasowych.\n\n\n\n\n\n\nAdnotacja\n\n\n\nAby zrozumieÄ‡ dziaÅ‚anie funkcji timeseries_dataset_from_array(), przyjrzyjmy siÄ™ prostemu przykÅ‚adowi. OgÃ³lna idea polega na dostarczeniu tablicy danych szeregÃ³w czasowych (argument dane), a funkcja timeseries_dataset_from_array() daje okna wyodrÄ™bnione z oryginalnych szeregÃ³w czasowych (nazwiemy je â€œsekwencjamiâ€).\nNa przykÅ‚ad, jeÅ›li uÅ¼yjesz data = [0 1 2 3 4 5 6] i sequence_length = 3, wÃ³wczas timeseries_dataset_from_array() wygeneruje nastÄ™pujÄ…ce prÃ³bki: [0 1 2], [1 2 3] , [2 3 4], [3 4 5], [4 5 6].\nDo funkcji timeseries_dataset_ from_array() moÅ¼na rÃ³wnieÅ¼ przekazaÄ‡ argument targets (tablicÄ™). Pierwszy wpis w tablicy targets powinien odpowiadaÄ‡ poÅ¼Ä…danemu celowi dla pierwszej sekwencji, ktÃ³ra zostanie wygenerowana z tablicy data. Tak wiÄ™c, jeÅ›li wykonujemy prognozowanie szeregÃ³w czasowych, targets powinny byÄ‡ tÄ… samÄ… tablicÄ… co data, przesuniÄ™tÄ… o pewnÄ… wartoÅ›Ä‡.\nNa przykÅ‚ad, z data = [0 1 2 3 4 5 6 ...] i sequence_length = 3, moÅ¼na utworzyÄ‡ zestaw danych do przewidywania nastÄ™pnego kroku w serii, przekazujÄ…c targets = [3 4 5 6 ...]. PrzykÅ‚adowo:\n\nKodint_sequence &lt;- seq(10)\ndummy_dataset &lt;- timeseries_dataset_from_array(\n  data = head(int_sequence, -3),\n  targets = tail(int_sequence, -3),\n  sequence_length = 3,\n  batch_size = 2\n)\n\nlibrary(tfdatasets)\ndummy_dataset_iterator &lt;- as_array_iterator(dummy_dataset)\n\nrepeat {\n  batch &lt;- iter_next(dummy_dataset_iterator)\n  if (is.null(batch))\n    break\n  c(inputs, targets) %&lt;-% batch\n  for (r in 1:nrow(inputs))\n    cat(sprintf(\"input: [ %s ]  target: %s\\n\",\n                paste(inputs[r, ], collapse = \" \"), targets[r]))\n  cat(strrep(\"-\", 27), \"\\n\")\n}\n\ninput: [ 1 2 3 ]  target: 4\ninput: [ 2 3 4 ]  target: 5\n--------------------------- \ninput: [ 3 4 5 ]  target: 6\ninput: [ 4 5 6 ]  target: 7\n--------------------------- \ninput: [ 5 6 7 ]  target: 8\n--------------------------- \n\n\n\n\nUÅ¼yjemy funkcji timeseries_dataset_from_array(), aby utworzyÄ‡ trzy zestawy danych: jeden do szkolenia, jeden do walidacji i jeden do testowania. UÅ¼yjemy nastÄ™pujÄ…cych wartoÅ›ci parametrÃ³w:\n\n\nsampling_rate = 6 - obserwacje bÄ™dÄ… prÃ³bkowane z czÄ™stotliwoÅ›ciÄ… jednego punktu danych na godzinÄ™ (zachowamy tylko jeden punkt danych z 6).\n\nsequence_length = 120 - obserwacje bÄ™dÄ… siÄ™gaÄ‡ piÄ™ciu dni wstecz (120 godzin).\n\ndelay = sampling_rate * (sequence_length + 24 - 1) - celem dla sekwencji bÄ™dzie temperatura 24 godziny po zakoÅ„czeniu sekwencji.\n\n\nKodsampling_rate &lt;- 6\nsequence_length &lt;- 120\ndelay &lt;- sampling_rate * (sequence_length + 24 - 1)\nbatch_size &lt;- 256\n\ndf_to_inputs_and_targets &lt;- function(df) {\n  inputs &lt;- df[input_data_colnames] %&gt;%\n    normalize_input_data() %&gt;%\n    as.matrix()\n\n  targets &lt;- as.array(df$`T (degC)`)\n\n  list(\n    head(inputs, -delay),\n    tail(targets, -delay)\n  )\n}\n\nmake_dataset &lt;- function(df) {\n  c(inputs, targets) %&lt;-% df_to_inputs_and_targets(df)\n  timeseries_dataset_from_array(\n    inputs, targets,\n    sampling_rate = sampling_rate,\n    sequence_length = sequence_length,\n    shuffle = TRUE,\n    batch_size = batch_size\n  )\n}\n\ntrain_dataset &lt;- make_dataset(train_df)\nval_dataset &lt;- make_dataset(val_df)\ntest_dataset &lt;- make_dataset(test_df)\n\n\nKaÅ¼dy zestaw danych daje partie jako parÄ™ (samples, targets), gdzie samples to partia 256 prÃ³bek, z ktÃ³rych kaÅ¼da zawiera 120 kolejnych godzin danych wejÅ›ciowych, a targets to odpowiednia tablica 256 temperatur docelowych. NaleÅ¼y pamiÄ™taÄ‡, Å¼e prÃ³bki sÄ… losowo tasowane, wiÄ™c dwie kolejne sekwencje w partii (takie jak sample[1, ] i sample[2, ]) niekoniecznie sÄ… czasowo blisko siebie.\n\nKodc(samples, targets) %&lt;-% iter_next(as_iterator(train_dataset))\ncat(\"samples shape: \", format(samples$shape), \"\\n\",\n    \"targets shape: \", format(targets$shape), \"\\n\", sep = \"\")\n\nsamples shape: (256, 120, 14)\ntargets shape: (256)\n\n\n\n\n\n\n\n\n\nWaÅ¼ne\n\n\n\nZanim zaczniemy wykorzystywaÄ‡ modele uczenia gÅ‚Ä™bokiego typu black-box w rozwiÄ…zywaniu problemu przewidywania temperatury, wyprÃ³bujmy proste, zdroworozsÄ…dkowe podejÅ›cie. PosÅ‚uÅ¼y ono jako sprawdzian poprawnoÅ›ci i ustanowi punkt odniesienia, ktÃ³ry bÄ™dziemy musieli pokonaÄ‡, aby zademonstrowaÄ‡ przydatnoÅ›Ä‡ bardziej zaawansowanych modeli uczenia maszynowego. Takie zdroworozsÄ…dkowe podstawy mogÄ… byÄ‡ przydatne, gdy zabieramy siÄ™ do nowego problemu, dla ktÃ³rego nie ma (jeszcze) znanego rozwiÄ…zania. Klasycznym przykÅ‚adem sÄ… niezrÃ³wnowaÅ¼one zadania klasyfikacyjne, w ktÃ³rych niektÃ³re klasy wystÄ™pujÄ… znacznie czÄ™Å›ciej niÅ¼ inne. JeÅ›li zbiÃ³r danych zawiera 90% przypadkÃ³w klasy A i 10% przypadkÃ³w klasy B, wÃ³wczas zdroworozsÄ…dkowym podejÅ›ciem do zadania klasyfikacji jest zawsze przewidywanie â€œAâ€, gdy prezentowana jest nowa prÃ³bka. Taki klasyfikator jest ogÃ³lnie dokÅ‚adny w 90%, a zatem kaÅ¼de podejÅ›cie oparte na uczeniu powinno pokonaÄ‡ ten 90% wynik, aby wykazaÄ‡ przydatnoÅ›Ä‡. Czasami takie elementarne wartoÅ›ci bazowe mogÄ… okazaÄ‡ siÄ™ zaskakujÄ…co trudne do pokonania.\n\n\n\n5.2.1 Model bazowy\nW tym przypadku moÅ¼na bezpiecznie zaÅ‚oÅ¼yÄ‡, Å¼e szereg czasowy temperatury jest ciÄ…gÅ‚y (temperatury jutro bÄ™dÄ… prawdopodobnie zbliÅ¼one do temperatur dzisiaj), a takÅ¼e okresowy z okresem dziennym. Dlatego zdroworozsÄ…dkowym podejÅ›ciem jest zawsze przewidywanie, Å¼e temperatura za 24 godziny bÄ™dzie rÃ³wna temperaturze w obecnej chwili. OceÅ„my to podejÅ›cie, korzystajÄ…c z metryki Å›redniego bÅ‚Ä™du bezwzglÄ™dnego (MAE).\nZamiast oceniaÄ‡ wszystko w R stosujÄ…c funkcje for, as_ array_iterator() i iter_next(), moÅ¼emy to rÃ³wnie Å‚atwo zrobiÄ‡ za pomocÄ… transformacji TF Dataset. Najpierw wywoÅ‚ujemy dataset_unbatch(), aby kaÅ¼dy element zbioru danych staÅ‚ siÄ™ pojedynczym przypadkiem (sample, target). NastÄ™pnie uÅ¼ywamy funkcji dataset_map(), aby obliczyÄ‡ bÅ‚Ä…d bezwzglÄ™dny dla kaÅ¼dej pary (sample, target), a nastÄ™pnie dataset_reduce(), aby zgromadziÄ‡ caÅ‚kowity bÅ‚Ä…d i caÅ‚kowitÄ… liczbÄ™ widzianych prÃ³bek.\nPrzypomnijmy, Å¼e funkcje przekazywane do dataset_map() i dataset_reduce() bÄ™dÄ… wywoÅ‚ywane z tensorami symoblicznymi. Wycinanie tensora z liczbÄ… ujemnÄ…, takÄ… jak samples[-1, ], wybiera ostatni wyraz wzdÅ‚uÅ¼ tej osi, tak jakbyÅ›my napisali samples[nrow(samples), ].\n\nKodevaluate_naive_method &lt;- function(dataset) {\n\n  unnormalize_temperature &lt;- function(x) {\n    nv &lt;- normalization_values$`T (degC)`\n    (x * nv$sd) + nv$mean\n  }\n\n  temp_col_idx &lt;- match(\"T (degC)\", input_data_colnames)\n\n  reduction &lt;- dataset %&gt;%\n    dataset_unbatch() %&gt;%\n    dataset_map(function(samples, target) {\n      last_temp_in_input &lt;- samples[-1, temp_col_idx]\n      pred &lt;- unnormalize_temperature(last_temp_in_input)\n      abs(pred - target)\n    }) %&gt;%\n    dataset_reduce(\n      initial_state = list(total_samples_seen = 0L,\n                           total_abs_error = 0),\n      reduce_func = function(state, element) {\n        state$total_samples_seen %&lt;&gt;% `+`(1L)\n        state$total_abs_error %&lt;&gt;% `+`(element)\n        state\n      }\n    ) %&gt;%\n    lapply(as.numeric)\n\n  mae &lt;- with(reduction,\n              total_abs_error / total_samples_seen)\n  mae\n}\n\nsprintf(\"Validation MAE: %.2f\", evaluate_naive_method(val_dataset))\n\n[1] \"Validation MAE: 2.43\"\n\nKodsprintf(\"Test MAE: %.2f\", evaluate_naive_method(test_dataset))\n\n[1] \"Test MAE: 2.62\"\n\n\nZdroworozsÄ…dkowy punkt odniesienia osiÄ…ga MAE na zbiorze walidacyjnym na poziomie 2,43 stopnia Celsjusza i MAE na testowym na poziomie 2,62 stopnia Celsjusza. JeÅ›li wiÄ™c zawsze zakÅ‚adamy, Å¼e temperatura w ciÄ…gu 24 godzin w przyszÅ‚oÅ›ci bÄ™dzie taka sama jak obecnie, bÄ™dziesz siÄ™ myliÅ‚ Å›rednio o okoÅ‚o dwa i pÃ³Å‚ stopnia. Prognoza nie jest zÅ‚a ale z pewnoÅ›ciÄ… da siÄ™ jÄ… poprawiÄ‡.\nW ten sam sposÃ³b, w jaki warto ustaliÄ‡ zdroworozsÄ…dkowy model bazowy przed uÅ¼yciem uczenia maszynowego, warto wyprÃ³bowaÄ‡ rÃ³wnieÅ¼ proste modele uczenia maszynowego (takie jak maÅ‚e, gÄ™sto poÅ‚Ä…czone sieci) przed przejÅ›ciem do bardziej skomplikownych i kosztownych obliczeniowo modeli, takich jak RNN. Jest to najlepszy sposÃ³b na upewnienie siÄ™, Å¼e jakakolwiek dalsza zÅ‚oÅ¼onoÅ›Ä‡ problemu jest uzasadniona i przynosi realne korzyÅ›ci.\n\n5.2.2 Prosty model sieci gÄ™stej\nPoniÅ¼szy kod pokazuje w peÅ‚ni poÅ‚Ä…czony model, ktÃ³ry rozpoczyna siÄ™ od spÅ‚aszczenia danych, a nastÄ™pnie przepuszcza je przez dwie warstwy layer_dense(). ZwrÃ³Ä‡my uwagÄ™ na brak funkcji aktywacji w ostatniej warstwie layer_dense(), co jest typowe dla problemu regresji. UÅ¼ywamy bÅ‚Ä™du Å›redniokwadratowego (MSE) jako funkcji straty, a nie MAE, poniewaÅ¼ w przeciwieÅ„stwie do MAE, jest on rÃ³Å¼niczkowalny wokÃ³Å‚ zera, co jest uÅ¼ytecznÄ… wÅ‚aÅ›ciwoÅ›ciÄ… dla spadku gradientu. MAE bÄ™dziemy rÃ³wnieÅ¼ monitorowaÄ‡, dodajÄ…c go jako metrykÄ™ w funkcji compile().\n\nKodncol_input_data &lt;- length(input_data_colnames)\n\ninputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(16, activation=\"relu\") %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;%\n  compile(optimizer = \"rmsprop\",\n          loss = \"mse\",\n          metrics = \"mae\")\n\nsave_model_tf(model, \"models/jena_dense.keras\")\nsaveRDS(history, \"models/jena_dense_history.rds\")\n\nhistory &lt;- model %&gt;%\n  fit(train_dataset,\n      epochs = 10,\n      validation_data = val_dataset)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_dense.keras\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 2s - loss: 58.4128 - mae: 6.2125 - 2s/epoch - 6ms/step\n\n\n[1] \"Test MAE: 6.21\"\n\nKodhistory &lt;- readRDS(\"models/jena_dense_history.rds\")\nplot(history, metrics = \"mae\")\n\n\n\n\n\n\n\nNiektÃ³re wartoÅ›ci funkcji straty na zbiorze walidacyjnym sÄ… zbliÅ¼one do modelu bazowego ale nie moÅ¼na powiedzieÄ‡, Å¼e model ten jest lepszy od bazowego. To pokazuje zaletÄ™ posiadania modelu odniesienia, bo okazuje siÄ™, Å¼e nie jest Å‚atwo go â€œpokonaÄ‡â€. Zdrowy rozsÄ…dek zawiera wiele cennych informacji, do ktÃ³rych model uczenia maszynowego nie ma dostÄ™pu. Jest to doÅ›Ä‡ istotne ograniczenie uczenia maszynowego w ogÃ³le: o ile algorytm uczenia nie jest zakodowany na sztywno, by szukaÄ‡ konkretnego rodzaju prostego modelu, moÅ¼e on czasem nie znaleÅºÄ‡ prostego rozwiÄ…zania problemu. WÅ‚aÅ›nie dlatego wykorzystanie dobrej inÅ¼ynierii cech i odpowiednich zaÅ‚oÅ¼eÅ„ dotyczÄ…cych architektury ma zasadnicze znaczenie. PowinniÅ›my dokÅ‚adnie powiedzieÄ‡ modelowi, czego powinien szukaÄ‡.\n\n5.2.3 Model oparty o konwolucje 1D\nMÃ³wiÄ…c o wykorzystaniu odpowiednich zaÅ‚oÅ¼eÅ„ co do architektury, byÄ‡ moÅ¼e model konwolucyjny bÄ™dzie dziaÅ‚aÄ‡ poprawnie. SieÄ‡ konwolucyjna 1D mogÅ‚aby ponownie wykorzystywaÄ‡ te same reprezentacje w rÃ³Å¼nych dniach, podobnie jak przestrzenna sieÄ‡ konwolucyjna moÅ¼e ponownie wykorzystywaÄ‡ te same reprezentacje w rÃ³Å¼nych lokalizacjach na obrazie. Znamy juÅ¼ warstwy layer_conv_2d() i layer_separable_conv_2d(), ktÃ³re widzÄ… dane wejÅ›ciowe przez maÅ‚e okna (filtry), ktÃ³re przesuwajÄ… siÄ™ po siatkach 2D. IstniejÄ… rÃ³wnieÅ¼ wersje 1D, a nawet 3D tych warstw: layer_conv_1d(), layer_separable_ conv_1d() i layer_conv_3d(). Warstwa layer_conv_1d() opiera siÄ™ na oknach 1D, ktÃ³re przesuwajÄ… siÄ™ po sekwencjach wejÅ›ciowych, natomiast warstwa layer_conv_3d() opiera siÄ™ na szeÅ›ciennych oknach, ktÃ³re przesuwajÄ… siÄ™ po woluminach wejÅ›ciowych.\nW ten sposÃ³b moÅ¼na budowaÄ‡ sieci konwolucyjne 1D, Å›ciÅ›le analogiczne do sieci 2D. Åšwietnie nadajÄ… siÄ™ do wszelkich danych sekwencyjnych, ktÃ³re sÄ… zgodne z zaÅ‚oÅ¼eniem niezmiennoÅ›ci translacji (co oznacza, Å¼e jeÅ›li przesuniesz okno po sekwencji, zawartoÅ›Ä‡ okna powinna mieÄ‡ te same wÅ‚aÅ›ciwoÅ›ci niezaleÅ¼nie od poÅ‚oÅ¼enia okna).\nWyprÃ³bujmy sieÄ‡ 1D na naszym problemie prognozowania temperatury. Wybierzemy poczÄ…tkowÄ… dÅ‚ugoÅ›Ä‡ okna wynoszÄ…cÄ… 24, tak abyÅ›my patrzyli na 24 godziny danych na raz (jeden cykl). Gdy zmniejszymy prÃ³bkowanie sekwencji (poprzez warstwy layer_max_pooling_1d()), odpowiednio zmniejszymy rozmiar okna:\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_conv_1d(8, 24, activation = \"relu\") %&gt;%\n  layer_max_pooling_1d(2) %&gt;%\n  layer_conv_1d(8, 12, activation = \"relu\") %&gt;%\n  layer_max_pooling_1d(2) %&gt;%\n  layer_conv_1d(8, 6, activation = \"relu\") %&gt;%\n  layer_global_average_pooling_1d() %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;% fit(\n  train_dataset,\n  epochs = 10,\n  validation_data = val_dataset\n)\n\nsave_model_tf(model, filepath = \"models/jena_conv1D.keras\")\nsaveRDS(history, file = \"models/jena_conv1D_history.rds\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_conv1D.keras\")\nhistory &lt;- readRDS(\"models/jena_conv1D_history.rds\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 2s - loss: 18.9587 - mae: 3.4415 - 2s/epoch - 6ms/step\n\n\n[1] \"Test MAE: 3.44\"\n\nKodplot(history)\n\n\n\n\n\n\n\nJak siÄ™ okazuje, model ten wypada jeszcze gorzej niÅ¼ model gÄ™sto poÅ‚Ä…czony, osiÄ…gajÄ…c jedynie testowy MAE na poziomie 3,4 stopnia, daleko od zdroworozsÄ…dkowej wartoÅ›ci bazowej. Co poszÅ‚o nie tak? Dwie rzeczy:\n\nPo pierwsze, dane pogodowe nie do koÅ„ca speÅ‚niajÄ… zaÅ‚oÅ¼enie o niezmiennoÅ›ci translacji. ChociaÅ¼ dane te charakteryzujÄ… siÄ™ cyklami dobowymi, dane z poranka majÄ… inne wÅ‚aÅ›ciwoÅ›ci niÅ¼ dane z wieczora lub ze Å›rodka nocy. Dane pogodowe sÄ… translacyjnie niezmienne tylko w bardzo okreÅ›lonej skali czasowej.\nPo drugie, kolejnoÅ›Ä‡ w naszych danych ma duÅ¼e znaczenie. Niedawna przeszÅ‚oÅ›Ä‡ jest znacznie bardziej pouczajÄ…ca dla przewidywania temperatury nastÄ™pnego dnia niÅ¼ dane sprzed piÄ™ciu dni. SieÄ‡ konwekcyjna 1D nie jest w stanie wykorzystaÄ‡ tego faktu. W szczegÃ³lnoÅ›ci nasze warstwy Å‚Ä…czenia maksymalnego i Å›redniego globalnego w duÅ¼ej mierze niszczÄ… informacje o kolejnoÅ›ci.\n\n5.2.4 Model LSTM\nAni w peÅ‚ni poÅ‚Ä…czone podejÅ›cie, ani podejÅ›cie konwolucyjne nie poradziÅ‚y sobie dobrze z zadanie, ale nie oznacza to, Å¼e uczenie maszynowe nie ma zastosowania do tego problemu. SieÄ‡ gÄ™sto poÅ‚Ä…czona najpierw spÅ‚aszczyÅ‚a szereg czasowy, co usunÄ™Å‚o pojÄ™cie czasu z danych wejÅ›ciowych. PodejÅ›cie konwolucyjne traktowaÅ‚o kaÅ¼dy segment danych w ten sam sposÃ³b, nawet stosujÄ…c Å‚Ä…czenie, ktÃ³re niszczyÅ‚o informacje o kolejnoÅ›ci. Zamiast tego spÃ³jrzmy na dane jako na to, czym sÄ… - sekwencjÄ…, w ktÃ³rej liczy siÄ™ przyczynowoÅ›Ä‡ i kolejnoÅ›Ä‡. W tym celu uÅ¼yjemy sieci LSTM.\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_lstm(16) %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;% fit(\n  train_dataset,\n  epochs = 10,\n  validation_data = val_dataset\n)\n\nsave_model_tf(model, filepath = \"models/jena_lstm.keras\")\nsaveRDS(history, file = \"models/jena_lstm_history.rds\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_lstm.keras\")\nhistory &lt;- readRDS(\"models/jena_lstm_history.rds\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 3s - loss: 11.2176 - mae: 2.6383 - 3s/epoch - 8ms/step\n\n\n[1] \"Test MAE: 2.64\"\n\nKodplot(history)\n\n\n\n\n\n\n\nDalej nie udaÅ‚o siÄ™ pokonaÄ‡ modelu bazowego, ale jesteÅ›my juÅ¼ bardzo blisko. MoÅ¼na siÄ™ teÅ¼ zastanawiÄ…c dlaczego model LSTM wypadÅ‚ znacznie lepiej niÅ¼ model gÄ™sto poÅ‚Ä…czony lub Conv1D? I jak moÅ¼emy dalej udoskonalaÄ‡ ten model? Aby odpowiedzieÄ‡ na to pytanie, przyjrzyjmy siÄ™ bliÅ¼ej rekurencyjnym sieciom neuronowym.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#rnn",
    "href": "rnn.html#rnn",
    "title": "\n5Â  DNN dla danych sekwencyjnych\n",
    "section": "\n6.1 RNN",
    "text": "6.1 RNN\nRekurencyjne sieci neuronowe (ang. Recurrent Neural Network) sÄ… bardzo czÄ™sto uÅ¼ywanym typem sztucznych sieci neuronowych w rozwiÄ…zywaniu zadaÅ„, w ktÃ³rych wartoÅ›ci pewnych cech sÄ… obserwowane w nastÄ™pstwie czasowym. RNN sÄ… specjalnym typem sieci, ktÃ³re pozwalajÄ… na przechowywanie informacji â€œna pÃ³Åºniejâ€ w celu wykorzystania ich przewidywaniu przyszÅ‚ych wartoÅ›ci. W dalszej czÄ™Å›ci tego rozdziaÅ‚u zostanÄ… one szczegÃ³Å‚owo omÃ³wione. RozdziaÅ‚ ten jednak zaczniemy od przybliÅ¼enie z jakimi typami szeregÃ³w czasowych moÅ¼emy mieÄ‡ do czynienia i w jaki sposÃ³b moÅ¼emy uÅ¼ywaÄ‡ do nich sieci rekurencyjnych.\n\nKodmodel &lt;- keras_model_sequential() %&gt;%\n  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;%\n  layer_simple_rnn(units = 32) # retunr only last state\n\nsummary(model)\n\n\nlub\n\nKodmodel &lt;- keras_model_sequential() %&gt;%\n  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;%\n  layer_simple_rnn(units = 32, return_sequences = TRUE) # returns the full state sequence\n\nsummary(model)\n\n\nCzasami przydatne jest uÅ‚oÅ¼enie kilku warstw rekurencyjnych jedna po drugiej w celu zwiÄ™kszenia mocy reprezentacyjnej sieci. W takiej konfiguracji musisz pamiÄ™taÄ‡ wszystkie warstwy poÅ›rednie, aby zwrÃ³ciÄ‡ peÅ‚ne sekwencje:\n\nKodmodel &lt;- keras_model_sequential() %&gt;%\n  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;%\n  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;%\n  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;%\n  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;%\n  layer_simple_rnn(units = 32)\n\nsummary(model)",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#lstm-i-gru",
    "href": "rnn.html#lstm-i-gru",
    "title": "\n5Â  DNN dla danych sekwencyjnych\n",
    "section": "\n6.2 LSTM i GRU",
    "text": "6.2 LSTM i GRU\nProste RNN to nie jedyne warstwy rekurencyjne dostÄ™pne w keras. SÄ… jeszcze dwie inne: layer_lstm i layer_gru. W praktyce zawsze bÄ™dziemy uÅ¼ywaÄ‡ jednej z nich, poniewaÅ¼ layer_simple_rnn jest zbyt prosta, aby byÅ‚a naprawdÄ™ uÅ¼yteczna. Jednym z gÅ‚Ã³wnych problemÃ³w z layer_simple_rnn jest to, Å¼e chociaÅ¼ teoretycznie powinna ona byÄ‡ w stanie zachowaÄ‡ w czasie \\(t\\) informacje o wejÅ›ciach widzianych wiele krokÃ³w czasowych wczeÅ›niej, w praktyce takie dÅ‚ugoterminowe zaleÅ¼noÅ›ci sÄ… niemoÅ¼liwe do nauczenia. Wynika to z problemu znikajÄ…cego gradientu, efektu podobnego do tego, ktÃ³ry obserwuje siÄ™ w sieciach nierekursywnych (feedforward networks), ktÃ³re majÄ… wiele warstw: w miarÄ™ dodawania warstw do sieci, sieÄ‡ w koÅ„cu staje siÄ™ nie do wytrenowania. Teoretyczne przyczyny tego efektu byÅ‚y badane przez Bengio, Simard, i Frasconi (1994) we wczesnych latach 90-tych. Warstwy LSTM i GRU zostaÅ‚y zaprojektowane w celu rozwiÄ…zania tego problemu.\nWeÅºmy pod uwagÄ™ warstwÄ™ LSTM. LeÅ¼Ä…cy u podstaw algorytmu Long Short-Term Memory (LSTM) kod zostaÅ‚ opracowany przez Hochreiter i Schmidhuber (1997) byÅ‚ on zwieÅ„czeniem ich badaÅ„ nad problemem znikajÄ…cego gradientu.\nTa warstwa jest wariantem layer_simple_rnn, wzbogaconym o sposÃ³b na przenoszenie informacji przez wiele krokÃ³w czasowych. WyobraÅº sobie taÅ›mÄ™ transportowÄ… biegnÄ…cÄ… rÃ³wnolegle do sekwencji, ktÃ³rÄ… przetwarzasz. Informacja z sekwencji moÅ¼e wskoczyÄ‡ na taÅ›mÄ™ w dowolnym punkcie, zostaÄ‡ przetransportowana do pÃ³Åºniejszego kroku czasowego i wyskoczyÄ‡ z niej, nienaruszona, kiedy jej potrzebujesz. To jest zasadniczo to, co robi LSTM: zapisuje informacje na pÃ³Åºniej, zapobiegajÄ…c w ten sposÃ³b stopniowemu znikaniu starszych sygnaÅ‚Ã³w podczas przetwarzania.\n\n\n\n\n\nRys.Â 6.2: Schemat sieci LSTM\n\n\nTypowa jednostka LSTM skÅ‚ada siÄ™ z komÃ³rki (ang. cell), bramki wejÅ›ciowej (ang. input gate), bramki wyjÅ›ciowej (ang. output gate) i bramki zapomnienia (ang. forget gate). KomÃ³rka zapamiÄ™tuje wartoÅ›ci w dowolnych odstÄ™pach czasu, a trzy bramki regulujÄ… przepÅ‚yw informacji do i z komÃ³rki. Bramki zapominania decydujÄ… o tym, jakie informacje z poprzedniego stanu naleÅ¼y odrzuciÄ‡, przypisujÄ…c poprzedniemu stanowi, w porÃ³wnaniu z bieÅ¼Ä…cym wejÅ›ciem, wartoÅ›Ä‡ z przedziaÅ‚u od 0 do 1. WartoÅ›Ä‡ 1 oznacza zachowanie informacji, a wartoÅ›Ä‡ 0 - jej odrzucenie. Bramki wejÅ›ciowe decydujÄ…, ktÃ³re kawaÅ‚ki nowej informacji zapisaÄ‡ w bieÅ¼Ä…cym stanie, uÅ¼ywajÄ…c tego samego systemu co bramki zapomnienia. Bramki wyjÅ›ciowe kontrolujÄ…, ktÃ³re fragmenty informacji z bieÅ¼Ä…cego stanu naleÅ¼y wypisaÄ‡, przypisujÄ…c im wartoÅ›Ä‡ od 0 do 1, biorÄ…c pod uwagÄ™ stan poprzedni i bieÅ¼Ä…cy. Selektywne wyprowadzanie odpowiednich informacji z bieÅ¼Ä…cego stanu pozwala sieci LSTM zachowaÄ‡ uÅ¼yteczne, dÅ‚ugoterminowe zaleÅ¼noÅ›ci, pozwalajÄ…ce na dokonywanie przewidywaÅ„, zarÃ³wno w bieÅ¼Ä…cych, jak i przyszÅ‚ych krokach czasowych.\nWrÃ³Ä‡my do modelu opartego na LSTM, ktÃ³rego uÅ¼ywaliÅ›my w przykÅ‚adzie przewidywania temperatury. JeÅ›li spojrzymy na krzywe uczenia, oczywiste jest, Å¼e model szybko ulega przeuczeniu (funkcje straty zaczynajÄ… siÄ™ znacznie rÃ³Å¼niÄ‡ po kilku epokach), mimo Å¼e jest doÅ›Ä‡ prosty. Znamy juÅ¼ klasycznÄ… technikÄ™ przeciwdziaÅ‚ania temu zjawisku. Dropout, ktÃ³ry losowo zeruje jednostki wejÅ›ciowe warstwy, aby przerwaÄ‡ przypadkowe korelacje w danych treningowych, na ktÃ³re naraÅ¼ona jest warstwa. Jednak to, jak prawidÅ‚owo stosowaÄ‡ dropout w sieciach rekurencyjnych, nie jest trywialnym pytaniem.\nSprawdzono, Å¼e zastosowanie dropout przed warstwÄ… rekurencyjnÄ… raczej utrudnia uczenie siÄ™ niÅ¼ pomaga w regularyzacji. W 2016 roku Yarin Gal, w ramach swojej pracy doktorskiej na temat gÅ‚Ä™bokiego uczenia bayesowskiego, okreÅ›liÅ‚ wÅ‚aÅ›ciwy sposÃ³b stosowania dropoutu w sieci rekurencyjnej: ta sama maska dropoutu (ten sam wzÃ³r porzuconych jednostek) powinna byÄ‡ stosowana w kaÅ¼dym kroku czasowym, zamiast stosowania maski dropoutu, ktÃ³ra zmienia siÄ™ losowo z kroku na krok czasowy. Co wiÄ™cej, aby uregulowaÄ‡ reprezentacje utworzone przez rekurencyjne bramki warstw, takich jak layer_gru() i layer_lstm(), do wewnÄ™trznych rekurencyjnych aktywacji warstwy naleÅ¼y zastosowaÄ‡ czasowo staÅ‚Ä… maskÄ™ dropout (rekurencyjnÄ… maskÄ™ porzucania). UÅ¼ywanie tej samej maski porzucania w kaÅ¼dym kroku czasowym pozwala sieci na prawidÅ‚owÄ… propagacjÄ™ bÅ‚Ä™du uczenia siÄ™ w czasie; czasowo losowa maska porzucania zakÅ‚Ã³ciÅ‚aby ten sygnaÅ‚ bÅ‚Ä™du i byÅ‚aby szkodliwa dla procesu uczenia siÄ™.\nYarin Gal przeprowadziÅ‚ swoje badania przy uÅ¼yciu keras i pomÃ³gÅ‚ zaimplementowaÄ‡ ten mechanizm bezpoÅ›rednio w warstwach rekurencyjnych keras. KaÅ¼da warstwa rekurencyjna w keras ma dwa argumenty zwiÄ…zane z dropout: dropout, zmienna okreÅ›lajÄ…ca wspÃ³Å‚czynnik porzucenia dla jednostek wejÅ›ciowych warstwy, oraz recurrent_dropout, okreÅ›lajÄ…ca wspÃ³Å‚czynnik porzucenia dla jednostek rekurencyjnych. Dodajmy rekurencyjne porzucanie do funkcji layer_lstm() naszego przykÅ‚adu LSTM i zobaczmy, jak wpÅ‚ywa to na overfitting.\nDziÄ™ki dropoutowi nie bÄ™dziemy musieli tak bardzo polegaÄ‡ na rozmiarze sieci do regularyzacji, wiÄ™c uÅ¼yjemy warstwy LSTM z dwukrotnie wiÄ™kszÄ… liczbÄ… jednostek, co powinno, miejmy nadziejÄ™, byÄ‡ bardziej wyraziste (bez dropoutu ta sieÄ‡ od razu zaczÄ™Å‚aby siÄ™ przeuczaÄ‡2). PoniewaÅ¼ sieci regularyzowane z dropoutem zawsze potrzebujÄ… znacznie wiÄ™cej czasu, aby osiÄ…gnÄ…Ä‡ zbieÅ¼noÅ›Ä‡, bÄ™dziemy trenowaÄ‡ model przez piÄ™Ä‡ razy wiÄ™cej epok.\n2Â moÅ¼esz sam sprÃ³bowaÄ‡\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_lstm(32, recurrent_dropout = 0.25) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;% fit(\n  train_dataset,\n  epochs = 50,\n  validation_data = val_dataset\n)\nsave_model_tf(\"models/jena_lstm_dropout.keras\")\n\n\n\n\n\n\n\n\nZagroÅ¼enie\n\n\n\nModele rekurencyjne z bardzo maÅ‚Ä… liczbÄ… parametrÃ³w, takie jak te w tym rozdziale, sÄ… zwykle znacznie szybsze na wielordzeniowym CPU niÅ¼ na GPU, poniewaÅ¼ obejmujÄ… tylko mnoÅ¼enia maÅ‚ych macierzy, a Å‚aÅ„cuch mnoÅ¼eÅ„ nie jest dobrze zrÃ³wnoleglony ze wzglÄ™du na obecnoÅ›Ä‡ pÄ™tli for. WiÄ™ksze sieci RNN mogÄ… jednak w znacznym stopniu skorzystaÄ‡ z moÅ¼liwoÅ›ci GPU.\nPodczas korzystania z warstw LSTM i GRU na GPU z domyÅ›lnymi argumentami, warstwy bÄ™dÄ… wykorzystywaÄ‡ jÄ…dro cuDNN, wysoce zoptymalizowanÄ…, niskopoziomowÄ… implementacjÄ™ algorytmu dostarczonÄ… przez firmÄ™ NVIDIA. Niestety, jÄ…dra cuDNN sÄ… wÄ…tpliwym bÅ‚ogosÅ‚awieÅ„stwem: sÄ… szybkie, ale nieelastyczne - jeÅ›li sprÃ³bujemy zrobiÄ‡ coÅ›, co nie jest obsÅ‚ugiwane przez domyÅ›lne jÄ…dro, doÅ›wiadczymy dramatycznego spowolnienia. PrzykÅ‚adowo, rekurencyjny dropout nie jest obsÅ‚ugiwany przez jÄ…dra LSTM i GRU cuDNN, wiÄ™c dodanie go do warstw zmusza algorytm do wykonywania na zwykÅ‚ej implementacji TensorFlow, ktÃ³ra jest generalnie od dwÃ³ch do piÄ™ciu razy wolniejsza na GPU (mimo Å¼e jej koszt obliczeniowy jest taki sam).\nAby przyspieszyÄ‡ dziaÅ‚anie warstwy RNN, gdy nie moÅ¼na uÅ¼yÄ‡ cuDNN, moÅ¼na sprÃ³bowaÄ‡ jÄ… rozwinÄ…Ä‡ (ang. unfold). Rozwijanie pÄ™tli for polega na usuniÄ™ciu pÄ™tli i po prostu wpisaniu jej zawartoÅ›ci N razy. W przypadku pÄ™tli for sieci RNN, rozwijanie moÅ¼e pomÃ³c TensorFlow zoptymalizowaÄ‡ bazowy graf obliczeniowy. Jednak znacznie zwiÄ™kszy to rÃ³wnieÅ¼ zuÅ¼ycie pamiÄ™ci przez sieÄ‡ RNN. W zwiÄ…zku z tym jest to opÅ‚acalne tylko w przypadku stosunkowo maÅ‚ych sekwencji (okoÅ‚o 100 krokÃ³w lub mniej). NaleÅ¼y rÃ³wnieÅ¼ pamiÄ™taÄ‡, Å¼e moÅ¼na to zrobiÄ‡ tylko wtedy, gdy liczba krokÃ³w czasowych w danych jest znana z gÃ³ry przez model. DziaÅ‚a to w nastÄ™pujÄ…cy sposÃ³b:\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, num_features))\nx &lt;- inputs %&gt;% layer_lstm(32, recurrent_dropout = 0.2, unroll = TRUE)\n\n\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_lstm_dropout.keras\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 273s - loss: 10.2870 - mae: 2.5415 - 273s/epoch - 675ms/step\n\n\n[1] \"Test MAE: 2.54\"\n\n\n\n\n\n\n\nRys.Â 6.3\n\n\nRys.Â 6.3 przedstawia wyniki uczenia. UsuniÄ™to przeuczenie (do co najmniej 20 epoki). OsiÄ…gamy MAE walidacji na poziomie zaledwie 2,37 stopnia (2,5% poprawa w stosunku do modelu bazowego bez uczenia) i testowy MAE na poziomie 2,54 stopnia (3% poprawa w stosunku do lini bazowej).\nPoniewaÅ¼ overfitting nie jest juÅ¼ tak wyraÅºnym problemem, ale wydaje siÄ™, Å¼e trafiliÅ›my na wÄ…skie gardÅ‚o wydajnoÅ›ci, powinniÅ›my rozwaÅ¼yÄ‡ zwiÄ™kszenie pojemnoÅ›ci i mocy obliczeniowej sieci. Przypomnijmy sobie opis uniwersalnego przepÅ‚ywu pracy uczenia maszynowego: generalnie dobrym pomysÅ‚em jest zwiÄ™kszenie pojemnoÅ›ci modelu, dopÃ³ki overfitting nie stanie siÄ™ gÅ‚Ã³wnym problemem.\nZwiÄ™kszenie pojemnoÅ›ci sieci odbywa siÄ™ zazwyczaj poprzez zwiÄ™kszenie liczby neuronÃ³w w warstwach lub dodanie wiÄ™kszej liczby warstw. SkÅ‚adanie warstw rekurencyjnych to klasyczny sposÃ³b budowania potÄ™Å¼niejszych sieci rekurencyjnych. Aby ukÅ‚adaÄ‡ warstwy rekurencyjne jedna na drugiej w Kerasie, wszystkie warstwy poÅ›rednie powinny zwracaÄ‡ peÅ‚nÄ… sekwencjÄ™ swoich wyjÅ›Ä‡ (tensor rangi 3), a nie swoje wyjÅ›cie w ostatnim kroku czasowym. Jak juÅ¼ siÄ™ dowiedzieliÅ›my, odbywa siÄ™ to poprzez ustawienie flagi return_ sequences = TRUE.\nW poniÅ¼szym przykÅ‚adzie wyprÃ³bujemy stos dwÃ³ch warstw rekurencyjnych z regularyzacjÄ… dropout. Dla odmiany uÅ¼yjemy warstw Gated Recurrent Unit (GRU) zamiast LSTM. GRU jest bardzo podobny do LSTM - moÅ¼na o nim myÅ›leÄ‡ jako o nieco prostszej, usprawnionej wersji architektury LSTM. ZostaÅ‚a ona wprowadzona w 2014 roku przez Cho i in. (b.d.), gdy sieci rekurencyjne dopiero zaczynaÅ‚y na nowo zyskiwaÄ‡ zainteresowanie w niewielkiej wÃ³wczas spoÅ‚ecznoÅ›ci badawczej.\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_gru(32, recurrent_dropout = 0.5, return_sequences = TRUE) %&gt;%\n  layer_gru(32, recurrent_dropout = 0.5) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;% fit(\n  train_dataset,\n  epochs = 50,\n  validation_data = val_dataset\n)\nsave_model_tf(\"models/jena_gru_dropout.keras\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_gru_dropout.keras\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 577s - loss: 9.6074 - mae: 2.4491 - 577s/epoch - 1s/step\n\n\n[1] \"Test MAE: 2.45\"\n\n\n\n\n\n\n\nRys.Â 6.4\n\n\nRys.Â 6.4 przedstawia wyniki uczenia. OsiÄ…gnÄ™liÅ›my testowy MAE na poziomie 2,45 stopnia (poprawa o 6,5% w stosunku do linii bazowej). WidaÄ‡, Å¼e dodana warstwa nieco poprawia wyniki, choÄ‡ nie dramatycznie, zatem moÅ¼na zaobserwowaÄ‡ malejÄ…ce zyski ze zwiÄ™kszania pojemnoÅ›ci sieci.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#dwukierunkowe-sieci-rekurencyjne",
    "href": "rnn.html#dwukierunkowe-sieci-rekurencyjne",
    "title": "\n5Â  DNN dla danych sekwencyjnych\n",
    "section": "\n6.3 Dwukierunkowe sieci rekurencyjne",
    "text": "6.3 Dwukierunkowe sieci rekurencyjne\nOstatniÄ… technikÄ…, ktÃ³rej przyjrzymy siÄ™ w tej sekcji, jest dwukierunkowa sieÄ‡ RNN. Dwukierunkowy RNN jest powszechnym wariantem RNN, ktÃ³ry moÅ¼e oferowaÄ‡ wiÄ™kszÄ… wydajnoÅ›Ä‡ niÅ¼ zwykÅ‚y RNN w niektÃ³rych zadaniach. Jest czÄ™sto uÅ¼ywany w przetwarzaniu jÄ™zyka naturalnego - moÅ¼na go nazwaÄ‡ szwajcarskim scyzorykiem gÅ‚Ä™bokiego uczenia siÄ™ do przetwarzania jÄ™zyka naturalnego.\nRNN sÄ… w szczegÃ³lnoÅ›ci zaleÅ¼ne od kolejnoÅ›ci: przetwarzajÄ… kroki czasowe swoich sekwencji wejÅ›ciowych w kolejnoÅ›ci, a tasowanie lub odwracanie krokÃ³w czasowych moÅ¼e caÅ‚kowicie zmieniÄ‡ reprezentacje, ktÃ³re RNN wyodrÄ™bnia z sekwencji. To jest wÅ‚aÅ›nie powÃ³d, dla ktÃ³rego dobrze radzÄ… sobie z problemami, w ktÃ³rych kolejnoÅ›Ä‡ ma znaczenie, takimi jak problem prognozowania temperatury. Dwukierunkowa RNN wykorzystuje wraÅ¼liwoÅ›Ä‡ RNN na kolejnoÅ›Ä‡: wykorzystuje dwie zwykÅ‚e RNN, takie jak GRU i LSTM, z ktÃ³rych kaÅ¼da przetwarza sekwencjÄ™ wejÅ›ciowÄ… w jednym kierunku (chronologicznie i antychronologicznie), a nastÄ™pnie Å‚Ä…czy ich reprezentacje. PrzetwarzajÄ…c sekwencjÄ™ w obie strony, dwukierunkowa sieÄ‡ RNN moÅ¼e wychwyciÄ‡ wzorce, ktÃ³re mogÄ… zostaÄ‡ przeoczone przez jednokierunkowÄ… sieÄ‡ RNN.\nCzy RNN mogÅ‚yby dziaÅ‚aÄ‡ wystarczajÄ…co dobrze, gdyby na przykÅ‚ad przetwarzaÅ‚y sekwencje wejÅ›ciowe w porzÄ…dku antychronologicznym (z nowszymi krokami czasowymi jako pierwszymi)? SprÃ³bujmy tego i zobaczmy, co siÄ™ stanie. Wszystko, co musimy zrobiÄ‡, to zmodyfikowaÄ‡ zestaw danych TF, aby sekwencje wejÅ›ciowe zostaÅ‚y odwrÃ³cone wzdÅ‚uÅ¼ wymiaru czasu. Wystarczy przeksztaÅ‚ciÄ‡ zbiÃ³r danych za pomocÄ… funkcji dataset_map() w nastÄ™pujÄ…cy sposÃ³b:\n\nKoddataset_map(function(samples, targets) {\n  list(samples[, NA:NA:-1, ], targets)\n})\n\n\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_lstm(16) %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\ncallbacks &lt;- list(callback_model_checkpoint(\"jena_lstm_reversed\",\n                                            save_best_only = TRUE))\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\ndataset_reverse_time &lt;- function(ds) {\n  dataset_map(ds, function(samples, targets)\n    list(samples[, NA:NA:-1, ], targets))\n}\nhistory &lt;- model %&gt;% fit(\n  train_dataset %&gt;% dataset_reverse_time(),\n  epochs = 10,\n  validation_data = val_dataset %&gt;% dataset_reverse_time(),\n  callbacks = callbacks\n)\nsave_model_tf(model, \"models/jena_lstm_rev.keras\")\nsaveRDS(history, \"models/jena_lstm_rev_hist.rds\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_lstm_rev.keras\")\nhistory &lt;- readRDS(\"models/jena_lstm_rev_hist.rds\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 3s - loss: 22.8799 - mae: 3.8075 - 3s/epoch - 8ms/step\n\n\n[1] \"Test MAE: 3.81\"\n\nKodplot(history)\n\n\n\n\n\n\n\nLSTM z odwrÃ³conym czasem silnie ustÄ™puje nawet zdroworozsÄ…dkowemu poziomowi bazowemu, wskazujÄ…c, Å¼e w tym przypadku przetwarzanie chronologiczne jest waÅ¼ne dla powodzenia tego podejÅ›cia. Ma to sens: warstwa LSTM zazwyczaj lepiej zapamiÄ™tuje niedawnÄ… przeszÅ‚oÅ›Ä‡ niÅ¼ odlegÅ‚Ä… przeszÅ‚oÅ›Ä‡, a naturalnie bardziej aktualne dane pogodowe niosÄ… w sobie wiÄ™cej informacji niÅ¼ starsze dane (to wÅ‚aÅ›nie sprawia, Å¼e zdroworozsÄ…dkowa linia bazowa jest doÅ›Ä‡ silna). Tak wiÄ™c chronologiczna wersja sieci z pewnoÅ›ciÄ… przewyÅ¼szy wersjÄ™ z odwrÃ³conym porzÄ…dkiem.\nNie jest to jednak prawdÄ… w przypadku wielu innych problemÃ³w, w tym jÄ™zyka naturalnego: intuicyjnie, znaczenie sÅ‚owa w zrozumieniu zdania zwykle nie zaleÅ¼y od jego pozycji w zdaniu. W przypadku danych tekstowych, przetwarzanie w odwrÃ³conej kolejnoÅ›ci dziaÅ‚a rÃ³wnie dobrze jak przetwarzanie chronologiczne - moÅ¼na czytaÄ‡ tekst od tyÅ‚u. ChociaÅ¼ kolejnoÅ›Ä‡ sÅ‚Ã³w ma znaczenie dla zrozumienia jÄ™zyka, to kolejnoÅ›Ä‡, ktÃ³rej uÅ¼ywamy, nie jest kluczowa. Co waÅ¼ne, RNN wytrenowana na odwrÃ³conych sekwencjach nauczy siÄ™ innych reprezentacji niÅ¼ ta wytrenowana na oryginalnych sekwencjach, podobnie jak w prawdziwym Å›wiecie mielibyÅ›my inne modele mentalne, gdyby czas pÅ‚ynÄ…Å‚ wstecz - gdybyÅ›my Å¼yli Å¼yciem, w ktÃ³rym umieramy pierwszego dnia i rodzimy siÄ™ ostatniego. W uczeniu maszynowym reprezentacje, ktÃ³re sÄ… uÅ¼yteczne, sÄ… zawsze warte wykorzystania, a im bardziej siÄ™ rÃ³Å¼niÄ…, tym lepiej, bo oferujÄ… nowy kÄ…t, z ktÃ³rego moÅ¼na spojrzeÄ‡ na dane, wychwytujÄ…c aspekty danych, ktÃ³re zostaÅ‚y pominiÄ™te przez inne podejÅ›cia, a tym samym mogÄ… pomÃ³c zwiÄ™kszyÄ‡ wydajnoÅ›Ä‡ zadania.\nDwukierunkowa sieÄ‡ RNN wykorzystuje ten pomysÅ‚, aby poprawiÄ‡ wydajnoÅ›Ä‡ sieci RNN z porzÄ…dkiem chronologicznym. Analizuje sekwencjÄ™ wejÅ›ciowÄ… w obie strony (patrz Rys.Â 6.5), uzyskujÄ…c potencjalnie bogatsze reprezentacje i wychwytujÄ…c wzorce, ktÃ³re mogÅ‚y zostaÄ‡ pominiÄ™te przez samÄ… wersjÄ™ chronologicznÄ….\n\n\n\n\n\nRys.Â 6.5: Zasada dziaÅ‚ania warstw dwukierunkowych\n\n\nAby utworzyÄ‡ instancjÄ™ dwukierunkowej RNN w Keras, naleÅ¼y uÅ¼yÄ‡ warstw bidirectional(), ktÃ³re jako pierwszy argument przyjmujÄ… instancjÄ™ warstwy rekurencyjnej. bidirectional() tworzy drugÄ…, oddzielnÄ… instancjÄ™ tej warstwy rekurencyjnej i wykorzystuje jednÄ… instancjÄ™ do przetwarzania sekwencji wejÅ›ciowych w porzÄ…dku chronologicznym, a drugÄ… instancjÄ™ do przetwarzania sekwencji wejÅ›ciowych w porzÄ…dku odwrÃ³conym.\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  bidirectional(layer_lstm(units = 16)) %&gt;%\n  layer_dense(1)\n\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;%\n  fit(train_dataset,\n      epochs = 10,\n      validation_data = val_dataset)\n\nsave_model_tf(model, \"models/jena_lstm_bi.keras\")\nsaveRDS(history, \"models/jena_lstm_bi_hist.rds\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_lstm_bi.keras\")\nhistory &lt;- readRDS(\"models/jena_lstm_bi_hist.rds\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 5s - loss: 10.8280 - mae: 2.5962 - 5s/epoch - 13ms/step\n\n\n[1] \"Test MAE: 2.60\"\n\nKodplot(history)\n\n\n\n\n\n\n\nJeÅ›li porÃ³wnamy wyniki do zwykÅ‚ej layer_lstm(), to zauwaÅ¼ymy tylko nieznacznÄ… poprawÄ™ wynikÃ³w. Åatwo jest zrozumieÄ‡, dlaczego - niemal caÅ‚a zdolnoÅ›Ä‡ predykcyjna musi pochodziÄ‡ z chronologicznej poÅ‚owy sieci, poniewaÅ¼ wiadomo, Å¼e antychronologiczna poÅ‚owa ma znacznie gorsze wyniki w tym zadaniu (ponownie, poniewaÅ¼ niedawna przeszÅ‚oÅ›Ä‡ ma w tym przypadku znacznie wiÄ™ksze znaczenie niÅ¼ odlegÅ‚a przeszÅ‚oÅ›Ä‡). JednoczeÅ›nie obecnoÅ›Ä‡ antychronologicznej poÅ‚owy podwaja pojemnoÅ›Ä‡ sieci i powoduje, Å¼e zaczyna siÄ™ ona przeuczaÄ‡ znacznie wczeÅ›niej.\nJednak dwukierunkowe sieci RNN doskonale nadajÄ… siÄ™ do danych tekstowych lub innych rodzajÃ³w danych, w ktÃ³rych kolejnoÅ›Ä‡ ma znaczenie, ale gdzie kolejnoÅ›Ä‡, ktÃ³rej uÅ¼ywasz, nie ma znaczenia. W rzeczywistoÅ›ci aÅ¼ do roku 2016 dwukierunkowe LSTM byÅ‚y uwaÅ¼ane za najnowoczeÅ›niejsze w wielu zadaniach przetwarzania jÄ™zyka naturalnego (przed pojawieniem siÄ™ architektury Transformer, o ktÃ³rej bÄ™dzie nieco pÃ³Åºniej).\nIstnieje wiele innych rzeczy, ktÃ³re moÅ¼na wyprÃ³bowaÄ‡ w celu poprawy wydajnoÅ›ci prognozowania temperatury:\n\nDostosowaÄ‡ liczbÄ™ neuronÃ³w w kaÅ¼dej warstwie rekurencyjnej, a takÅ¼e iloÅ›Ä‡ porzuconych neurnonÃ³w. Obecne wybory sÄ… w duÅ¼ej mierze arbitralne, a zatem prawdopodobnie nieoptymalne.\nDostosowaÄ‡ szybkoÅ›Ä‡ uczenia optymalizatora RMSprop lub wyprÃ³bowaÄ‡ inny optymalizator.\nUÅ¼yÄ‡ stosu kilku warstw gÄ™stych layer_dense() jako regresora na wierzchu warstwy rekurencyjnej, zamiast pojedynczej.\nUlepszyÄ‡ dane wejÅ›ciowe do modelu - uÅ¼yÄ‡ dÅ‚uÅ¼szych lub krÃ³tszych sekwencji lub innej czÄ™stotliwoÅ›ci prÃ³bkowania lub wykonaÄ‡ inÅ¼ynieriÄ™ cech.\n\n\n\n\n\n\n\nWskazÃ³wka\n\n\n\nGÅ‚Ä™bokie uczenie jest bardziej sztukÄ… niÅ¼ naukÄ…. MoÅ¼emy dostarczaÄ‡ wskazÃ³wek, ktÃ³re sugerujÄ…, co moÅ¼e dziaÅ‚aÄ‡ lub nie dziaÅ‚aÄ‡ w danym problemie, ale ostatecznie kaÅ¼dy zbiÃ³r danych jest wyjÄ…tkowy; bÄ™dziesz musiaÅ‚ empirycznie oceniÄ‡ rÃ³Å¼ne strategie. Obecnie nie istnieje Å¼adna teoria, ktÃ³ra z gÃ³ry powiedziaÅ‚aby, co powinniÅ›my zrobiÄ‡, aby optymalnie rozwiÄ…zaÄ‡ dany problem.\n\n\n\n\n\n\n\n\nOstrzeÅ¼enie\n\n\n\nNiektÃ³rzy czytelnicy z pewnoÅ›ciÄ… bÄ™dÄ… chcieli skorzystaÄ‡ z technik, ktÃ³re tu przedstawiliÅ›my i wyprÃ³bowaÄ‡ je w problemie prognozowania przyszÅ‚ych cen papierÃ³w wartoÅ›ciowych na gieÅ‚dzie (lub kursÃ³w wymiany walut itp.). Rynki majÄ… jednak zupeÅ‚nie innÄ… charakterystykÄ™ statystycznÄ… niÅ¼ zjawiska naturalne, takie jak wzorce pogodowe. JeÅ›li chodzi o rynki, przeszÅ‚e wyniki nie sÄ… dobrym predyktorem przyszÅ‚ych zwrotÃ³w3. Z drugiej strony uczenie maszynowe ma zastosowanie do zbiorÃ³w danych, w ktÃ³rych przeszÅ‚oÅ›Ä‡ jest dobrym predyktorem przyszÅ‚oÅ›ci, takich jak pogoda, zuÅ¼ycie energii elektrycznej lub ruch pieszych na danym odcinku drogi.\nZawsze pamiÄ™tajmy, Å¼e caÅ‚y handel papierami wartoÅ›ciowymi jest zasadniczo arbitraÅ¼em informacyjnym: zdobywaniem przewagi poprzez wykorzystanie danych lub spostrzeÅ¼eÅ„, ktÃ³rych brakuje innym uczestnikom rynku. PrÃ³ba wykorzystania dobrze znanych technik uczenia maszynowego i publicznie dostÄ™pnych danych w celu pokonania rynkÃ³w jest w rzeczywistoÅ›ci Å›lepym zauÅ‚kiem, poniewaÅ¼ nie bÄ™dzie dawaÄ‡ Å¼adnej przewagi informacyjnej w porÃ³wnaniu do wszystkich innych.\n\n\n\n\n3Â patrzenie w lusterko wsteczne nie jest najlepszÄ… metodÄ… prowadzenia auta ğŸ™‰ ğŸ¤”\n\nBengio, Y., P. Simard, i P. Frasconi. 1994. â€Learning long-term dependencies with gradient descent is difficultâ€. IEEE Transactions on Neural Networks 5 (2): 157â€“66. https://doi.org/10.1109/72.279181.\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, i Yoshua Bengio. b.d. â€On the Properties of Neural Machine Translation: Encoder-Decoder Approachesâ€. https://doi.org/10.48550/arXiv.1409.1259.\n\n\nHochreiter, Sepp, i JÃ¼rgen Schmidhuber. 1997. â€Long Short-Term Memoryâ€. Neural Computation 9 (8): 1735â€“80. https://doi.org/10.1162/neco.1997.9.8.1735.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "llm.html",
    "href": "llm.html",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "",
    "text": "6.1 Rys historyczny\nW informatyce, ludzkie jÄ™zyki, takie jak angielski czy mandaryÅ„ski, okreÅ›la siÄ™ mianem â€œnaturalnychâ€, aby odrÃ³Å¼niÄ‡ je od jÄ™zykÃ³w zaprojektowanych dla maszyn, takich jak Assembly, LISP czy XML. KaÅ¼dy jÄ™zyk maszynowy zostaÅ‚ zaprojektowany - jego punkt wyjÅ›cia stanowiÅ‚ inÅ¼ynier, ktÃ³ry spisaÅ‚ zbiÃ³r formalnych zasad opisujÄ…cych, jakie stwierdzenia moÅ¼na formuÅ‚owaÄ‡ w danym jÄ™zyku i co one oznaczajÄ…. Zasady powstaÅ‚y jako pierwsze, a ludzie zaczÄ™li uÅ¼ywaÄ‡ jÄ™zyka dopiero po ukoÅ„czeniu zestawu reguÅ‚. W przypadku jÄ™zyka ludzkiego jest odwrotnie, najpierw pojawia siÄ™ uÅ¼ycie, a zasady powstajÄ… pÃ³Åºniej. JÄ™zyk naturalny zostaÅ‚ uksztaÅ‚towany przez proces ewolucji, podobnie jak organizmy biologiczne - to czyni go â€œnaturalnymâ€. Jego â€œzasadyâ€, takie jak gramatyka jÄ™zyka polskiego, zostaÅ‚y sformalizowane dopiero po fakcie i czÄ™sto sÄ… ignorowane lub Å‚amane przez jego uÅ¼ytkownikÃ³w. W rezultacie, chociaÅ¼ jÄ™zyk â€œmaszynowyâ€, czytelny dla maszyn jest wysoce uporzÄ…dkowany i rygorystyczny, uÅ¼ywajÄ…c precyzyjnych zasad skÅ‚adniowych do Å‚Ä…czenia dokÅ‚adnie zdefiniowanych pojÄ™Ä‡ z ustalonego sÅ‚ownictwa, jÄ™zyk naturalny jest nieuporzÄ…dkowany â€“ niejednoznaczny, chaotyczny, rozlegÅ‚y i ciÄ…gle podlegajÄ…cy zmianom.\nTworzenie algorytmÃ³w zdolnych do rozumienia jÄ™zyka naturalnego to powaÅ¼ne wyzwanie, bo w koÅ„cu jÄ™zyk, a w szczegÃ³lnoÅ›ci tekst, stanowi podstawÄ™ wiÄ™kszoÅ›ci naszych komunikatÃ³w i rozwoju kulturowego. Internet to w wiÄ™kszoÅ›ci tekst. JÄ™zyk to sposÃ³b, w jaki przechowujemy niemal caÅ‚Ä… naszÄ… wiedzÄ™. Nasze myÅ›li w duÅ¼ej mierze opierajÄ… siÄ™ na jÄ™zyku. Jednak zdolnoÅ›Ä‡ rozumienia jÄ™zyka naturalnego przez dÅ‚ugi czas byÅ‚a poza zasiÄ™giem maszyn. NiektÃ³rzy ludzie naiwnie sÄ…dzili, Å¼e moÅ¼na po prostu spisaÄ‡ â€œzestaw zasad jÄ™zyka angielskiegoâ€, podobnie jak moÅ¼na spisaÄ‡ zestaw zasad LISP1. Wczesne prÃ³by budowy systemÃ³w przetwarzania jÄ™zyka naturalnego (NLP) byÅ‚y wiÄ™c podejmowane przez pryzmat â€œstosowanej lingwistykiâ€. InÅ¼ynierowie i lingwiÅ›ci rÄ™cznie tworzyli zÅ‚oÅ¼one zestawy zasad, aby wykonaÄ‡ podstawowe tÅ‚umaczenia maszynowe lub stworzyÄ‡ proste chatboty, takie jak sÅ‚ynny program ELIZA2 z lat 60., ktÃ³ry uÅ¼ywaÅ‚ dopasowywania wzorcÃ³w, aby podtrzymaÄ‡ bardzo podstawowÄ… konwersacjÄ™. Ale jÄ™zyk jest rzeczÄ… niepokornÄ…: nie poddaje siÄ™ Å‚atwo formalizacji. Po kilku dekadach wysiÅ‚kÃ³w moÅ¼liwoÅ›ci tych systemÃ³w pozostaÅ‚y rozczarowujÄ…ce.\nRÄ™cznie tworzone reguÅ‚y utrzymywaÅ‚y siÄ™ jako dominujÄ…ce podejÅ›cie aÅ¼ do lat 90. XX wieku. Jednak zaczynajÄ…c od koÅ„ca lat 80., szybsze komputery i wiÄ™ksza dostÄ™pnoÅ›Ä‡ danych zaczÄ™Å‚y czyniÄ‡ alternatywÄ™ bardziej wykonalnÄ…. Gdy znajdziemy siÄ™ w sytuacji, w ktÃ³rej budujemy systemy bÄ™dÄ…ce duÅ¼ymi stosami reguÅ‚ ad hoc, prawdopodobnie zaczniemy zadawaÄ‡ sobie pytanie: â€œCzy mogÄ™ uÅ¼yÄ‡ korpusu danych, aby zautomatyzowaÄ‡ proces znajdowania tych reguÅ‚? Czy mogÄ™ szukaÄ‡ reguÅ‚ w pewnej przestrzeni reguÅ‚, zamiast samemu je wymyÅ›laÄ‡?â€ I tak wÅ‚aÅ›nie przeszÅ‚o siÄ™ do uczenia maszynowego. W zwiÄ…zku z tym, pod koniec lat 80. zaczÄ™liÅ›my obserwowaÄ‡ podejÅ›cia uczenia maszynowego do przetwarzania jÄ™zyka naturalnego. NajwczeÅ›niejsze z nich opieraÅ‚y siÄ™ na drzewach decyzyjnych â€“ intencjÄ… byÅ‚o dosÅ‚ownie zautomatyzowanie rozwoju rodzaju reguÅ‚ if/then/else poprzednich systemÃ³w. NastÄ™pnie podejÅ›cia statystyczne zaczÄ™Å‚y zyskiwaÄ‡ na popularnoÅ›ci, zaczynajÄ…c od regresji logistycznej. Z czasem, modele parametryczne oparte na uczeniu w peÅ‚ni przejÄ™Å‚y kontrolÄ™, a lingwistyka zaczÄ™Å‚a byÄ‡ postrzegana bardziej jako przeszkoda niÅ¼ uÅ¼yteczne narzÄ™dzie. Frederick Jelinek, wczesny badacz rozpoznawania mowy, Å¼artowaÅ‚ w latach 90.: â€œZa kaÅ¼dym razem, gdy zwalniam lingwistÄ™, wydajnoÅ›Ä‡ systemu rozpoznawania mowy roÅ›nie.â€\nTo na czym polega wspÃ³Å‚czesne przetwarzanie jÄ™zyka naturalnego (NLP), to wykorzystanie uczenia maszynowego i duÅ¼ych zbiorÃ³w danych, aby daÄ‡ komputerom zdolnoÅ›Ä‡ nie tyle rozumienia jÄ™zyka, co bardziej ambitnego celu, przyswajania fragmentu jÄ™zyka jako danych wejÅ›ciowych i zwracania czegoÅ› uÅ¼ytecznego, na przykÅ‚ad przewidywania nastÄ™pujÄ…cych kwestii:\nOczywiÅ›cie, powinniÅ›my pamiÄ™taÄ‡, Å¼e modele przetwarzania tekstu, ktÃ³re bÄ™dziemy szkoliÄ‡, nie bÄ™dÄ… posiadaÄ‡ ludzkiego zrozumienia jÄ™zyka, raczej bÄ™dÄ… po prostu szukaÄ‡ statystycznych reguÅ‚ w danych wejÅ›ciowych, co okazuje siÄ™ wystarczajÄ…ce do dobrego wykonywania wielu prostych zadaÅ„. W podobny sposÃ³b, w jaki rozpoznawanie obrazÃ³w, to rozpoznawanie wzorcÃ³w stosowane do pikseli, przetwarzanie jÄ™zyka naturalnego (NLP) to rozpoznawanie wzorcÃ³w stosowane do sÅ‚Ã³w, zdaÅ„ i akapitÃ³w.\nNarzÄ™dzia NLP - drzewa decyzyjne i regresja logistyczna - ewoluowaÅ‚y, choÄ‡ powoli od lat 90. do wczesnych lat 2010. WiÄ™kszoÅ›Ä‡ badaÅ„ skupiaÅ‚a siÄ™ na inÅ¼ynierii cech. Kiedy FranÃ§ois Chollet wygraÅ‚ swÃ³j pierwszy konkurs NLP na Kaggle w 2013 roku, jego model opieraÅ‚ siÄ™ na drzewach decyzyjnych i regresji logistycznej. Jednak okoÅ‚o 2014-2015 roku sytuacja zaczÄ™Å‚a siÄ™ wreszcie zmieniaÄ‡. Wielu badaczy zaczÄ™Å‚o badaÄ‡ zdolnoÅ›ci rozumienia jÄ™zyka przez rekurencyjne sieci neuronowe, w szczegÃ³lnoÅ›ci LSTM.\nNa poczÄ…tku 2015 roku, keras udostÄ™pniÅ‚ pierwszÄ… otwartÄ…, Å‚atwÄ… w uÅ¼yciu implementacjÄ™ LSTM, tuÅ¼ na poczÄ…tku ogromnej fali zainteresowania sieciami neuronowymi rekurencyjnymi. NastÄ™pnie od 2015 do 2017 roku, sieci neuronowe rekurencyjne zdominowaÅ‚y rozwijajÄ…cÄ… siÄ™ scenÄ™ NLP. Modele LSTM dwukierunkowe, w szczegÃ³lnoÅ›ci, ustanowiÅ‚y standard w wielu waÅ¼nych zadaniach, od streszczania, przez odpowiedzi na pytania, po tÅ‚umaczenie maszynowe. W koÅ„cu okoÅ‚o 2017â€“2018 roku pojawiÅ‚a siÄ™ nowa architektura, ktÃ³ra zastÄ…piÅ‚a RNN - transformer, o ktÃ³rym dowiemy siÄ™ wiÄ™cej w drugiej czÄ™Å›ci tego rozdziaÅ‚u. Transformatory umoÅ¼liwiÅ‚y znaczÄ…cy postÄ™p w caÅ‚ej dziedzinie w krÃ³tkim czasie, a obecnie wiÄ™kszoÅ›Ä‡ systemÃ³w NLP opiera siÄ™ na nich.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#rys-historyczny",
    "href": "llm.html#rys-historyczny",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "",
    "text": "1Â LISP (skrÃ³t od LISt Processing) jest jednym z najstarszych jÄ™zykÃ³w programowania, nadal uÅ¼ywanych. ZostaÅ‚ zaprojektowany w 1958 roku przez Johna McCarthyâ€™ego w Massachusetts Institute of Technology (MIT)2Â ELIZA to jeden z pierwszych programÃ³w komputerowych, ktÃ³ry imitowaÅ‚ rozmowÄ™ z czÅ‚owiekiem. ZostaÅ‚ stworzony w poÅ‚owie lat 60. XX wieku przez Josepha Weizenbauma w Massachusetts Institute of Technology (MIT)\n\n\nâ€œJaki jest temat tego tekstu?â€ (klasyfikacja tekstu);\nâ€œCzy ten tekst zawiera treÅ›ci obraÅºliwe?â€ (filtrowanie treÅ›ci);\nâ€œCzy ten tekst brzmi pozytywnie czy negatywnie?â€ (analiza sentymentu);\nâ€œJaki powinno byÄ‡ nastÄ™pne sÅ‚owo w tym niekompletnym zdaniu?â€ (modelowanie jÄ™zyka);\nâ€œJak powiedziaÅ‚byÅ› to po niemiecku?â€ (tÅ‚umaczenie);\nâ€œJak moÅ¼na by streÅ›ciÄ‡ ten artykuÅ‚ w jednym akapicie?â€ (streszczanie);\nI tak dalej.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#standaryzacja-tekstu",
    "href": "llm.html#standaryzacja-tekstu",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.1 Standaryzacja tekstu",
    "text": "7.1 Standaryzacja tekstu\nStandaryzacja tekstu jest podstawowÄ… formÄ… inÅ¼ynierii cech, ktÃ³ra ma na celu usuniÄ™cie rÃ³Å¼nic w kodowaniu, z ktÃ³rymi nie chcemy, aby nasz model miaÅ‚ do czynienia. Nie jest to wyÅ‚Ä…czna dziedzina uczenia maszynowego - musielibyÅ›my zrobiÄ‡ to samo, gdybyÅ›my budowali wyszukiwarkÄ™. Jednym z najprostszych i najbardziej rozpowszechnionych schematÃ³w standaryzacji jest â€œkonwersja na maÅ‚e litery i usuniÄ™cie znakÃ³w interpunkcyjnychâ€.\nInnym czÄ™stym przeksztaÅ‚ceniem jest konwersja znakÃ³w specjalnych do standardowej formy, np. zastÄ…pienie â€œÃ©â€ przez â€œeâ€, â€œÃ¦â€ przez â€œaeâ€ itd. Np. token â€œmÃ©xicoâ€ staÅ‚by siÄ™ wtedy â€œmexicoâ€.\nOstatnim, znacznie bardziej zaawansowanym wzorcem standaryzacji, ktÃ³ry jest rzadziej uÅ¼ywany w kontekÅ›cie uczenia maszynowego, jest stemming: przeksztaÅ‚canie odmian sÅ‚Ã³w (takich jak rÃ³Å¼ne formy koniugacyjne czasownika) w jednÄ… wspÃ³lnÄ… reprezentacjÄ™, jak przeksztaÅ‚canie â€œzÅ‚apanyâ€ i â€œÅ‚apiÄ…câ€ w â€œ[Å‚apaÄ‡]â€ lub â€œkotyâ€ w â€œ[kot]â€. DziÄ™ki stemmingowi, â€œrozpoczynajÄ…câ€ i â€œrozpoczÄ™tyâ€ staÅ‚yby siÄ™ czymÅ› w rodzaju â€œ[rozpoczynaÄ‡]â€.\nDziÄ™ki tym technikom standaryzacji, nasz model bÄ™dzie wymagaÅ‚ mniej danych treningowych i bÄ™dzie lepiej generalizowaÅ‚ - nie bÄ™dzie potrzebowaÅ‚ wielu przykÅ‚adÃ³w zarÃ³wno â€œZachodu sÅ‚oÅ„caâ€, jak i â€œzachodÃ³w sÅ‚oÅ„caâ€, aby nauczyÄ‡ siÄ™, Å¼e oznaczajÄ… one to samo, i bÄ™dzie w stanie nadaÄ‡ sens sÅ‚owu â€œMeksykâ€, nawet jeÅ›li widziaÅ‚ tylko â€œmeksykâ€ w swoim zestawie treningowym. OczywiÅ›cie standaryzacja moÅ¼e rÃ³wnieÅ¼ wymazaÄ‡ pewnÄ… iloÅ›Ä‡ informacji, wiÄ™c zawsze naleÅ¼y pamiÄ™taÄ‡ o kontekÅ›cie: na przykÅ‚ad, jeÅ›li piszesz model, ktÃ³ry wyodrÄ™bnia pytania z artykuÅ‚Ã³w z wywiadami, powinien on zdecydowanie traktowaÄ‡ â€œ?â€ jako oddzielny token zamiast go upuszczaÄ‡, poniewaÅ¼ jest to przydatny sygnaÅ‚ dla tego konkretnego zadania.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#tokenizacja",
    "href": "llm.html#tokenizacja",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.2 Tokenizacja",
    "text": "7.2 Tokenizacja\nKiedy tekst jest juÅ¼ znormalizowany, musimy podzieliÄ‡ go na jednostki do wektoryzacji (tokeny) - krok zwany tokenizacjÄ…. MoÅ¼na to zrobiÄ‡ na trzy rÃ³Å¼ne sposoby:\n\nTokenizacja na poziomie sÅ‚owa - gdzie tokeny sÄ… oddzielonymi spacjami (lub interpunkcjÄ…) podciÄ…gami. Wariantem tego jest dalsze dzielenie sÅ‚Ã³w na podsÅ‚owia, gdy ma to zastosowanie, na przykÅ‚ad traktowanie â€œzaczynaâ€ jako â€œzaczyna+jÄ…câ€ lub â€œwezwanyâ€ jako â€œwezwaniâ€.\nN-gram tokenizacji - gdzie tokeny sÄ… grupami N kolejnych sÅ‚Ã³w. Na przykÅ‚ad â€œthe catâ€ lub â€œhe wasâ€ byÅ‚yby tokenami 2-gramowymi (zwanymi rÃ³wnieÅ¼ bigramami). Generalnie N-gramy sÅ‚Ã³w to grupy N (lub mniej) kolejnych sÅ‚Ã³w, ktÃ³re moÅ¼na wyodrÄ™bniÄ‡ ze zdania. Ta sama koncepcja moÅ¼e byÄ‡ rÃ³wnieÅ¼ zastosowana do znakÃ³w zamiast sÅ‚Ã³w.\nTokenizacja na poziomie znakÃ³w - gdzie kaÅ¼dy znak jest swoim wÅ‚asnym tokenem. W praktyce, ten schemat jest rzadko uÅ¼ywany i naprawdÄ™ widzisz go tylko w specjalistycznych kontekstach, takich jak generowanie tekstu lub rozpoznawanie mowy.\n\nOgÃ³lnie rzecz biorÄ…c, zawsze bÄ™dziemy uÅ¼ywaÄ‡ tokenizacji na poziomie sÅ‚owa lub N-gramu. IstniejÄ… dwa rodzaje modeli przetwarzania tekstu: te, ktÃ³re dbajÄ… o kolejnoÅ›Ä‡ sÅ‚Ã³w, zwane modelami sekwencyjnymi, oraz te, ktÃ³re traktujÄ… sÅ‚owa wejÅ›ciowe jako zestaw, odrzucajÄ…c ich oryginalnÄ… kolejnoÅ›Ä‡, zwane modelami bag-of-words. JeÅ›li budujesz model sekwencyjny, uÅ¼ywasz tokenizacji na poziomie sÅ‚Ã³w, a jeÅ›li budujesz model worka sÅ‚Ã³w, uÅ¼ywasz tokenizacji N-gramÃ³w. N-gramy sÄ… sposobem na sztuczne wprowadzenie do modelu niewielkiej iloÅ›ci informacji o lokalnym porzÄ…dku sÅ‚Ã³w. W tym rozdziale dowiesz siÄ™ wiÄ™cej o kaÅ¼dym typie modelu i o tym, kiedy naleÅ¼y ich uÅ¼ywaÄ‡.\nPrzykÅ‚adowo zdanie â€œthe cat sat on the matâ€ moÅ¼na zamieniÄ‡ na bigramy w nastÄ™pujÄ…cy sposÃ³b:\n\nKodc(\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\")\n\n [1] \"the\"     \"the cat\" \"cat\"     \"cat sat\" \"sat\"     \"sat on\"  \"on\"     \n [8] \"on the\"  \"the mat\" \"mat\"    \n\n\nnatomiast w 3-gramy:\n\nKodc(\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\",\n \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\",\n \"sat on the\", \"the mat\", \"mat\", \"on the mat\")\n\n [1] \"the\"         \"the cat\"     \"cat\"         \"cat sat\"     \"the cat sat\"\n [6] \"sat\"         \"sat on\"      \"on\"          \"cat sat on\"  \"on the\"     \n[11] \"sat on the\"  \"the mat\"     \"mat\"         \"on the mat\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#indeksowanie-sÅ‚ownika",
    "href": "llm.html#indeksowanie-sÅ‚ownika",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.3 Indeksowanie sÅ‚ownika",
    "text": "7.3 Indeksowanie sÅ‚ownika\nGdy nasz tekst jest podzielony na tokeny, musimy zakodowaÄ‡ kaÅ¼dy token w reprezentacji numerycznej. Wszystkie procesy wektoryzacji tekstu polegajÄ… na zastosowaniu pewnego schematu tokenizacji, a nastÄ™pnie skojarzeniu wektorÃ³w liczbowych z wygenerowanymi tokenami. Wektory te, spakowane w tensory sekwencji, sÄ… wprowadzane do gÅ‚Ä™bokich sieci neuronowych. Istnieje wiele sposobÃ³w na powiÄ…zanie wektora z tokenem. W tej sekcji przedstawimy dwa gÅ‚Ã³wne: kodowanie tokenÃ³w metodÄ… one-hot oraz osadzanie tokenÃ³w (ang. embeddings - zwykle uÅ¼ywane wyÅ‚Ä…cznie dla sÅ‚Ã³w). PozostaÅ‚a czÄ™Å›Ä‡ tego rozdziaÅ‚u wyjaÅ›nia te techniki i pokazuje jak ich uÅ¼yÄ‡, aby przejÅ›Ä‡ od surowego tekstu do tensora, ktÃ³ry moÅ¼na wysÅ‚aÄ‡ do sieci.\nTechnicznie rzecz ujmujÄ…c, naleÅ¼y zauwaÅ¼yÄ‡, Å¼e na tym etapie czÄ™sto ogranicza siÄ™ sÅ‚ownictwo tylko do 20000 lub 30000 najczÄ™Å›ciej wystÄ™pujÄ…cych sÅ‚Ã³w znalezionych w danych treningowych. KaÅ¼dy zbiÃ³r danych tekstowych ma tendencjÄ™ do zawierania ogromnej liczby unikalnych terminÃ³w, z ktÃ³rych wiÄ™kszoÅ›Ä‡ pojawia siÄ™ tylko raz lub dwa. Indeksowanie tych rzadkich terminÃ³w skutkowaÅ‚oby nadmiernie duÅ¼Ä… przestrzeniÄ… cech, gdzie wiÄ™kszoÅ›Ä‡ cech miaÅ‚aby prawie Å¼adnÄ… zawartoÅ›Ä‡ informacyjnÄ….\nWaÅ¼ny szczegÃ³Å‚, ktÃ³ry nie powinien umknÄ…Ä‡ naszej uwadze: gdy szukamy nowego tokenu w naszym indeksie sÅ‚ownictwa, moÅ¼e siÄ™ okazaÄ‡, Å¼e go tam nie ma. Nasze dane treningowe mogÅ‚y nie zawieraÄ‡ Å¼adnego wystÄ…pienia sÅ‚owa â€œcherimoyaâ€3 (lub byÄ‡ moÅ¼e wykluczyliÅ›my je z indeksu, poniewaÅ¼ byÅ‚o zbyt rzadkie), wiÄ™c wykonanie polecenia token_index = match(\"cherimoya\", vocabulary) moÅ¼e zwrÃ³ciÄ‡ NA. Aby sobie z tym poradziÄ‡, naleÅ¼y uÅ¼yÄ‡ indeksu â€œpoza sÅ‚ownictwemâ€ (skrÃ³towo OOV index) - rodzaju schowka na wszystkie tokeny, ktÃ³re nie znalazÅ‚y siÄ™ w indeksie. Zwykle jest to indeks 1: tak naprawdÄ™ wykonujesz token_index = match(\"cherimoya\", vocabulary, nomatch = 1). DekodujÄ…c sekwencjÄ™ liczb caÅ‚kowitych z powrotem na sÅ‚owa, zastÄ…pisz 1 czymÅ› w rodzaju â€œ[UNK]â€ (co nazwaÅ‚byÅ› â€œtokenem OOVâ€).\n3Â przypominajÄ…cy ksztaÅ‚tem serce owoc, jest sÅ‚odki jak budyÅ„, dlatego Anglicy nazywajÄ… go custard apple - czyli jabÅ‚ko o smaku kremu budyniowegoâ€œDlaczego uÅ¼ywamy 1, a nie 0?â€ moÅ¼na zapytaÄ‡. PoniewaÅ¼ 0 jest juÅ¼ zajÄ™te. IstniejÄ… dwa specjalne tokeny, ktÃ³rych bÄ™dziemy czÄ™sto uÅ¼ywaÄ‡: token OOV (indeks 1) i token maski (indeks 0). ChociaÅ¼ token OOV oznacza â€œto jest sÅ‚owo, ktÃ³rego nie rozpoznaliÅ›myâ€, to token maski mÃ³wi nam â€œzignoruj mnie, nie jestem sÅ‚owemâ€. UÅ¼ywa siÄ™ go w szczegÃ³lnoÅ›ci do uzupeÅ‚niania danych sekwencyjnych: poniewaÅ¼ partie danych muszÄ… byÄ‡ ciÄ…gÅ‚e, wszystkie sekwencje w partii danych sekwencyjnych muszÄ… mieÄ‡ tÄ™ samÄ… dÅ‚ugoÅ›Ä‡, wiÄ™c krÃ³tsze sekwencje powinny byÄ‡ uzupeÅ‚niane do dÅ‚ugoÅ›ci najdÅ‚uÅ¼szej sekwencji. JeÅ›li chcemy utworzyÄ‡ partiÄ™ danych z sekwencjami c(5, 7, 124, 4, 89) i c(8, 34, 21), musiaÅ‚aby ona wyglÄ…daÄ‡ nastÄ™pujÄ…co:\n\nKodrbind(c(5,  7, 124, 4, 89),\n      c(8, 34,  21, 0,  0))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    5    7  124    4   89\n[2,]    8   34   21    0    0\n\n\nWszystkie omÃ³wione do tej pory kroki moÅ¼na w prosty sposÃ³b zaprogramowaÄ‡ w R:\n\nKodlibrary(keras) # pÃ³ki co potrzebny tylko by mieÄ‡ %&gt;%\nnew_vectorizer &lt;- function() {\n  self &lt;- new.env(parent = emptyenv()) # Utworzenie nowego Å›rodowiska dla wektoryzatora\n  attr(self, \"class\") &lt;- \"Vectorizer\"   # Nadanie Å›rodowisku klasy \"Vectorizer\"\n\n  self$vocabulary &lt;- c(\"[UNK]\")         # Inicjalizacja sÅ‚ownika sÅ‚Ã³w z tokenem \"[UNK]\" (nieznane sÅ‚owo)\n\n  self$standardize &lt;- function(text) {\n    text &lt;- tolower(text)               # PrzeksztaÅ‚cenie tekstu na maÅ‚e litery\n    gsub(\"[[:punct:]]\", \"\", text)       # UsuniÄ™cie znakÃ³w interpunkcyjnych z tekstu\n  }\n\n  self$tokenize &lt;- function(text) {\n    unlist(strsplit(text, \"[[:space:]]+\")) # PodziaÅ‚ tekstu na tokeny (sÅ‚owa) na podstawie spacji\n  }\n\n  self$make_vocabulary &lt;- function(text_dataset) {\n    tokens &lt;- text_dataset %&gt;%\n      self$standardize() %&gt;%\n      self$tokenize()                   # Standardyzacja i tokenizacja tekstu\n    self$vocabulary &lt;- unique(c(self$vocabulary, tokens)) # Aktualizacja sÅ‚ownika o unikalne tokeny\n  }\n\n  self$encode &lt;- function(text) {\n    tokens &lt;- text %&gt;%\n      self$standardize() %&gt;%\n      self$tokenize()                   # Standardyzacja i tokenizacja tekstu\n    match(tokens, table = self$vocabulary, nomatch = 1) # Zamiana tokenÃ³w na ich indeksy w sÅ‚owniku\n  }\n\n  self$decode &lt;- function(int_sequence) {\n    vocab_w_mask_token &lt;- c(\"\", self$vocabulary) # SÅ‚ownik z dodanym pustym tokenem na poczÄ…tku\n    vocab_w_mask_token[int_sequence + 1]         # Zamiana sekwencji indeksÃ³w na sÅ‚owa\n  }\n\n  self # ZwrÃ³cenie Å›rodowiska wektoryzatora\n}\n\n\nA tak wyglÄ…da on w dziaÅ‚aniuâ€¦ Najpierw tworzymy sÅ‚ownik na podstawie prostego korpusu.\n\nKodvectorizer &lt;- new_vectorizer()\n\ndataset &lt;- c(\n    \"I write, erase, rewrite\",\n    \"Erase again, and then\",\n    \"A poppy blooms.\"\n)\n\nvectorizer$make_vocabulary(dataset)\nvectorizer$vocabulary\n\n [1] \"[UNK]\"   \"i\"       \"write\"   \"erase\"   \"rewrite\" \"again\"   \"and\"    \n [8] \"then\"    \"a\"       \"poppy\"   \"blooms\" \n\n\nA nastÄ™pnie wykorzystujemy go do nowego zdania.\n\nKodtest_sentence &lt;- \"I write, rewrite, and still rewrite again\"\nencoded_sentence &lt;- vectorizer$encode(test_sentence)\nprint(encoded_sentence)\n\n[1] 2 3 5 7 1 5 6\n\nKoddecoded_sentence &lt;- vectorizer$decode(encoded_sentence)\nprint(decoded_sentence)\n\n[1] \"i\"       \"write\"   \"rewrite\" \"and\"     \"[UNK]\"   \"rewrite\" \"again\"  \n\n\nChoÄ‡ jak widaÄ‡ wszystko dziaÅ‚a poprawnie, to w praktycznych zastosowaniach bÄ™dziemy korzystali raczej z rozwiÄ…zaÅ„ w keras typu warstwa layer_text_vectorization().\n\nKodtext_vectorization &lt;- layer_text_vectorization(output_mode = \"int\")\n\n\nDomyÅ›lnie, layer_text_vectorization() bÄ™dzie uÅ¼ywaÄ‡ ustawienia â€œkonwertuj na maÅ‚e litery i usuÅ„ znaki interpunkcyjneâ€ do standaryzacji tekstu oraz â€œdziel wzglÄ™dem znakÃ³w przerw (typu spacja)â€ do tokenizacji. Ale co waÅ¼ne, moÅ¼na dostarczyÄ‡ niestandardowe funkcje do standaryzacji i tokenizacji, co oznacza, Å¼e warstwa jest wystarczajÄ…co elastyczna, aby obsÅ‚uÅ¼yÄ‡ kaÅ¼dy przypadek uÅ¼ycia. NaleÅ¼y pamiÄ™taÄ‡, Å¼e takie niestandardowe funkcje powinny dziaÅ‚aÄ‡ na tensorach typu tf.string, a nie na zwykÅ‚ych wektorach znakÃ³w R! Na przykÅ‚ad, domyÅ›lne zachowanie warstwy jest rÃ³wnowaÅ¼ne nastÄ™pujÄ…cemu:\n\nKodlibrary(tensorflow)\ncustom_standardization_fn &lt;- function(string_tensor) {\n  string_tensor %&gt;%\n    tf$strings$lower() %&gt;%\n    tf$strings$regex_replace(\"[[:punct:]]\", \"\")\n}\n\ncustom_split_fn &lt;- function(string_tensor) {\n  tf$strings$split(string_tensor)\n}\n\ntext_vectorization &lt;- layer_text_vectorization(\n  output_mode = \"int\",\n  standardize = custom_standardization_fn,\n  split = custom_split_fn\n)\n\n\nAby zindeksowaÄ‡ sÅ‚ownictwo korpusu tekstowego, wystarczy wywoÅ‚aÄ‡ metodÄ™ adapt() warstwy z obiektem TF Dataset, ktÃ³ry daje ciÄ…gi znakÃ³w, lub po prostu z wektorem znakÃ³w R:\n\nKoddataset &lt;- c(\"I write, erase, rewrite\",\n             \"Erase again, and then\",\n             \"A poppy blooms.\")\nadapt(text_vectorization, dataset)\n\n\nNaleÅ¼y pamiÄ™taÄ‡, Å¼e obliczone sÅ‚ownictwo moÅ¼na pobraÄ‡ za pomocÄ… funkcji get_vocabulary(). MoÅ¼e to byÄ‡ przydatne, jeÅ›li trzeba przekonwertowaÄ‡ tekst zakodowany jako sekwencje liczb caÅ‚kowitych z powrotem na sÅ‚owa. Pierwsze dwa wpisy w sÅ‚owniku to token maski (indeks 0) i token OOV (indeks 1). Wpisy na liÅ›cie sÅ‚ownictwa sÄ… sortowane wedÅ‚ug czÄ™stotliwoÅ›ci, wiÄ™c w przypadku zbioru danych z rzeczywistego Å›wiata bardzo popularne sÅ‚owa, takie jak â€œtheâ€ lub â€œaâ€, bÄ™dÄ… na pierwszym miejscu.\n\nKodget_vocabulary(text_vectorization)\n\n [1] \"\"        \"[UNK]\"   \"erase\"   \"write\"   \"then\"    \"rewrite\" \"poppy\"  \n [8] \"i\"       \"blooms\"  \"and\"     \"again\"   \"a\"      \n\n\nNa potrzeby prezentacji, sprÃ³bujmy zakodowaÄ‡, a nastÄ™pnie zdekodowaÄ‡ przykÅ‚adowe zdanie:\n\nKodvocabulary &lt;- text_vectorization %&gt;% get_vocabulary()\ntest_sentence &lt;- \"I write, rewrite, and still rewrite again\"\nencoded_sentence &lt;- text_vectorization(test_sentence)\ndecoded_sentence &lt;- paste(vocabulary[as.integer(encoded_sentence) + 1],\n                          collapse = \" \")\n\nencoded_sentence\n\ntf.Tensor([ 7  3  5  9  1  5 10], shape=(7), dtype=int64)\n\nKoddecoded_sentence\n\n[1] \"i write rewrite and [UNK] rewrite again\"\n\n\n\n\n\n\n\n\nWskazÃ³wka\n\n\n\nPoniewaÅ¼ layer_text_vectorization() jest gÅ‚Ã³wnie operacjÄ… wyszukiwania sÅ‚ownika, ktÃ³ra konwertuje tokeny na liczby caÅ‚kowite, nie moÅ¼e byÄ‡ wykonywana na GPU (lub TPU) - tylko na CPU. JeÅ›li wiÄ™c trenujemy nasz model na GPU, funkcja layer_text_vectorization() zostanie uruchomiona na CPU przed wysÅ‚aniem danych wyjÅ›ciowych do GPU. Ma to istotny wpÅ‚yw na wydajnoÅ›Ä‡.\nIstniejÄ… dwa sposoby wykorzystania funkcji layer_text_vectorization(). PierwszÄ… opcjÄ… jest umieszczenie jej w potoku TF Dataset, tak jak poniÅ¼ej:\n\nKodint_sequence_dataset &lt;- string_dataset %&gt;%\n  dataset_map(text_vectorization, num_parallel_calls = 4)\n\n\nDrugÄ… opcjÄ… jest uczynienie go czÄ™Å›ciÄ… modelu (w koÅ„cu jest to warstwa keras), jak poniÅ¼ej (w pseudokodzie):\n\nKodtext_input &lt;- layer_input(shape = shape(), dtype = \"string\")\nvectorized_text &lt;- text_vectorization(text_input)\nembedded_input &lt;- vectorized_text %&gt;% layer_embedding(...)\noutput &lt;- embedded_input %&gt;% ...\nmodel &lt;- keras_model(text_input, output)\n\n\nJest miÄ™dzy nimi waÅ¼na rÃ³Å¼nica: jeÅ›li krok wektoryzacji jest czÄ™Å›ciÄ… modelu, bÄ™dzie on wykonywany synchronicznie z resztÄ… modelu. Oznacza to, Å¼e na kaÅ¼dym etapie uczenia reszta modelu (umieszczona na GPU) bÄ™dzie musiaÅ‚a poczekaÄ‡, aÅ¼ dane wyjÅ›ciowe layer_text_vectorization() (umieszczone na CPU) bÄ™dÄ… gotowe, zanim bÄ™dzie mogÅ‚a rozpoczÄ…Ä‡ pracÄ™. Tymczasem umieszczenie warstwy w potoku TF Dataset umoÅ¼liwia asynchroniczne wstÄ™pne przetwarzanie danych na CPU: podczas gdy GPU uruchamia model na jednej partii zwektoryzowanych danych, CPU pozostaje zajÄ™ty wektoryzacjÄ… nastÄ™pnej partii surowych ciÄ…gÃ³w.\nJeÅ›li trenujemy model na GPU lub TPU, prawdopodobnie bÄ™dziemy chcieli wybraÄ‡ pierwszÄ… opcjÄ™4, aby uzyskaÄ‡ najlepszÄ… wydajnoÅ›Ä‡. Podczas trenowania na CPU, przetwarzanie synchroniczne jest w porzÄ…dku: uzyskamy wÃ³wczas 100% wykorzystania rdzeni, niezaleÅ¼nie od wybranej opcji.\nTeraz, jeÅ›li mielibyÅ›my wyeksportowaÄ‡ nasz model do Å›rodowiska produkcyjnego, chcielibyÅ›my wysÅ‚aÄ‡ model, ktÃ³ry akceptuje surowe ciÄ…gi znakÃ³w jako dane wejÅ›ciowe, jak w powyÅ¼szym fragmencie kodu dla drugiej opcji; w przeciwnym razie musielibyÅ›my ponownie wdroÅ¼yÄ‡ standaryzacjÄ™ tekstu i tokenizacjÄ™ w naszym Å›rodowisku produkcyjnym (np. w JavaScript). StanÄ™libyÅ›my w obliczu ryzyka wprowadzenia niewielkich zmian w przetwarzaniu wstÄ™pnym, ktÃ³re zaszkodziÅ‚yby dokÅ‚adnoÅ›ci modelu. Na szczÄ™Å›cie funkcja layer_text_vectorization() umoÅ¼liwia wÅ‚Ä…czenie wstÄ™pnego przetwarzania tekstu bezpoÅ›rednio do modelu, co uÅ‚atwia jego wdroÅ¼enie, nawet jeÅ›li pierwotnie warstwa byÅ‚a uÅ¼ywana jako czÄ™Å›Ä‡ potoku TF Dataset.\n\n\n4Â czyli osadzenie wektoryzacji w potoku",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#one-hot-encoding",
    "href": "llm.html#one-hot-encoding",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.4 One-hot encoding",
    "text": "7.4 One-hot encoding\nKodowanie one-hot jest najczÄ™stszym, najbardziej podstawowym sposobem przeksztaÅ‚cenia tokena w wektor. Polega ono na skojarzeniu unikalnego indeksu z kaÅ¼dym sÅ‚owem, a nastÄ™pnie przeksztaÅ‚ceniu tego indeksu \\(i\\) w wektor binarny o rozmiarze \\(N\\) (rozmiar sÅ‚ownika); wektor skÅ‚ada siÄ™ ze wszystkich zer, z wyjÄ…tkiem \\(i\\)-tego wpisu, ktÃ³ry jest 1.\n\nKodone_hot_encode_token &lt;- function(token) {\n  vector &lt;- array(0, dim = length(vocabulary))\n  token_index &lt;- match(token, vocabulary)\n  vector[token_index] &lt;- 1\n  vector\n}\n\n\nMetoda ta, zwaÅ¼ywszy na swojÄ… rzadkÄ… reprezentacjÄ™ (wiÄ™kszoÅ›Ä‡ wartoÅ›ci to 0), rzadko stosowana w praktyce. SÅ‚aba wydajnoÅ›Ä‡ tej techniki spowodowaÅ‚a powstanie embedingÃ³w. Zanim jednak przejdziemy do embedingÃ³w przyjrzymy siÄ™ dokÅ‚adniej dwÃ³m podejÅ›ciom do reprezentacji grup sÅ‚Ã³w: zbiorom sÅ‚Ã³w (ang. bag-of-words) i ciÄ…gom sÅ‚Ã³w, w ktÃ³rych kolejnoÅ›Ä‡ jest waÅ¼na. Zrobimy to na przykÅ‚adzie danych IMDB.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#tokenizacja-na-przykÅ‚adach",
    "href": "llm.html#tokenizacja-na-przykÅ‚adach",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.5 Tokenizacja na przykÅ‚adach",
    "text": "7.5 Tokenizacja na przykÅ‚adach\nSposÃ³b, w jaki model uczenia maszynowego powinien reprezentowaÄ‡ poszczegÃ³lne sÅ‚owa, jest stosunkowo niekontrowersyjnÄ… kwestiÄ…: sÄ… to cechy kategorialne (wartoÅ›ci z predefiniowanego zestawu) i wiemy, jak sobie z nimi radziÄ‡. Powinny one byÄ‡ zakodowane jako wymiary w przestrzeni cech lub jako wektory kategorii (w tym przypadku wektory sÅ‚Ã³w). Znacznie bardziej problematycznÄ… kwestiÄ… jest jednak to, jak zakodowaÄ‡ sposÃ³b, w jaki sÅ‚owa sÄ… wplecione w zdania - kolejnoÅ›Ä‡ sÅ‚Ã³w.\nProblem kolejnoÅ›ci w jÄ™zyku naturalnym jest interesujÄ…cy. W przeciwieÅ„stwie do krokÃ³w szeregu czasowego, sÅ‚owa w zdaniu nie majÄ… naturalnej, kanonicznej kolejnoÅ›ci. RÃ³Å¼ne jÄ™zyki porzÄ…dkujÄ… podobne sÅ‚owa na bardzo rÃ³Å¼ne sposoby. Na przykÅ‚ad, struktura zdaÅ„ w jÄ™zyku angielskim jest zupeÅ‚nie inna niÅ¼ w jÄ™zyku japoÅ„skim. Nawet w obrÄ™bie danego jÄ™zyka moÅ¼na zazwyczaj powiedzieÄ‡ to samo na rÃ³Å¼ne sposoby, zmieniajÄ…c nieco kolejnoÅ›Ä‡ sÅ‚Ã³w. Co wiÄ™cej, jeÅ›li sÅ‚owa w krÃ³tkim zadaniu uÅ‚oÅ¼ysz losowo, to nadal moÅ¼esz w duÅ¼ej mierze dowiedzieÄ‡ siÄ™, co zostaÅ‚o powiedziane, choÄ‡ w wielu przypadkach pojawia siÄ™ dwuznacznoÅ›Ä‡. KolejnoÅ›Ä‡ jest wyraÅºnie waÅ¼na, ale jej zwiÄ…zek ze znaczeniem nie jest prosty.\nSposÃ³b reprezentowania kolejnoÅ›ci sÅ‚Ã³w jest kluczowym pytaniem, z ktÃ³rego wynikajÄ… rÃ³Å¼ne rodzaje architektur NLP. NajprostszÄ… rzeczÄ…, jakÄ… moÅ¼na zrobiÄ‡, jest po prostu odrzucenie kolejnoÅ›ci i traktowanie tekstu jako nieuporzÄ…dkowanego zbioru sÅ‚Ã³w - daje to modele workÃ³w sÅ‚Ã³w. MoÅ¼na rÃ³wnieÅ¼ zdecydowaÄ‡, Å¼e sÅ‚owa powinny byÄ‡ przetwarzane Å›ciÅ›le w kolejnoÅ›ci, w jakiej siÄ™ pojawiajÄ…, pojedynczo, jak kroki w szeregu czasowym - moÅ¼na wtedy wykorzystaÄ‡ modele rekurencyjne z poprzedniego rozdziaÅ‚u. Wreszcie, moÅ¼liwe jest rÃ³wnieÅ¼ podejÅ›cie hybrydowe: architektura transformer jest technicznie niezaleÅ¼na od kolejnoÅ›ci, ale wprowadza informacje o pozycji sÅ‚Ã³w do przetwarzanych reprezentacji, co pozwala jej jednoczeÅ›nie patrzeÄ‡ na rÃ³Å¼ne czÄ™Å›ci zdania (w przeciwieÅ„stwie do RNN), a jednoczeÅ›nie jest Å›wiadoma kolejnoÅ›ci. PoniewaÅ¼ uwzglÄ™dniajÄ… one kolejnoÅ›Ä‡ sÅ‚Ã³w, zarÃ³wno RNN, jak i transformatory nazywane sÄ… modelami sekwencyjnymi.\nHistorycznie rzecz ujmujÄ…c, wiÄ™kszoÅ›Ä‡ wczesnych zastosowaÅ„ uczenia maszynowego w NLP obejmowaÅ‚a po prostu modele bag-of-words. Zainteresowanie modelami sekwencyjnymi zaczÄ™Å‚o rosnÄ…Ä‡ dopiero w 2015 roku, wraz z odrodzeniem siÄ™ rekurencyjnych sieci neuronowych. Obecnie oba podejÅ›cia pozostajÄ… istotne.\n\n7.5.1 Przygotowanie danych IMDB\nZacznijmy od pobrania danych.\n\nKod# Zdefiniowanie URL, z ktÃ³rego ma byÄ‡ pobrany plik\nurl &lt;- \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" \n\nfilename &lt;- paste0(\"/Users/majerek/\", basename(url)) # Pobranie nazwy pliku z URL, w tym przypadku 'aclImdb_v1.tar.gz'\n\noptions(timeout = 60*10) # Ustawienie limitu czasu na pobieranie na 10 minut (60 sekund * 10)\n\ndownload.file(url, destfile = filename) # Pobranie pliku z zadanego URL i zapisanie go pod nazwÄ… \n\nuntar(filename, exdir = \"/Users/majerek/\") # Rozpakowanie pobranego pliku tar.gz\n\n\nTak wyglÄ…da struktura katalogu po rozpakowaniu.\n\nKodfs::dir_tree(\"/Users/majerek/aclImdb\", recurse = 1, type = \"directory\")\n\n/Users/majerek/aclImdb\nâ”œâ”€â”€ test\nâ”‚   â”œâ”€â”€ neg\nâ”‚   â””â”€â”€ pos\nâ”œâ”€â”€ train\nâ”‚   â”œâ”€â”€ neg\nâ”‚   â””â”€â”€ pos\nâ””â”€â”€ val\n    â”œâ”€â”€ neg\n    â””â”€â”€ pos\n\n\nNa przykÅ‚ad katalog train/pos/ zawiera zestaw 12500 plikÃ³w tekstowych, z ktÃ³rych kaÅ¼dy zawiera tekst recenzji filmu o pozytywnym wydÅºwiÄ™ku, ktÃ³ry zostanie wykorzystany jako dane szkoleniowe. Recenzje o negatywnym wydÅºwiÄ™ku znajdujÄ… siÄ™ w katalogach â€œnegâ€. W sumie istnieje 25000 plikÃ³w tekstowych do szkolenia i kolejne 25000 do testowania. Znajduje siÄ™ tam rÃ³wnieÅ¼ podkatalog train/unsup, ktÃ³rego nie potrzebujemy. UsuÅ„my go:\n\nKodfs::dir_delete(\"/Users/majerek/aclImdb/train/unsup/\")\n\n\nSpÃ³jrzmy na zawartoÅ›Ä‡ kilku z tych plikÃ³w tekstowych.\n\nKodwriteLines(readLines(\"/Users/majerek/aclImdb/train/pos/4000_10.txt\", warn = FALSE))\n\nWow. Saw this last night and I'm still reeling from how good it was. Every character felt so real (although most of them petty, selfish a**holes) and the bizarre story - middle aged widow starts shagging her daughter's feckless boyfriend - felt utterly convincing. Top performances all round but hats off to Anne Reid and Our Friends in the North's Daniel Craig (the latter coming across as the next David Thewlis).&lt;br /&gt;&lt;br /&gt;And director Roger Michell? This is as far from Notting Hill as it's possible to be. Thank God.&lt;br /&gt;&lt;br /&gt;Watch this movie!!!\n\n\nNastÄ™pnie przygotujmy zestaw walidacyjny, oddzielajÄ…c 20% treningowych plikÃ³w tekstowych w nowym katalogu, aclImdb/val.\n\nKodlibrary(fs)\nset.seed(1337)\nbase_dir &lt;- path(\"/Users/majerek/aclImdb\")\n\nfor (category in c(\"neg\", \"pos\")) {\n  filepaths &lt;- dir_ls(base_dir / \"train\" / category)\n  num_val_samples &lt;- round(0.2 * length(filepaths))\n  val_files &lt;- sample(filepaths, num_val_samples)\n\n  dir_create(base_dir / \"val\" / category)\n  file_move(val_files,\n            base_dir / \"val\" / category)\n}\n\n\nW zagadnieniach z analizy obrazÃ³w uÅ¼ywaliÅ›my funkcji image_dataset_from_directory() do utworzenia zbiorÃ³w obrazÃ³w i ich etykiet dla struktury katalogÃ³w. DokÅ‚adnie to samo moÅ¼na zrobiÄ‡ dla plikÃ³w tekstowych za pomocÄ… narzÄ™dzia text_dataset_from_directory(). UtwÃ³rzmy trzy obiekty TF Dataset do uczenia, walidacji i testowania:\n\nKodlibrary(keras)\nlibrary(tfdatasets)\n\ntrain_ds &lt;- text_dataset_from_directory(\"/Users/majerek/aclImdb/train\")\n\nFound 20000 files belonging to 2 classes.\n\nKodval_ds &lt;- text_dataset_from_directory(\"/Users/majerek/aclImdb/val\")\n\nFound 5000 files belonging to 2 classes.\n\nKodtest_ds &lt;- text_dataset_from_directory(\"/Users/majerek/aclImdb/test\")\n\nFound 25000 files belonging to 2 classes.\n\n\nTe zestawy danych tworzÄ… dane wejÅ›ciowe, ktÃ³re sÄ… tensorami tf.string i wyjÅ›cia, ktÃ³re sÄ… tensorami int32 kodujÄ…cymi wartoÅ›Ä‡ â€œ0â€ lub â€œ1â€.\n\nKodc(inputs, targets) %&lt;-% iter_next(as_iterator(train_ds))\nstr(inputs)\n\n&lt;tf.Tensor: shape=(32), dtype=string, numpy=â€¦&gt;\n\nKodstr(targets)\n\n&lt;tf.Tensor: shape=(32), dtype=int32, numpy=â€¦&gt;\n\nKodinputs[1]\n\ntf.Tensor(b'I think the movie was pretty good, will add it to my \"clasic collection\" after all this time. I believe I saw other posters who reminded some of the pickier people that it is still just a movie. Maybe some of the more esoteric points defy \"logic\", but a great many religious matters accepted \"on faith\" fail to pass the smell test. If you\\'re going to accept whatever faith you subscribe to you can certainly accept a movie. Is it just me or has anyone else noticed the Aja-Yee Dagger is the same possessed knife Lamonte Cranston had so much trouble gaining control of in \"The Shadow\". No mention of it in the trivia section for either movie here (IMDB), but I would bet a dollar to a donut it\\'s the same prop.', shape=(), dtype=string)\n\nKodtargets[1]\n\ntf.Tensor(1, shape=(), dtype=int32)\n\n\nWszystko gotowe. Teraz sprÃ³bujmy nauczyÄ‡ siÄ™ czegoÅ› z tych danych.\n\n7.5.2 Modelowanie za pomocÄ… bag-of-words\nNajprostszym sposobem zakodowania fragmentu tekstu do przetwarzania przez model uczenia maszynowego jest odrzucenie kolejnoÅ›ci i potraktowanie go jako zbioru (â€œworkaâ€) tokenÃ³w. NajczÄ™Å›ciej stosuje siÄ™ tu unigramy, czyli podziaÅ‚ na pojedyncze sÅ‚owa lub znaki. Zdarza siÄ™ jednak, gdy chcemy zachowaÄ‡ czÄ™Å›Ä‡ informacji o kolejnoÅ›ci sÅ‚Ã³w, Å¼e stosuje siÄ™ N-gramy.\nDla przywoÅ‚ywanego juÅ¼ przykÅ‚adu â€œthe cat sat on the matâ€ podziaÅ‚ na unigramy jest nastÄ™pujÄ…cy:\n\nKodc(\"cat\", \"mat\", \"on\", \"sat\", \"the\")\n\n[1] \"cat\" \"mat\" \"on\"  \"sat\" \"the\"\n\n\nGÅ‚Ã³wnÄ… zaletÄ… tego kodowania jest moÅ¼liwoÅ›Ä‡ reprezentowania caÅ‚ego tekstu jako pojedynczego wektora, gdzie kaÅ¼dy element jest wskaÅºnikiem obecnoÅ›ci danego sÅ‚owa. Na przykÅ‚ad, korzystajÄ…c z kodowania binarnego (ang. multi-hot encoding), zakodujemy tekst jako wektor majÄ…cy tyle wspÃ³Å‚rzÄ™dnych, ile jest sÅ‚Ã³w w naszym sÅ‚owniku, z 0 niemal wszÄ™dzie i kilkoma 1 dla wymiarÃ³w, ktÃ³re kodujÄ… sÅ‚owa obecne w tekÅ›cie.\nNajpierw przetwÃ³rzmy nasze surowe zbiory danych tekstowych za pomocÄ… warstwy layer_text_vectorization(), tak aby uzyskaÄ‡ wielokrotnie zakodowane binarnie wektory sÅ‚Ã³w. Nasza warstwa bÄ™dzie patrzeÄ‡ tylko na pojedyncze sÅ‚owa (czyli unigramy).\n\nKodtext_vectorization &lt;-\n  layer_text_vectorization(max_tokens = 20000,\n                           output_mode = \"multi_hot\")\n\ntext_only_train_ds &lt;- train_ds %&gt;%\n  dataset_map(function(x, y) x)\n\nadapt(text_vectorization, text_only_train_ds)\n\nbinary_1gram_train_ds &lt;- train_ds %&gt;%\n  dataset_map( ~ list(text_vectorization(.x), .y),\n               num_parallel_calls = 4)\nbinary_1gram_val_ds &lt;- val_ds %&gt;%\n  dataset_map( ~ list(text_vectorization(.x), .y),\n               num_parallel_calls = 4)\nbinary_1gram_test_ds &lt;- test_ds %&gt;%\n  dataset_map( ~ list(text_vectorization(.x), .y),\n               num_parallel_calls = 4)\n\n\nSprawdÅºmy dane wyjÅ›ciowe dla jednego z tych zestawÃ³w.\n\nKodc(inputs, targets) %&lt;-% iter_next(as_iterator(binary_1gram_train_ds))\nstr(inputs)\n\n&lt;tf.Tensor: shape=(32, 20000), dtype=float32, numpy=â€¦&gt;\n\nKodstr(targets)\n\n&lt;tf.Tensor: shape=(32), dtype=int32, numpy=â€¦&gt;\n\nKodinputs[1, ]\n\ntf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000), dtype=float32)\n\nKodtargets[1]\n\ntf.Tensor(1, shape=(), dtype=int32)\n\n\nNapiszmy funkcjÄ™ budowania modelu wielokrotnego uÅ¼ytku, ktÃ³rej bÄ™dziemy uÅ¼ywaÄ‡ we wszystkich naszych eksperymentach w tej sekcji.\n\nKodget_model &lt;- function(max_tokens = 20000, hidden_dim = 16) {\n  inputs &lt;- layer_input(shape = c(max_tokens))\n  outputs &lt;- inputs %&gt;%\n    layer_dense(hidden_dim, activation = \"relu\") %&gt;%\n    layer_dropout(0.5) %&gt;%\n    layer_dense(1, activation = \"sigmoid\")\n  model &lt;- keras_model(inputs, outputs)\n  model %&gt;% compile(optimizer = \"rmsprop\",\n                    loss = \"binary_crossentropy\",\n                    metrics = \"accuracy\")\n  model\n}\n\n\nNa koniec wytrenujmy i przetestujmy nasz model.\n\nKodmodel &lt;- get_model()\nmodel\ncallbacks = list(\n  callback_model_checkpoint(\"models/binary_1gram\", save_best_only = TRUE)\n)\n\nmodel %&gt;% fit(\n  dataset_cache(binary_1gram_train_ds),\n  validation_data = dataset_cache(binary_1gram_val_ds),\n  epochs = 10,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/binary_1gram\")\ncat(sprintf(\n  \"Test acc: %.3f\\n\", evaluate(model, binary_1gram_test_ds)[\"accuracy\"]))\n\n782/782 - 3s - loss: 0.2856 - accuracy: 0.8868 - 3s/epoch - 3ms/step\nTest acc: 0.887\n\n\nDokÅ‚adnoÅ›Ä‡ modelu na zbiorze testowym jest na poziomie 88,7%: nieÅºle! NaleÅ¼y zauwaÅ¼yÄ‡, Å¼e w tym przypadku, poniewaÅ¼ zbiÃ³r danych jest zrÃ³wnowaÅ¼onym dwuklasowym zbiorem danych klasyfikacyjnych (jest tyle samo prÃ³bek pozytywnych, co negatywnych), â€œnaiwny poziom bazowyâ€, ktÃ³ry moglibyÅ›my osiÄ…gnÄ…Ä‡ bez trenowania rzeczywistego modelu, wynosiÅ‚by tylko 50%. Tymczasem najlepszy wynik, jaki moÅ¼na osiÄ…gnÄ…Ä‡ na tym zbiorze danych bez wykorzystywania danych zewnÄ™trznych, wynosi okoÅ‚o 95% dokÅ‚adnoÅ›ci testu.\nOczywiÅ›cie odrzucenie kolejnoÅ›ci sÅ‚Ã³w jest bardzo redukcyjne, poniewaÅ¼ nawet pojÄ™cia elementarne moÅ¼na wyraziÄ‡ za pomocÄ… wielu sÅ‚Ã³w: termin â€œStany Zjednoczoneâ€ przekazuje pojÄ™cie, ktÃ³re jest zupeÅ‚nie odmienne od znaczenia sÅ‚Ã³w â€œstanyâ€ i â€œzjednoczoneâ€ rozpatrywanych osobno. Z tego powodu zwykle koÅ„czy siÄ™ to ponownym wprowadzeniem informacji o lokalnym porzÄ…dku do reprezentacji worka sÅ‚Ã³w, patrzÄ…c na N-gramy, a nie na pojedyncze sÅ‚owa (najczÄ™Å›ciej bigramy).\nWarstwÄ™ layer_text_vectorization() moÅ¼na skonfigurowaÄ‡ tak, aby zwracaÅ‚a dowolne N-gramy: bigramy, trygramy itd. Wystarczy przekazaÄ‡ argument ngrams = N, jak na poniÅ¼szym listingu.\n\nKodtext_vectorization &lt;-\n  layer_text_vectorization(ngrams = 2,\n                           max_tokens = 20000,\n                           output_mode = \"multi_hot\")\n\n\nPrzetestujmy nasz model oparty na bigramach.\n\nKodadapt(text_vectorization, text_only_train_ds)\n\ndataset_vectorize &lt;- function(dataset) {\n  dataset %&gt;%\n    dataset_map(~ list(text_vectorization(.x), .y),\n                num_parallel_calls = 4)\n}\n\nbinary_2gram_train_ds &lt;- train_ds %&gt;% dataset_vectorize()\nbinary_2gram_val_ds &lt;- val_ds %&gt;% dataset_vectorize()\nbinary_2gram_test_ds &lt;- test_ds %&gt;% dataset_vectorize()\n\n\n\nKodmodel &lt;- get_model()\nmodel\ncallbacks = list(callback_model_checkpoint(\"models/binary_2gram\",\n                                           save_best_only = TRUE))\n\nmodel %&gt;% fit(\n  dataset_cache(binary_2gram_train_ds),\n  validation_data = dataset_cache(binary_2gram_val_ds),\n  epochs = 10,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/binary_2gram\")\nevaluate(model, binary_2gram_test_ds)[\"accuracy\"] %&gt;%\n  sprintf(\"Test acc: %.3f\\n\", .) %&gt;% cat()\n\n782/782 - 3s - loss: 0.2556 - accuracy: 0.9012 - 3s/epoch - 4ms/step\nTest acc: 0.901\n\n\nUzyskujemy teraz 90,1% dokÅ‚adnoÅ›ci testu, co stanowi znacznÄ… poprawÄ™! Okazuje siÄ™, Å¼e lokalna kolejnoÅ›Ä‡ jest doÅ›Ä‡ waÅ¼na.\nMoÅ¼na rÃ³wnieÅ¼ dodaÄ‡ nieco wiÄ™cej informacji do tej reprezentacji, liczÄ…c, ile razy wystÄ™puje kaÅ¼de sÅ‚owo lub N-gram, czyli biorÄ…c histogram sÅ‚Ã³w w tekÅ›cie:\n\nKodc(\"the\" = 2, \"the cat\" = 1, \"cat\" = 1, \"cat sat\" = 1, \"sat\" = 1,\n  \"sat on\" = 1, \"on\" = 1, \"on the\" = 1, \"the mat\" = 1, \"mat\" = 1)\n\n    the the cat     cat cat sat     sat  sat on      on  on the the mat     mat \n      2       1       1       1       1       1       1       1       1       1 \n\n\nJeÅ›li przeprowadzamy klasyfikacjÄ™ tekstu, wiedza o tym, ile razy sÅ‚owo wystÄ™puje w prÃ³bce, ma kluczowe znaczenie: kaÅ¼da wystarczajÄ…co dÅ‚uga recenzja filmu moÅ¼e zawieraÄ‡ sÅ‚owo â€œokropnyâ€ niezaleÅ¼nie od nastroju, ale recenzja zawierajÄ…ca wiele wystÄ…pieÅ„ sÅ‚owa â€œokropnyâ€ jest prawdopodobnie negatywna. Oto jak policzyÄ‡ wystÄ…pienia bigramÃ³w za pomocÄ… layer_text_vectorization():\n\nKodtext_vectorization &lt;-\n  layer_text_vectorization(ngrams = 2,\n                           max_tokens = 20000,\n                           output_mode = \"count\")\n\n\nOczywiÅ›cie niektÃ³re sÅ‚owa bÄ™dÄ… wystÄ™powaÄ‡ czÄ™Å›ciej niÅ¼ inne, niezaleÅ¼nie od tego, o czym jest tekst. SÅ‚owa â€œtheâ€, â€œaâ€, â€œisâ€ i â€œareâ€ zawsze bÄ™dÄ… dominowaÄ‡ w histogramach liczby sÅ‚Ã³w, zagÅ‚uszajÄ…c inne sÅ‚owa, mimo Å¼e sÄ… w zasadzie bezuÅ¼ytecznymi cechami w kontekÅ›cie klasyfikacji. Jak moÅ¼emy temu zaradziÄ‡?\nPoprzez normalizacjÄ™. MoglibyÅ›my po prostu znormalizowaÄ‡ liczbÄ™ sÅ‚Ã³w, odejmujÄ…c Å›redniÄ… i dzielÄ…c przez wariancjÄ™ (obliczonÄ… dla caÅ‚ego zbioru danych szkoleniowych). To miaÅ‚oby sens. Z wyjÄ…tkiem tego, Å¼e wiÄ™kszoÅ›Ä‡ wektoryzowanych zdaÅ„ skÅ‚ada siÄ™ prawie wyÅ‚Ä…cznie z zer (nasz poprzedni przykÅ‚ad zawiera 12 niezerowych wpisÃ³w i 19 988 zerowych wpisÃ³w), co jest wÅ‚aÅ›ciwoÅ›ciÄ… zwanÄ… â€œrzadkoÅ›ciÄ…â€. Jest to Å›wietna wÅ‚aÅ›ciwoÅ›Ä‡, poniewaÅ¼ znacznie zmniejsza obciÄ…Å¼enie obliczeniowe i zmniejsza ryzyko przeuczenia. GdybyÅ›my odjÄ™li Å›redniÄ… od kaÅ¼dej cechy, â€œzniszczylibyÅ›myâ€ rzadkoÅ›Ä‡. W zwiÄ…zku z tym kaÅ¼dy schemat normalizacji, ktÃ³rego uÅ¼ywamy, powinien byÄ‡ oparty wyÅ‚Ä…cznie na dzieleniu. Czego zatem powinniÅ›my uÅ¼yÄ‡ jako mianownika? NajlepszÄ… praktykÄ… jest zastosowanie czegoÅ›, co nazywa siÄ™ normalizacjÄ… TF-IDF - co oznacza â€œczÄ™stotliwoÅ›Ä‡ terminÃ³w, odwrotnoÅ›Ä‡ czÄ™stotliwoÅ›ci dokumentÃ³wâ€.\n\n\n\n\n\n\nAdnotacja\n\n\n\nIm czÄ™Å›ciej dany termin pojawia siÄ™ w dokumencie, tym waÅ¼niejszy jest on dla zrozumienia jego treÅ›ci. JednoczeÅ›nie czÄ™stotliwoÅ›Ä‡, z jakÄ… termin pojawia siÄ™ we wszystkich dokumentach w zbiorze danych, rÃ³wnieÅ¼ ma znaczenie: terminy, ktÃ³re pojawiajÄ… siÄ™ w prawie kaÅ¼dym dokumencie (takie jak â€œtheâ€ lub â€œaâ€) nie sÄ… szczegÃ³lnie pouczajÄ…ce, podczas gdy terminy, ktÃ³re pojawiajÄ… siÄ™ tylko w niewielkim podzbiorze wszystkich tekstÃ³w (takich jak â€œHerzogâ€) sÄ… bardzo charakterystyczne, a zatem waÅ¼ne. TF-IDF to metryka, ktÃ³ra Å‚Ä…czy te dwie koncepcje. WaÅ¼y dany termin, biorÄ…c â€œczÄ™stotliwoÅ›Ä‡ terminuâ€, ile razy termin pojawia siÄ™ w bieÅ¼Ä…cym dokumencie i dzielÄ…c go przez miarÄ™ â€œczÄ™stotliwoÅ›ci dokumentuâ€, ktÃ³ra szacuje, jak czÄ™sto termin pojawia siÄ™ w caÅ‚ym zbiorze danych. MoÅ¼na to obliczyÄ‡ w nastÄ™pujÄ…cy sposÃ³b:\n\nKodtf_idf &lt;- function(term, document, dataset) {\n  term_freq &lt;- sum(document == term)\n  doc_freqs &lt;- sapply(dataset, function(doc) sum(doc == term))\n  doc_freq &lt;- log(1 + sum(doc_freqs))\n  term_freq / doc_freq\n}\n\n\nTF-IDF jest tak powszechny, Å¼e jest wbudowany w funkcjÄ™ layer_text_vectorization(). Wszystko, co musimy zrobiÄ‡, aby zaczÄ…Ä‡ go uÅ¼ywaÄ‡, to przeÅ‚Ä…czyÄ‡ argument output_mode na â€œtf_idfâ€.\n\nKodtext_vectorization &lt;-\n  layer_text_vectorization(ngrams = 2,\n                           max_tokens = 20000,\n                           output_mode = \"tf_idf\")\n\n\n\n\nPrzetrenujmy model z tym schematem.\n\nKod# WysyÅ‚amy tÄ™ operacjÄ™ tylko do CPU, poniewaÅ¼ wykorzystuje ona operacje, ktÃ³rych urzÄ…dzenie GPU jeszcze nie obsÅ‚uguje.\nwith(tf$device(\"CPU\"), {\n  adapt(text_vectorization, text_only_train_ds)\n})\n\ntfidf_2gram_train_ds &lt;- train_ds %&gt;% dataset_vectorize()\ntfidf_2gram_val_ds &lt;- val_ds %&gt;% dataset_vectorize()\ntfidf_2gram_test_ds &lt;- test_ds %&gt;% dataset_vectorize()\n\n\n\nKodmodel &lt;- get_model()\nmodel\ncallbacks &lt;- list(callback_model_checkpoint(\"models/tfidf_2gram\",\n                                            save_best_only = TRUE))\nmodel %&gt;% fit(\n  dataset_cache(tfidf_2gram_train_ds),\n  validation_data = dataset_cache(tfidf_2gram_val_ds),\n  epochs = 10,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/tfidf_2gram\")\nevaluate(model, tfidf_2gram_test_ds)[\"accuracy\"] %&gt;%\n  sprintf(\"Test acc: %.3f\", .) %&gt;% cat(\"\\n\")\n\n782/782 - 3s - loss: 0.2973 - accuracy: 0.8940 - 3s/epoch - 4ms/step\nTest acc: 0.894 \n\n\nDaje nam to 89,4% dokÅ‚adnoÅ›ci testowej w zadaniu klasyfikacji IMDB: nie wydaje siÄ™ to byÄ‡ szczegÃ³lnie pomocne w tym przypadku. Jednak w przypadku wielu zestawÃ³w danych do klasyfikacji tekstu typowy byÅ‚by jednoprocentowy wzrost przy uÅ¼yciu TF-IDF w porÃ³wnaniu do zwykÅ‚ego kodowania binarnego.\n\n\n\n\n\n\nAdnotacja\n\n\n\nW poprzednich przykÅ‚adach przeprowadziliÅ›my standaryzacjÄ™, podziaÅ‚ i indeksowanie tekstu w ramach potoku TF Dataset. JeÅ›li jednak chcemy wyeksportowaÄ‡ samodzielny model niezaleÅ¼ny od tego potoku, powinniÅ›my upewniÄ‡ siÄ™, Å¼e zawiera on wÅ‚asne wstÄ™pne przetwarzanie tekstu (w przeciwnym razie konieczne bÄ™dzie ponowne wdroÅ¼enie w Å›rodowisku produkcyjnym, co moÅ¼e byÄ‡ trudne lub moÅ¼e prowadziÄ‡ do subtelnych rozbieÅ¼noÅ›ci miÄ™dzy danymi szkoleniowymi a danymi produkcyjnymi). Na szczÄ™Å›cie jest to Å‚atwe.\nWystarczy utworzyÄ‡ nowy model, ktÃ³ry ponownie wykorzysta warstwÄ™ text_vectorization i doda do niej wÅ‚aÅ›nie wytrenowany model:\n\nKodinputs &lt;- layer_input(shape = c(1), dtype = \"string\")\noutputs &lt;- inputs %&gt;%\n  text_vectorization() %&gt;%\n  model()\ninference_model &lt;- keras_model(inputs, outputs)\n\n\nWynikowy model moÅ¼e przetwarzaÄ‡ partie nieprzetworzonych ciÄ…gÃ³w znakÃ³w:\n\nKodraw_text_data &lt;- \"That was an excellent movie, I loved it.\" %&gt;%\n  as_tensor(shape = c(-1, 1))\n\npredictions &lt;- inference_model(raw_text_data)\nstr(predictions)\n\n&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8422111]], dtype=float32)&gt;\n\nKodcat(sprintf(\"%.2f percent positive\\n\",\n            as.numeric(predictions) * 100))\n\n84.22 percent positive\n\n\n\n\n\n7.5.3 Modelowanie za pomocÄ… sekwencji\nTe kilka ostatnich przykÅ‚adÃ³w wyraÅºnie pokazuje, Å¼e kolejnoÅ›Ä‡ sÅ‚Ã³w ma znaczenie: rÄ™czna inÅ¼ynieria funkcji opartych na kolejnoÅ›ci, takich jak bigramy, zapewnia niewielki wzrost dokÅ‚adnoÅ›ci. ZapamiÄ™tajmy - historia gÅ‚Ä™bokiego uczenia polega na odejÅ›ciu od rÄ™cznej inÅ¼ynierii cech, w kierunku umoÅ¼liwienia modelom uczenia siÄ™ wÅ‚asnych cech na podstawie samej ekspozycji na dane. Co by byÅ‚o, gdybyÅ›my zamiast rÄ™cznie tworzyÄ‡ funkcje oparte na kolejnoÅ›ci, wystawili model na surowe sekwencje sÅ‚Ã³w i pozwolili mu samodzielnie wymyÅ›liÄ‡ takie funkcje? O to wÅ‚aÅ›nie chodzi w modelach opartych na sekwencji.\nAby zaimplementowaÄ‡ model sekwencji, naleÅ¼y zaczÄ…Ä‡ od reprezentowania prÃ³bek wejÅ›ciowych jako sekwencji indeksÃ³w liczb caÅ‚kowitych (jedna liczba caÅ‚kowita oznacza jedno sÅ‚owo). NastÄ™pnie mapujemy kaÅ¼dÄ… liczbÄ™ caÅ‚kowitÄ… na wektor, aby uzyskaÄ‡ sekwencje wektorowe. Wreszcie, te sekwencje wektorÃ³w naleÅ¼y wprowadziÄ‡ do stosu warstw, ktÃ³re mogÄ… korelowaÄ‡ krzyÅ¼owo cechy z sÄ…siednich wektorÃ³w, takich jak konwolucje 1D, RNN lub Transformery.\nPrzez pewien czas, okoÅ‚o 2016-2017 roku, dwukierunkowe RNN (w szczegÃ³lnoÅ›ci dwukierunkowe LSTM) byÅ‚y uwaÅ¼ane za najnowoczeÅ›niejsze rozwiÄ…zanie do modelowania sekwencji. PoniewaÅ¼ jesteÅ›my juÅ¼ zaznajomieni z tÄ… architekturÄ…, to wÅ‚aÅ›nie jej uÅ¼yjemy w naszych pierwszych przykÅ‚adach modeli sekwencji. Jednak obecnie modelowanie sekwencji jest prawie zawsze wykonywane za pomocÄ… transformatorÃ³w, ktÃ³re omÃ³wimy wkrÃ³tce.\nWyprÃ³bujmy pierwszy model sekwencji w praktyce. Najpierw przygotujmy zestawy danych zwracajÄ…ce sekwencje liczb caÅ‚kowitych.\n\nKodmax_length &lt;- 600\nmax_tokens &lt;- 20000\n\ntext_vectorization &lt;- layer_text_vectorization(\n  max_tokens = max_tokens,\n  output_mode = \"int\",\n  output_sequence_length = max_length\n)\n\nadapt(text_vectorization, text_only_train_ds)\n\nint_train_ds &lt;- train_ds %&gt;% dataset_vectorize()\nint_val_ds &lt;- val_ds %&gt;% dataset_vectorize()\nint_test_ds &lt;- test_ds %&gt;% dataset_vectorize()\n\n\nNastÄ™pnie stwÃ³rzmy model. Najprostszym sposobem konwersji naszych sekwencji liczb caÅ‚kowitych na sekwencje wektorowe jest zakodowanie liczb caÅ‚kowitych metodÄ… one-hot (kaÅ¼dy wymiar reprezentowaÅ‚by jeden moÅ¼liwy termin w sÅ‚owniku). Na tych wektorach one-hot dodamy prostÄ… dwukierunkowÄ… sieÄ‡ LSTM.\n\nKodinputs  &lt;- layer_input(shape(NULL), dtype = \"int64\")\nembedded &lt;- tf$one_hot(inputs, depth = as.integer(max_tokens))\noutputs &lt;- embedded %&gt;%\n  bidirectional(layer_lstm(units = 32)) %&gt;%\n  layer_dropout(.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel &lt;- keras_model(inputs, outputs)\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"binary_crossentropy\",\n                  metrics = \"accuracy\")\nmodel\n\nModel: \"model_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_2 (InputLayer)               [(None, None)]                  0           \n tf.one_hot (TFOpLambda)            (None, None, 20000)             0           \n bidirectional (Bidirectional)      (None, 64)                      5128448     \n dropout (Dropout)                  (None, 64)                      0           \n dense (Dense)                      (None, 1)                       65          \n================================================================================\nTotal params: 5128513 (19.56 MB)\nTrainable params: 5128513 (19.56 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks &lt;- list(\n  callback_model_checkpoint(\"models/one_hot_bidir_lstm.keras\",\n                            save_best_only = TRUE))\n\n\nPodczas uczenia modelu moÅ¼na poczyniÄ‡ dwie obserwacje. Po pierwsze, model ten trenuje siÄ™ bardzo wolno, szczegÃ³lnie w porÃ³wnaniu do modeli z poprzedniego podrozdziaÅ‚u. Wynika to z faktu, Å¼e nasze dane wejÅ›ciowe sÄ… doÅ›Ä‡ duÅ¼e: kaÅ¼da prÃ³bka wejÅ›ciowa jest zakodowana jako macierz o rozmiarze (600, 20000) (600 sÅ‚Ã³w na prÃ³bkÄ™, 20 000 moÅ¼liwych sÅ‚Ã³w). To 12 000 000 wartoÅ›ci dla pojedynczej recenzji filmu. Po drugie, model osiÄ…ga tylko 84,7% dokÅ‚adnoÅ›ci testowej, wiÄ™c nie radzi sobie tak dobrze, jak nasz najlepszy model.\n\nKod# aby nie przekroczyÄ‡ zasobÃ³w pamiÄ™ci GPU zmnieszamy wielkoÅ›Ä‡ paczek do 16\nint_train_ds_smaller &lt;- int_train_ds %&gt;%\n  dataset_unbatch() %&gt;%\n  dataset_batch(16)\n\nmodel %&gt;% fit(int_train_ds_smaller, validation_data = int_val_ds,\n              epochs = 10, callbacks = callbacks)\n\n\n\nKod# predykcja trwa dÅ‚ugo dlatego nie pozwalam na wywoÅ‚anie tego chunk-a\nmodel &lt;- load_model_tf(\"models/one_hot_bidir_lstm.keras\")\nsprintf(\"Test acc: %.3f\", evaluate(model, int_test_ds)[\"accuracy\"])\n\n\nNajwyraÅºniej uÅ¼ycie kodowania one-hot do przeksztaÅ‚cenia sÅ‚Ã³w w wektory, nie byÅ‚o dobrym pomysÅ‚em. Jest lepszy sposÃ³b - osadzanie sÅ‚Ã³w.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#osadzenia",
    "href": "llm.html#osadzenia",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.6 Osadzenia",
    "text": "7.6 Osadzenia\nKiedy kodujemy coÅ› za pomocÄ… kodowania one-hot, podejmujemy decyzjÄ™ o inÅ¼ynierii cech. Wprowadzamy do naszego modelu fundamentalne zaÅ‚oÅ¼enie dotyczÄ…ce struktury przestrzeni cech. ZaÅ‚oÅ¼eniem tym jest to, Å¼e rÃ³Å¼ne tokeny, ktÃ³re kodujemy, sÄ… od siebie niezaleÅ¼ne: w rzeczywistoÅ›ci wektory one-hot sÄ… wzglÄ™dem siebie ortogonalne. W przypadku sÅ‚Ã³w zaÅ‚oÅ¼enie to jest oczywiÅ›cie bÅ‚Ä™dne. SÅ‚owa tworzÄ… ustrukturyzowanÄ… przestrzeÅ„: dzielÄ… siÄ™ ze sobÄ… informacjami. SÅ‚owa â€œmovieâ€ i â€œfilmâ€ sÄ… wymienne w wiÄ™kszoÅ›ci zdaÅ„, wiÄ™c wektor reprezentujÄ…cy â€œmovieâ€ nie powinien byÄ‡ ortogonalny do wektora reprezentujÄ…cego â€œfilmâ€ - powinny byÄ‡ tym samym wektorem lub bardzo zbliÅ¼onym.\nPodchodzÄ…c do sprawy niecobardziej abstrakcyjnie, geometryczna relacja miÄ™dzy dwoma wektorami sÅ‚Ã³w powinna odzwierciedlaÄ‡ semantycznÄ… relacjÄ™ miÄ™dzy tymi sÅ‚owami. Na przykÅ‚ad, w rozsÄ…dnej przestrzeni wektorÃ³w sÅ‚Ã³w moÅ¼na oczekiwaÄ‡, Å¼e synonimy bÄ™dÄ… osadzone w podobnych wektorach sÅ‚Ã³w, a ogÃ³lnie rzecz biorÄ…c, moÅ¼na oczekiwaÄ‡, Å¼e odlegÅ‚oÅ›Ä‡ geometryczna (taka jak odlegÅ‚oÅ›Ä‡ cosinusowa lub odlegÅ‚oÅ›Ä‡ L2) miÄ™dzy dowolnymi dwoma wektorami sÅ‚Ã³w bÄ™dzie odnosiÄ‡ siÄ™ do â€œodlegÅ‚oÅ›ci semantycznejâ€ miÄ™dzy powiÄ…zanymi sÅ‚owami. SÅ‚owa, ktÃ³re oznaczajÄ… rÃ³Å¼ne pojÄ™cia, powinny znajdowaÄ‡ siÄ™ daleko od siebie, podczas gdy podobne sÅ‚owa powinny znajdowaÄ‡ siÄ™ bliÅ¼ej.\nOsadzenia sÅ‚Ã³w to wektorowe reprezentacje sÅ‚Ã³w, ktÃ³re osiÄ…gajÄ… ten cel: mapujÄ… ludzki jÄ™zyk na ustrukturyzowanÄ… przestrzeÅ„ geometrycznÄ…. Podczas gdy wektory uzyskane w wyniku kodowania one-hot sÄ… binarne, rzadkie (w wiÄ™kszoÅ›ci skÅ‚adajÄ… siÄ™ z zer) i wielowymiarowe (taka sama wymiarowoÅ›Ä‡ jak liczba sÅ‚Ã³w w sÅ‚owniku), osadzenia sÅ‚Ã³w sÄ… niskowymiarowymi wektorami typu float (tj. gÄ™stymi wektorami, w przeciwieÅ„stwie do rzadkich wektorÃ³w) - patrz Rys.Â 7.2. W przypadku bardzo duÅ¼ych sÅ‚ownikÃ³w czÄ™sto spotyka siÄ™ osadzenia sÅ‚Ã³w, ktÃ³re sÄ… 256-wymiarowe, 512-wymiarowe lub 1024-wymiarowe. Z drugiej strony, kodowanie sÅ‚Ã³w metodÄ… one-hot zazwyczaj prowadzi do wektorÃ³w, ktÃ³re majÄ… 20 000 wymiarÃ³w lub wiÄ™cej (sÅ‚ownik skÅ‚adajÄ…cy siÄ™ z 20,000 tokenÃ³w). Tak wiÄ™c osadzanie sÅ‚Ã³w zawiera wiÄ™cej informacji w znacznie mniejszej liczbie wymiarÃ³w.\n\n\n\n\n\nRys.Â 7.2\n\n\nOprÃ³cz tego, Å¼e sÄ… gÄ™stymi reprezentacjami, osadzenia sÅ‚Ã³w sÄ… rÃ³wnieÅ¼ reprezentacjami strukturalnymi, a ich struktura jest uczona na podstawie danych. Podobne sÅ‚owa sÄ… osadzone w bliskich lokalizacjach, a ponadto okreÅ›lone kierunki w przestrzeni osadzania majÄ… znaczenie. Aby to wyjaÅ›niÄ‡, spÃ³jrzmy na konkretny przykÅ‚ad.\n\n\n\n\n\nRys.Â 7.3\n\n\nNa Rys.Â 7.3 cztery sÅ‚owa sÄ… osadzone na pÅ‚aszczyÅºnie 2D: cat, dog, wolf i tiger. DziÄ™ki reprezentacjom wektorowym, ktÃ³re tutaj wybraliÅ›my, niektÃ³re relacje semantyczne miÄ™dzy tymi sÅ‚owami moÅ¼na zakodowaÄ‡ jako transformacje geometryczne. Na przykÅ‚ad ten sam wektor pozwala nam przejÅ›Ä‡ od kota do tygrysa i od psa do wilka: wektor ten moÅ¼na interpretowaÄ‡ jako wektor â€œod zwierzÄ™cia domowego do dzikiego zwierzÄ™ciaâ€. Podobnie, inny wektor pozwala nam przejÅ›Ä‡ od psa do kota i od wilka do tygrysa, co moÅ¼na interpretowaÄ‡ jako wektor â€œod psa do kotaâ€.\nW rzeczywistych przestrzeniach osadzania sÅ‚Ã³w, typowymi przykÅ‚adami znaczÄ…cych transformacji geometrycznych sÄ… wektory â€œpÅ‚ciâ€ i wektory â€œliczby mnogiejâ€. Na przykÅ‚ad, dodajÄ…c wektor â€œÅ¼eÅ„skiâ€ do wektora â€œkrÃ³lâ€, otrzymujemy wektor â€œkrÃ³lowaâ€. DodajÄ…c wektor â€œliczby mnogiejâ€, otrzymujemy â€œkrÃ³lÃ³wâ€. Przestrzenie osadzania sÅ‚Ã³w zazwyczaj zawierajÄ… tysiÄ…ce takich interpretowalnych i potencjalnie uÅ¼ytecznych wektorÃ³w.\nJeszcze inny przykÅ‚ad osadzenia sÅ‚Ã³w moÅ¼na dostrzec na Rys.Â 7.4\n\n\n\n\n\nRys.Â 7.4: PrzykÅ‚ad osadzenia sÅ‚Ã³w\n\n\nIstniejÄ… dwa sposoby na uzyskanie osadzenia sÅ‚Ã³w:\n\nUczenie siÄ™ embeddingÃ³w wspÃ³lnie z gÅ‚Ã³wnym zadaniem (takim jak klasyfikacja dokumentÃ³w lub przewidywanie sentymentu). W tej konfiguracji zaczynamy od losowych wektorÃ³w sÅ‚Ã³w, a nastÄ™pnie uczymy siÄ™ wektorÃ³w sÅ‚Ã³w w taki sam sposÃ³b, w jaki uczymy siÄ™ wag sieci neuronowej.\nWczytanie do modelu osadzenia sÅ‚Ã³w, ktÃ³re zostaÅ‚y wstÄ™pnie wytrenowane przy uÅ¼yciu innego zadania uczenia maszynowego niÅ¼ to, ktÃ³re prÃ³bujemy rozwiÄ…zaÄ‡. SÄ… to tzw. wstÄ™pnie wytrenowane osadzenia sÅ‚Ã³w.\n\nPrzyjrzyjmy siÄ™ obu tym metodom.\n\n7.6.1 Osadzenie poÅ‚Ä…czone z sieciÄ…\nCzy istnieje jakaÅ› idealna przestrzeÅ„ sÅ‚owotwÃ³rcza, ktÃ³ra doskonale odwzorowywaÅ‚aby ludzki jÄ™zyk i mogÅ‚aby byÄ‡ wykorzystana do kaÅ¼dego zadania zwiÄ…zanego z przetwarzaniem jÄ™zyka naturalnego? MoÅ¼liwe, ale nie udaÅ‚o nam siÄ™ jeszcze takiej znaleÅºÄ‡. Nie ma teÅ¼ czegoÅ› takiego jak jÄ™zyk ludzki - istnieje wiele rÃ³Å¼nych jÄ™zykÃ³w i nie sÄ… one izomorficzne, poniewaÅ¼ jÄ™zyk jest odzwierciedleniem konkretnej kultury i konkretnego kontekstu. Bardziej pragmatycznie moÅ¼emy stwierdziÄ‡, Å¼e to co czyni przestrzeÅ„ osadzania sÅ‚Ã³w dobrÄ…, zaleÅ¼y w duÅ¼ej mierze od zadania: idealna przestrzeÅ„ osadzania sÅ‚Ã³w dla anglojÄ™zycznego modelu analizy sentymentÃ³w w recenzji filmowej moÅ¼e wyglÄ…daÄ‡ inaczej niÅ¼ idealna przestrzeÅ„ osadzania dla anglojÄ™zycznego modelu klasyfikacji dokumentÃ³w prawnych, poniewaÅ¼ znaczenie pewnych relacji semantycznych rÃ³Å¼ni siÄ™ w zaleÅ¼noÅ›ci od zadania.\nDlatego rozsÄ…dne jest uczenie siÄ™ nowej przestrzeni osadzania z kaÅ¼dym nowym zadaniem. Na szczÄ™Å›cie wsteczna propagacja to uÅ‚atwia, a pakiet keras czyni to jeszcze Å‚atwiejszym. Chodzi o uczenie wag warstwy za pomocÄ… layer_embedding.\n\nKodembedding_layer &lt;- layer_embedding(input_dim = max_tokens, output_dim = 256)\n\n\nFunkcja layer_embedding() jest najlepiej rozumiana jako sÅ‚ownik, ktÃ³ry mapuje indeksy liczb caÅ‚kowitych (ktÃ³re oznaczajÄ… okreÅ›lone sÅ‚owa) na wektory gÄ™ste. Przyjmuje liczby caÅ‚kowite jako dane wejÅ›ciowe, wyszukuje tych liczb caÅ‚kowitych w wewnÄ™trznym sÅ‚owniku i zwraca powiÄ…zane wektory.\nWarstwa osadzania przyjmuje jako dane wejÅ›ciowe tensor liczb caÅ‚kowitych, o ksztaÅ‚cie (batch_size, sequence_length), gdzie kaÅ¼dy wpis jest sekwencjÄ… liczb caÅ‚kowitych. NastÄ™pnie warstwa zwraca tensor zmiennoprzecinkowy 3D o ksztaÅ‚cie (batch_size, sequence_length, embedding_dimensionality).\nPo utworzeniu instancji layer_embedding(), jej wagi (wewnÄ™trzny sÅ‚ownik wektorÃ³w tokenÃ³w) sÄ… poczÄ…tkowo losowe, tak jak w przypadku kaÅ¼dej innej warstwy. Podczas uczenia te wektory sÅ‚Ã³w sÄ… stopniowo dostosowywane za pomocÄ… wstecznej propagacji, strukturyzujÄ…c przestrzeÅ„ w coÅ›, co moÅ¼e wykorzystaÄ‡ dalszy model. Po peÅ‚nym wytrenowaniu, przestrzeÅ„ osadzania bÄ™dzie wykazywaÄ‡ duÅ¼Ä… strukturÄ™ - rodzaj struktury wyspecjalizowanej dla konkretnego problemu, dla ktÃ³rego trenujemy nasz model.\nZbudujmy model zawierajÄ…cy funkcjÄ™ layer_embedding() i przetestujmy go w naszym zadaniu.\n\nKodinputs &lt;- layer_input(shape(NA), dtype = \"int64\")\nembedded &lt;- inputs %&gt;%\n  layer_embedding(input_dim = max_tokens, output_dim = 256)\noutputs &lt;- embedded %&gt;%\n  bidirectional(layer_lstm(units = 32)) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\nmodel &lt;- keras_model(inputs, outputs)\nmodel %&gt;%\n  compile(optimizer = \"rmsprop\",\n          loss = \"binary_crossentropy\",\n          metrics = \"accuracy\")\nmodel\n\nModel: \"model_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_3 (InputLayer)               [(None, None)]                  0           \n embedding_1 (Embedding)            (None, None, 256)               5120000     \n bidirectional_1 (Bidirectional)    (None, 64)                      73984       \n dropout_1 (Dropout)                (None, 64)                      0           \n dense_1 (Dense)                    (None, 1)                       65          \n================================================================================\nTotal params: 5194049 (19.81 MB)\nTrainable params: 5194049 (19.81 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks = list(callback_model_checkpoint(\"models/embeddings_bidir_lstm.keras\",\n                                           save_best_only = TRUE))\n\n\n\nKodmodel %&gt;%\n  fit(int_train_ds,\n      validation_data = int_val_ds,\n      epochs = 10,\n      callbacks = callbacks)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/embeddings_bidir_lstm.keras\")\nevaluate(model, int_test_ds)[\"accuracy\"] %&gt;%\n  sprintf(\"Test acc: %.3f\\n\", .) %&gt;% cat(\"\\n\")\n\n782/782 - 26s - loss: 0.4049 - accuracy: 0.8601 - 26s/epoch - 33ms/step\nTest acc: 0.860\n \n\n\nModel ten uczy siÄ™ znacznie szybciej niÅ¼ model one-hot (poniewaÅ¼ LSTM musi przetwarzaÄ‡ tylko 256-wymiarowe wektory zamiast 20 000-wymiarowych), a jego dokÅ‚adnoÅ›Ä‡ na zbiorze testowym jest porÃ³wnywalna (86,0%). Jednak wciÄ…Å¼ jesteÅ›my daleko od wynikÃ³w naszego podstawowego modelu bigramowego. CzÄ™Å›ciowo wynika to z faktu, Å¼e model ten analizuje nieco mniej danych: model bigramowy przetwarza peÅ‚ne recenzje, podczas gdy nasz model sekwencji obcina sekwencje po 600 sÅ‚owach.\nJednÄ… z rzeczy, ktÃ³ra nieco obniÅ¼a wydajnoÅ›Ä‡ modelu, jest to, Å¼e nasze sekwencje wejÅ›ciowe sÄ… peÅ‚ne zer. Wynika to z naszego uÅ¼ycia opcji output_sequence_length = max_length w layer_text_vectorization() (z max_length rÃ³wnÄ… 600) - zdania dÅ‚uÅ¼sze niÅ¼ 600 tokenÃ³w sÄ… obcinane do dÅ‚ugoÅ›ci 600 tokenÃ³w, a zdania krÃ³tsze niÅ¼ 600 tokenÃ³w sÄ… wypeÅ‚niane zerami na koÅ„cu, aby moÅ¼na je byÅ‚o poÅ‚Ä…czyÄ‡ z innymi sekwencjami w celu utworzenia ciÄ…gÅ‚ych partii.\nUÅ¼ywamy dwukierunkowej sieci RNN - dwie warstwy RNN dziaÅ‚ajÄ…ce rÃ³wnolegle, z ktÃ³rych jedna przetwarza tokeny w ich naturalnej kolejnoÅ›ci, a druga przetwarza te same tokeny w odwrotnej kolejnoÅ›ci. RNN, ktÃ³ra patrzy na tokeny w ich naturalnej kolejnoÅ›ci, poÅ›wiÄ™ci swoje ostatnie iteracje na â€œoglÄ…daniuâ€ tylko tych wektory, ktÃ³re kodujÄ… padding - prawdopodobnie przez kilkaset iteracji, jeÅ›li oryginalne zdanie byÅ‚o krÃ³tkie. Informacje przechowywane w wewnÄ™trznym stanie RNN bÄ™dÄ… stopniowo zanikaÄ‡, gdy bÄ™dÄ… naraÅ¼one na bezsensowne dane wejÅ›ciowe.\nPotrzebujemy zatem jakiegoÅ› sposobu, aby przekazaÄ‡ RNN, Å¼e powinna pominÄ…Ä‡ te iteracje. Jest do tego API - maskowanie. layer_embedding() jest w stanie wygenerowaÄ‡ â€œmaskÄ™â€, ktÃ³ra odpowiada danym wejÅ›ciowym. Maska ta jest tensorem jedynek i zer (lub TRUE/FALSE), o ksztaÅ‚cie (batch_size, sequence_length), gdzie wpis mask[i, t] odpowiada, czy krok czasowy t prÃ³bki i powinien zostaÄ‡ pominiÄ™ty, czy nie (krok czasowy zostanie pominiÄ™ty, jeÅ›li mask[i, t] ma wartoÅ›Ä‡ 0 lub FALSE, i przetworzony w przeciwnym razie).\nDomyÅ›lnie opcja ta nie jest aktywna - moÅ¼na jÄ… wÅ‚Ä…czyÄ‡, przekazujÄ…c mask_zero = TRUE do funkcji layer_embedding(). MaskÄ™ moÅ¼na pobraÄ‡ za pomocÄ… metody compute_mask():\n\nKodembedding_layer &lt;- layer_embedding(input_dim = 10, output_dim = 256,\n                                   mask_zero = TRUE)\nsome_input &lt;- rbind(c(4, 3, 2, 1, 0, 0, 0),\n                    c(5, 4, 3, 2, 1, 0, 0),\n                    c(2, 1, 0, 0, 0, 0, 0))\nmask &lt;- embedding_layer$compute_mask(some_input)\nmask\n\ntf.Tensor(\n[[ True  True  True  True False False False]\n [ True  True  True  True  True False False]\n [ True  True False False False False False]], shape=(3, 7), dtype=bool)\n\n\nW praktyce prawie nigdy nie bÄ™dziemy musieli rÄ™cznie zarzÄ…dzaÄ‡ maskami. Zamiast tego, keras automatycznie przekaÅ¼e maskÄ™ do kaÅ¼dej warstwy, ktÃ³ra jest w stanie jÄ… przetworzyÄ‡ (jako metadane doÅ‚Ä…czone do sekwencji, ktÃ³rÄ… reprezentuje). Maska ta bÄ™dzie uÅ¼ywana przez warstwy RNN do pomijania zamaskowanych krokÃ³w. JeÅ›li model zwraca caÅ‚Ä… sekwencjÄ™, maska bÄ™dzie rÃ³wnieÅ¼ uÅ¼ywana przez funkcjÄ™ straty do pomijania zamaskowanych krokÃ³w w sekwencji wyjÅ›ciowej. SprÃ³bujmy przeformuÅ‚owaÄ‡ nasz model na model z wÅ‚Ä…czonÄ… funkcjÄ… maskowania.\n\nKodinputs &lt;- layer_input(c(NA), dtype = \"int64\")\nembedded &lt;- inputs %&gt;%\n  layer_embedding(input_dim = max_tokens,\n                  output_dim = 256,\n                  mask_zero = TRUE)\n\noutputs &lt;- embedded %&gt;%\n  bidirectional(layer_lstm(units = 32)) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel &lt;- keras_model(inputs, outputs)\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"binary_crossentropy\",\n                  metrics = \"accuracy\")\nmodel\n\nModel: \"model_3\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_4 (InputLayer)               [(None, None)]                  0           \n embedding_3 (Embedding)            (None, None, 256)               5120000     \n bidirectional_2 (Bidirectional)    (None, 64)                      73984       \n dropout_2 (Dropout)                (None, 64)                      0           \n dense_2 (Dense)                    (None, 1)                       65          \n================================================================================\nTotal params: 5194049 (19.81 MB)\nTrainable params: 5194049 (19.81 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks = list(\n  callback_model_checkpoint(\"models/embeddings_bidir_lstm_with_masking.keras\",\n                            save_best_only = TRUE)\n)\n\n\n\nKodmodel %&gt;% fit(\n  int_train_ds,\n  validation_data = int_val_ds,\n  epochs = 10,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/embeddings_bidir_lstm_with_masking.keras\")\ncat(sprintf(\"Test acc: %.3f\\n\",\n            evaluate(model, int_test_ds)[\"accuracy\"]))\n\n782/782 - 30s - loss: 2.4718 - accuracy: 0.5019 - 30s/epoch - 39ms/step\nTest acc: 0.502\n\n\nTym razem osiÄ…gnÄ™liÅ›my 86,8% dokÅ‚adnoÅ›ci na zbiorze testowym - niewielka, ale zauwaÅ¼alna poprawa.\n\n7.6.2 UÅ¼ycie wstÄ™pnie wytrenowanego osadzenia\nCzasami dostÄ™pnych jest tak maÅ‚o danych szkoleniowych, aby uÅ¼yÄ‡ ich do nauczenia siÄ™ odpowiedniego, specyficznego dla danego zadania osadzenia sÅ‚ownictwa. W takich przypadkach, zamiast uczyÄ‡ osadzania sÅ‚Ã³w razem z gÅ‚Ã³wnym problemem, ktÃ³ry chcemy rozwiÄ…zaÄ‡, moÅ¼emy zaÅ‚adowaÄ‡ wektory osadzania ze wstÄ™pnie obliczonej przestrzeni osadzania, o ktÃ³rej wiemy, Å¼e jest wysoce ustrukturyzowana i wykazuje uÅ¼yteczne wÅ‚aÅ›ciwoÅ›ci - takie, ktÃ³re wychwytujÄ… ogÃ³lne aspekty struktury jÄ™zyka. Uzasadnienie uÅ¼ywania wstÄ™pnie wytrenowanych osadzeÅ„ sÅ‚Ã³w w przetwarzaniu jÄ™zyka naturalnego jest takie samo, jak w przypadku uÅ¼ywania wstÄ™pnie wytrenowanych sieci konwolucyjnych w klasyfikacji obrazÃ³w: nie mamy wystarczajÄ…cej iloÅ›ci danych, aby samodzielnie nauczyÄ‡ siÄ™ odpowiednich funkcji, ale spodziewamy siÄ™, Å¼e funkcje, ktÃ³rych potrzebujemy, sÄ… doÅ›Ä‡ ogÃ³lne - to znaczy, Å¼e majÄ… wspÃ³lne cechy wizualne lub cechy semantyczne. W takim przypadku sensowne jest ponowne wykorzystanie cech wyuczonych dla innego problemu.\nTakie osadzenia sÅ‚Ã³w sÄ… zazwyczaj tworzone przy uÅ¼yciu statystyk wystÄ™powania sÅ‚Ã³w (obserwacje dotyczÄ…ce tego, jakie sÅ‚owa wspÃ³Å‚wystÄ™pujÄ… w zdaniach lub dokumentach), przy uÅ¼yciu rÃ³Å¼nych technik, z ktÃ³rych niektÃ³re obejmujÄ… sieci neuronowe, a inne nie. Idea gÄ™stej, niskowymiarowej przestrzeni osadzania sÅ‚Ã³w, obliczanej w sposÃ³b nienadzorowany, zostaÅ‚a poczÄ…tkowo zbadana przez Bengio i in. na poczÄ…tku XXI wieku (Bengio i in., b.d.), ale zaczÄ™Å‚a siÄ™ rozwijaÄ‡ w badaniach i zastosowaniach przemysÅ‚owych dopiero po wydaniu jednego z najbardziej znanych i udanych schematÃ³w osadzania sÅ‚Ã³w: algorytmu Word2Vec (https://code.google.com/archive/p/word2vec), opracowanego przez Tomasa Mikolova w Google w 2013 roku. Osadzenie Word2Vec wychwytuje specyficzne wÅ‚aÅ›ciwoÅ›ci semantyczne, takie jak pÅ‚eÄ‡.\nMoÅ¼na pobraÄ‡ rÃ³Å¼ne wstÄ™pnie wytrenowane bazy danych osadzania sÅ‚Ã³w i uÅ¼yÄ‡ ich w funkcji Keras layer_embedding(). Word2Vec jest jednÄ… z nich. InnÄ… popularnÄ… wersjÄ… osadzenia sÅ‚Ã³w jest Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), ktÃ³ra zostaÅ‚a opracowana przez naukowcÃ³w ze Stanford w 2014 roku. Ta technika osadzania opiera siÄ™ na faktoryzacji macierzy statystyk wspÃ³Å‚wystÄ™powania sÅ‚Ã³w. Jej twÃ³rcy udostÄ™pnili wstÄ™pnie obliczone osadzenia dla milionÃ³w angielskich tokenÃ³w, uzyskanych z Wikipedii i danych Common Crawl5.\n5Â Common Crawl to non-profit organizacja udostÄ™pniajÄ…ca darmowe, ogromne archiwa danych internetowych do wykorzystania w badaniach, analizie i projektach z zakresu technologii informatycznych.Przyjrzyjmy siÄ™, jak moÅ¼na rozpoczÄ…Ä‡ korzystanie z GloVe embeddings w modelu Keras. Ta sama metoda jest odpowiednia dla osadzeÅ„ Word2Vec lub dowolnej innej bazy danych osadzeÅ„ sÅ‚Ã³w. Zaczniemy od pobrania plikÃ³w GloVe i przeanalizowania ich. NastÄ™pnie zaÅ‚adujemy wektory sÅ‚Ã³w do warstwy Keras layer_embedding(), ktÃ³rej uÅ¼yjemy do zbudowania nowego modelu.\nNajpierw pobierzmy osadzenia sÅ‚Ã³w GloVe wstÄ™pnie obliczone na zbiorze danych angielskiej Wikipedii z 2014 roku. Jest to plik zip o rozmiarze 822 MB zawierajÄ…cy 100-wymiarowe wektory osadzania dla 400 000 sÅ‚Ã³w (lub tokenÃ³w niebÄ™dÄ…cych sÅ‚owami):\n\nKoddownload.file(\"http://nlp.stanford.edu/data/glove.6B.zip\",\n              destfile = \"glove.6B.zip\")\nzip::unzip(\"glove.6B.zip\")\n\n\nPrzeanalizujmy rozpakowany plik (plik .txt), aby utworzyÄ‡ indeks, ktÃ³ry mapuje sÅ‚owa (jako ciÄ…gi) na ich reprezentacjÄ™ wektorowÄ….\n\nKodpath_to_glove_file &lt;- \"~/Downloads/glove.6B/glove.6B.100d.txt\"\nembedding_dim &lt;- 100\n\ndf &lt;- readr::read_table(\n  path_to_glove_file,\n  col_names = FALSE,\n  col_types = paste0(\"c\", strrep(\"n\", 100))\n)\nembeddings_index &lt;- as.matrix(df[, -1])\nrownames(embeddings_index) &lt;- df[[1]]\ncolnames(embeddings_index) &lt;- NULL\nrm(df)\n\n\nOto jak wyglÄ…da embedding_matrix:\n\nKodstr(embeddings_index)\n\n num [1:400000, 1:100] -0.0382 -0.1077 -0.3398 -0.1529 -0.1897 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:400000] \"the\" \",\" \".\" \"of\" ...\n  ..$ : NULL\n\n\nNastÄ™pnie zbudujmy macierz osadzania, ktÃ³rÄ… moÅ¼na zaÅ‚adowaÄ‡ do funkcji layer_embedding(), Musi to byÄ‡ macierz o ksztaÅ‚cie (max_words, embedding_dim), gdzie kaÅ¼dy wpis i zawiera wektor wymiaru embedding_dim dla sÅ‚owa o indeksie i w indeksie sÅ‚owa referencyjnego (zbudowanym podczas tokenizacji).\n\nKodvocabulary &lt;- text_vectorization %&gt;% get_vocabulary()\nstr(vocabulary)\n\n chr [1:20000] \"\" \"[UNK]\" \"the\" \"a\" \"and\" \"of\" \"to\" \"is\" \"in\" \"it\" \"i\" ...\n\nKodtokens &lt;- head(vocabulary[-1], max_tokens)\n\ni &lt;- match(vocabulary, rownames(embeddings_index),\n           nomatch = 0)\n\nembedding_matrix &lt;- array(0, dim = c(max_tokens, embedding_dim))\nembedding_matrix[i != 0, ] &lt;- embeddings_index[i, ]\nstr(embedding_matrix)\n\n num [1:20000, 1:100] 0 0 -0.0382 -0.2709 -0.072 ...\n\n\nNa koniec uÅ¼yjemy funkcji initializer_constant(), aby zaÅ‚adowaÄ‡ wstÄ™pnie wytrenowane osadzenia do layer_embedding(). Aby nie zakÅ‚Ã³caÄ‡ wstÄ™pnie wytrenowanych reprezentacji podczas szkolenia, zamraÅ¼amy warstwÄ™ za pomocÄ… trainable = FALSE:\n\nKodembedding_layer &lt;- layer_embedding(\n  input_dim = max_tokens,\n  output_dim = embedding_dim,\n  embeddings_initializer = initializer_constant(embedding_matrix),\n  trainable = FALSE,\n  mask_zero = TRUE\n)\n\n\nJesteÅ›my teraz gotowi do trenowania nowego modelu - identycznego z naszym poprzednim modelem, ale wykorzystujÄ…cego 100-wymiarowe wstÄ™pnie wytrenowane osadzenia GloVe zamiast 128-wymiarowych wyuczonych osadzeÅ„.\n\nKodinputs &lt;- layer_input(shape(NA), dtype=\"int64\")\nembedded &lt;- embedding_layer(inputs)\noutputs &lt;- embedded %&gt;%\n  bidirectional(layer_lstm(units = 32)) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = 'rmsprop',\n                  loss = \"binary_crossentropy\",\n                  metrics = \"accuracy\")\nmodel\n\nModel: \"model_4\"\n________________________________________________________________________________\n Layer (type)                  Output Shape               Param #    Trainable  \n================================================================================\n input_5 (InputLayer)          [(None, None)]             0          Y          \n embedding_4 (Embedding)       (None, None, 100)          2000000    N          \n bidirectional_3 (Bidirection  (None, 64)                 34048      Y          \n al)                                                                            \n dropout_3 (Dropout)           (None, 64)                 0          Y          \n dense_3 (Dense)               (None, 1)                  65         Y          \n================================================================================\nTotal params: 2034113 (7.76 MB)\nTrainable params: 34113 (133.25 KB)\nNon-trainable params: 2000000 (7.63 MB)\n________________________________________________________________________________\n\nKodcallbacks &lt;- list(callback_model_checkpoint(\"models/glove_embeddings_sequence_model2.keras\",\n                            save_best_only = TRUE))\n\n\n\nKodmodel %&gt;%\n  fit(int_train_ds, validation_data = int_val_ds,\n      epochs = 10, callbacks = callbacks)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/glove_embeddings_sequence_model2.keras\")\ncat(sprintf(\n  \"Test acc: %.3f\\n\", evaluate(model, int_test_ds)[\"accuracy\"]))\n\n782/782 - 31s - loss: 0.5601 - accuracy: 0.7365 - 31s/epoch - 39ms/step\nTest acc: 0.736\n\n\nPrzekonaliÅ›my siÄ™, Å¼e w tym konkretnym zadaniu wstÄ™pnie wytrenowane osadzenia nie sÄ… zbyt pomocne (dopasowanie na zbiorze testowym na poziomie 73,6%), poniewaÅ¼ zbiÃ³r danych zawiera wystarczajÄ…cÄ… liczbÄ™ prÃ³bek, aby moÅ¼na byÅ‚o nauczyÄ‡ siÄ™ od podstaw wystarczajÄ…co wyspecjalizowanej przestrzeni osadzania. Jednak wykorzystanie wstÄ™pnie wytrenowanych osadzeÅ„ moÅ¼e byÄ‡ bardzo pomocne, gdy pracujesz z mniejszym zbiorem danych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#transformery",
    "href": "llm.html#transformery",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.7 Transformery",
    "text": "7.7 Transformery\nPoczÄ…wszy od 2017 roku, nowa architektura modelu zaczÄ™Å‚a wyprzedzaÄ‡ rekurencyjne sieci neuronowe w wiÄ™kszoÅ›ci zadaÅ„ przetwarzania jÄ™zyka naturalnego. Jest niÄ… transformer lub transformator (ang. transformer). Transformery zostaÅ‚y wprowadzone w przeÅ‚omowym artykule â€œAttention Is All You Needâ€ autorstwa Vaswani i in. (b.d.). Sedno artykuÅ‚u znajduje siÄ™ w tytule: jak siÄ™ okazaÅ‚o, prosty mechanizm zwany â€œuwagÄ… neuronowÄ…â€ (ang. neural attention) moÅ¼na wykorzystaÄ‡ do zbudowania potÄ™Å¼nych modeli sekwencji, ktÃ³re nie zawierajÄ… Å¼adnych warstw rekurencyjnych ani warstw splotowych.\nOdkrycie to zapoczÄ…tkowaÅ‚o rewolucjÄ™ w przetwarzaniu jÄ™zyka naturalnego i nie tylko. Uwaga neuronowa szybko staÅ‚a siÄ™ jednÄ… z najbardziej wpÅ‚ywowych idei w uczeniu gÅ‚Ä™bokim. W tym podrozdziale przybliÅ¼ymy jak to dziaÅ‚a i dlaczego okazaÅ‚o siÄ™ tak skuteczne w przypadku danych sekwencyjnych. NastÄ™pnie wykorzystamy samouczenie siÄ™ do stworzenia kodera Transformer, jednego z podstawowych komponentÃ³w architektury Transformer i zastosujemy go do zadania klasyfikacji recenzji filmÃ³w IMDB.\n\n7.7.1 Warstwy atencji\nCzytajÄ…c tÄ™ ksiÄ…Å¼kÄ™, moÅ¼esz jedynie przeglÄ…daÄ‡ niektÃ³re jej czÄ™Å›ci, a inne uwaÅ¼nie czytaÄ‡, w zaleÅ¼noÅ›ci od tego, jakie sÄ… twoje cele lub zainteresowania. Co by byÅ‚o, gdyby modele robiÅ‚y to samo? Wykorzystamy prosty pomysÅ‚: nie wszystkie informacje wejÅ›ciowe widziane przez model sÄ… rÃ³wnie waÅ¼ne dla danego zadania, wiÄ™c modele powinny â€œzwracaÄ‡ wiÄ™kszÄ… uwagÄ™â€ na niektÃ³re funkcje i â€œzwracaÄ‡ mniejszÄ… uwagÄ™â€ na inne funkcje. Czy to brzmi znajomo? Z podobnÄ… koncepcjÄ… spotkaÅ‚eÅ› siÄ™ juÅ¼ dwukrotnie:\n\n\nMax pooling w sieciach splotowych patrzy na pulÄ™ cech w regionie przestrzennym i wybiera tylko jednÄ… cechÄ™ do zachowania. Jest to forma uwagi â€œwszystko albo nicâ€ - zachowaj najwaÅ¼niejszÄ… cechÄ™ i odrzuÄ‡ resztÄ™.\nNormalizacja TF-IDF przypisuje tokenom wspÃ³Å‚czynnik waÅ¼noÅ›ci w oparciu o to, ile informacji mogÄ… przenosiÄ‡ rÃ³Å¼ne tokeny. WaÅ¼ne tokeny sÄ… wzmacniane, podczas gdy nieistotne tokeny sÄ… wygaszane. Jest to ciÄ…gÅ‚a forma uwagi.\n\nIstnieje wiele rÃ³Å¼nych form uwagi, ktÃ³re moÅ¼na sobie wyobraziÄ‡, ale wszystkie zaczynajÄ… siÄ™ od znalezienia wskaÅºnika uwagi dla zestawu cech, z wyÅ¼szymi wskaÅºnikami dla bardziej istotnych cech i niÅ¼szymi dla mniej istotnych (patrz Rys.Â 7.5). SposÃ³b obliczania tych wskaÅºnikÃ³w i to, co naleÅ¼y z nimi zrobiÄ‡, bÄ™dzie siÄ™ rÃ³Å¼niÄ‡ w zaleÅ¼noÅ›ci od podejÅ›cia.\n\n\n\n\n\nRys.Â 7.5: OgÃ³lna koncepcja atencji w uczeniu gÅ‚Ä™bokim: Cechom wejÅ›ciowym przypisywane sÄ… wskaÅºniki uwagi, ktÃ³re moÅ¼na przekazaÄ‡ do nastÄ™pnej reprezentacji danych wejÅ›ciowych.\n\n\nCo najwaÅ¼niejsze, ten rodzaj mechanizmu uwagi moÅ¼e byÄ‡ wykorzystywany do czegoÅ› wiÄ™cej niÅ¼ tylko podkreÅ›lania lub wymazywania pewnych cech. MoÅ¼na go wykorzystaÄ‡ do uÅ›wiadomienia kontekstu funkcji. WÅ‚aÅ›nie dowiedzieliÅ›my siÄ™ o osadzaniu sÅ‚Ã³w, czyli przestrzeniach wektorowych, ktÃ³re przechwytujÄ… â€œstrukturÄ™â€ relacji semantycznych miÄ™dzy rÃ³Å¼nymi sÅ‚owami. W przestrzeni osadzania pojedyncze sÅ‚owo ma staÅ‚Ä… pozycjÄ™ - staÅ‚y zestaw relacji z kaÅ¼dym innym sÅ‚owem w przestrzeni. Ale nie do koÅ„ca tak dziaÅ‚a jÄ™zyk - znaczenie sÅ‚owa jest zwykle zaleÅ¼ne od kontekstu. Kiedy zaznaczamy datÄ™ w kalendarzu (ang. date), nie mÃ³wimy o â€œrandceâ€. Kiedy mÃ³wimy â€œIâ€™ll see you soonâ€, znaczenie sÅ‚owa â€œseeâ€ jest subtelnie inne niÅ¼ â€œseeâ€ w â€œI see what you meanâ€ lub â€œIâ€™ll see this project to its endâ€. I oczywiÅ›cie znaczenie zaimkÃ³w takich jak â€œheâ€, â€œitâ€, â€œyouâ€ i tak dalej jest caÅ‚kowicie zaleÅ¼ne od zdania i mogÄ… zmieniaÄ‡ siÄ™ wielokrotnie w jednym zdaniu.\nOczywiÅ›cie inteligentna przestrzeÅ„ osadzania zapewniÅ‚aby innÄ… reprezentacjÄ™ wektorowÄ… dla sÅ‚owa w zaleÅ¼noÅ›ci od innych otaczajÄ…cych go sÅ‚Ã³w. W tym miejscu pojawia siÄ™ samo-atencja. Celem samo-uwagi jest modulowanie reprezentacji tokena poprzez wykorzystanie reprezentacji powiÄ…zanych tokenÃ³w w sekwencji. W ten sposÃ³b powstajÄ… reprezentacje tokenÃ³w Å›wiadome kontekstu. RozwaÅ¼my przykÅ‚adowe zdanie: â€œThe train left the station on timeâ€. RozwaÅ¼my teraz jedno sÅ‚owo w zdaniu: station. O jakiej stacji mÃ³wimy? Czy moÅ¼e to byÄ‡ stacja radiowa? MoÅ¼e MiÄ™dzynarodowa Stacja Kosmiczna? RozwaÅ¼my to algorytmicznie za pomocÄ… samo-atencji (patrz Rys.Â 7.6)\n\n\n\n\n\nRys.Â 7.6: Samo-atencja. WskaÅºniki uwagi sÄ… obliczane miÄ™dzy â€œstationâ€ a kaÅ¼dym innym sÅ‚owem w sekwencji, a nastÄ™pnie sÄ… uÅ¼ywane do waÅ¼enia sumy wektorÃ³w sÅ‚Ã³w, ktÃ³re stajÄ… siÄ™ nowym wektorem â€œstationâ€.\n\n\nKrok 1 polega na obliczeniu wskaÅºnikÃ³w trafnoÅ›ci miÄ™dzy wektorem dla â€œstationâ€ a kaÅ¼dym innym sÅ‚owem w zdaniu. SÄ… to nasze â€œwskaÅºniki uwagiâ€. Zamierzamy zastosowaÄ‡ iloczyn skalarny pomiÄ™dzy dwoma wektorami sÅ‚Ã³w jako miary siÅ‚y ich zwiÄ…zku. Jest to bardzo wydajna obliczeniowo funkcja odlegÅ‚oÅ›ci i byÅ‚a juÅ¼ standardowym sposobem powiÄ…zania ze sobÄ… dwÃ³ch osadzeÅ„ sÅ‚Ã³w na dÅ‚ugo przed transformerami. W praktyce wyniki te bÄ™dÄ… rÃ³wnieÅ¼ przechodziÄ‡ przez funkcjÄ™ skalowania i softmax, ale na razie jest to tylko szczegÃ³Å‚ implementacji.\nKrok 2 polega na obliczeniu sumy wszystkich wektorÃ³w sÅ‚Ã³w w zdaniu, waÅ¼onych przez nasze wskaÅºniki atencji. SÅ‚owa blisko zwiÄ…zane ze sÅ‚owem â€œstationâ€ bÄ™dÄ… miaÅ‚y wiÄ™kszy udziaÅ‚ w sumie (w tym samo sÅ‚owo â€œstationâ€), podczas gdy sÅ‚owa nieistotne nie wniosÄ… prawie nic. Wynikowy wektor jest naszÄ… nowÄ… reprezentacjÄ… dla â€œstationâ€, reprezentacjÄ…, ktÃ³ra uwzglÄ™dnia otaczajÄ…cy kontekst. W szczegÃ³lnoÅ›ci zawiera ona czÄ™Å›Ä‡ wektora â€œtrainâ€, wyjaÅ›niajÄ…c, Å¼e w rzeczywistoÅ›ci jest to â€œtrain stationâ€.\nPowtarzamy ten proces dla kaÅ¼dego sÅ‚owa w zdaniu, tworzÄ…c nowÄ… sekwencjÄ™ wektorÃ³w kodujÄ…cych zdanie. Zobaczmy to w pseudokodzie R:\n\nKodself_attention &lt;- function(input_sequence) {\n  c(sequence_len, embedding_size) %&lt;-% dim(input_sequence)\n\n  output &lt;- array(0, dim(input_sequence))\n\n  for (i in 1:sequence_len) {\n\n    pivot_vector &lt;- input_sequence[i, ]\n\n    scores &lt;- sapply(1:sequence_len, function(j)\n      pivot_vector %*% input_sequence[j, ])\n\n    scores &lt;- softmax(scores / sqrt(embedding_size))\n\n    broadcast_scores &lt;- as.matrix(scores)[,rep(1, embedding_size)]\n\n    new_pivot_representation &lt;- colSums(input_sequence * broadcast_scores)\n\n    output[i, ] &lt;- new_pivot_representation\n  }\n\n  output\n}\n\nsoftmax &lt;- function(x) {\n   e &lt;- exp(x - max(x))\n   e / sum(e)\n}\n\n\n\nKodsequence_length &lt;- 20\nembed_dim &lt;- 256\ninputs &lt;- layer_input(c(sequence_length, embed_dim))\n\n\nOczywiÅ›cie, w praktyce moÅ¼na uÅ¼yÄ‡ implementacji zwektoryzowanej. Keras ma wbudowanÄ… warstwÄ™ do obsÅ‚ugi tego - layer_multi_head_attention(). Oto jak moÅ¼na jej uÅ¼yÄ‡:\n\nKodnum_heads &lt;- 4\nembed_dim &lt;- 256\n\nmha_layer &lt;- layer_multi_head_attention(num_heads = num_heads,\n                                        key_dim = embed_dim)\noutputs &lt;- mha_layer(inputs, inputs, inputs)\n\n\nCzytajÄ…c to, prawdopodobnie zastanawiasz siÄ™: dlaczego przekazujemy dane wejÅ›ciowe do warstwy trzy razy? Wydaje siÄ™ to zbÄ™dne. Czym sÄ… te â€œwielokrotne gÅ‚owyâ€? Oba te pytania majÄ… proste odpowiedzi.\n\n7.7.2 UogÃ³lnienie samo-atencji\nDo tej pory rozwaÅ¼aliÅ›my tylko jednÄ… sekwencjÄ™ wejÅ›ciowÄ…. Jednak architektura transformera zostaÅ‚a pierwotnie opracowana dla tÅ‚umaczenia maszynowego, gdzie mamy do czynienia z dwiema sekwencjami wejÅ›ciowymi: sekwencjÄ… ÅºrÃ³dÅ‚owÄ…, ktÃ³rÄ… aktualnie tÅ‚umaczysz (np. â€œHowâ€™s the weather today?â€) i sekwencjÄ… docelowÄ…, na ktÃ³rÄ… jÄ… konwertujesz (np. â€œÂ¿QuÃ© tiempo hace hoy?â€). Transformer jest modelem dziÅ‚ajÄ…cym od sekwencji do sekwencji. ZostaÅ‚ zaprojektowany do konwersji jednej sekwencji na innÄ….\nTeraz cofnijmy siÄ™ o krok. Mechanizm samo-uwagi, tak jak go przedstawiliÅ›my, wykonuje nastÄ™pujÄ…ce czynnoÅ›ci, schematycznie:\n\n\n\n\n\nRys.Â 7.7\n\n\nOznacza to, Å¼e â€œdla kaÅ¼dego tokena w wejÅ›ciach (A) obliczamy, jak bardzo token jest powiÄ…zany z kaÅ¼dym tokenem w wejÅ›ciach (B) i uÅ¼ywamy tych wynikÃ³w do waÅ¼enia sumy tokenÃ³w z wejÅ›Ä‡ (C)â€. Co najwaÅ¼niejsze, nie wymgamy, aby A, B i C odnosiÅ‚y siÄ™ do tej samej sekwencji wejÅ›ciowej. W ogÃ³lnym przypadku moÅ¼na to zrobiÄ‡ z trzema rÃ³Å¼nymi sekwencjami. Nazwiemy je â€œzapytaniemâ€ (ang. query), â€œkluczamiâ€ (ang. key) i â€œwartoÅ›ciamiâ€ (ang. value). Operacja ta zamienia siÄ™ w â€œdla kaÅ¼dego elementu w zapytaniu oblicz, jak bardzo element jest powiÄ…zany z kaÅ¼dym kluczem i uÅ¼yj tych wynikÃ³w do waÅ¼enia sumy wartoÅ›ciâ€:\n\nKodoutputs &lt;- sum( values * pairwise_scores( query, keys ))\n\n\nTerminologia ta pochodzi z wyszukiwarek i systemÃ³w rekomendacji (patrz Rys.Â 7.8). WyobraÅºmy sobie, Å¼e wpisujemy zapytanie, aby pobraÄ‡ zdjÄ™cie ze swojej kolekcji, â€œdogs on the beachâ€. WewnÄ™trznie, kaÅ¼de z naszych zdjÄ™Ä‡ w bazie danych jest opisane przez zestaw sÅ‚Ã³w kluczowych - â€œcatâ€, â€œdogâ€, â€œpartyâ€ i tak dalej. Nazwiemy je â€œkluczamiâ€. Wyszukiwarka rozpocznie od porÃ³wnania zapytania z kluczami w bazie danych. â€œdogâ€ daje dopasowanie 1 dla naszego wyszukiwania, a â€œcatâ€ daje dopasowanie 0. NastÄ™pnie algorytm szereguje te klucze wedÅ‚ug siÅ‚y dopasowania - trafnoÅ›ci - i zwraca zdjÄ™cia powiÄ…zane z \\(N\\) najlepszymi dopasowaniami, w kolejnoÅ›ci trafnoÅ›ci.\n\n\n\n\n\nRys.Â 7.8: Pobieranie obrazÃ³w z bazy danych. â€œZapytanieâ€ jest porÃ³wnywane z zestawem â€œkluczyâ€, a wyniki dopasowania sÄ… wykorzystywane do uszeregowania â€œwartoÅ›ciâ€ (obrazÃ³w).\n\n\nChcÄ…c wyraziÄ‡ dokÅ‚adnie wskaÅºniki atencji wzorem matematycznym, napisalibyÅ›my:\n\\[\n\\operatorname{AS}(Q,K,V)=\\operatorname{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\n\\]\ngdzie \\(Q,K,V\\) oznaczajÄ… odpowiednio query, key i value, \\(d_k\\) oznacza dÅ‚ugoÅ›Ä‡ reprezentacji tokenu w osadzeniu.\nKoncepcyjnie, to wÅ‚aÅ›nie te czynnoÅ›ci wykonuje warstwa atencji w transformerach. Mamy sekwencjÄ™ referencyjnÄ…, ktÃ³ra opisuje coÅ›, czego szukamy: zapytanie. Mamy zbiÃ³r wiedzy, z ktÃ³rego prÃ³bujemy wydobyÄ‡ informacje: wartoÅ›ci. KaÅ¼da wartoÅ›Ä‡ ma przypisany klucz, ktÃ³ry opisuje wartoÅ›Ä‡ w formacie, ktÃ³ry moÅ¼na Å‚atwo porÃ³wnaÄ‡ z zapytaniem. Wystarczy dopasowaÄ‡ zapytanie do kluczy. NastÄ™pnie zwracana jest waÅ¼ona suma wartoÅ›ci.\nW praktyce klucze i wartoÅ›ci to czÄ™sto ta sama sekwencja. Na przykÅ‚ad w tÅ‚umaczeniu maszynowym zapytanie byÅ‚oby sekwencjÄ… docelowÄ…, a sekwencja ÅºrÃ³dÅ‚owa odgrywaÅ‚aby rolÄ™ zarÃ³wno kluczy, jak i wartoÅ›ci: dla kaÅ¼dego elementu celu (takiego jak â€œtiempoâ€) chcemy wrÃ³ciÄ‡ do ÅºrÃ³dÅ‚a (â€œHowâ€™s the weather today?â€) i zidentyfikowaÄ‡ rÃ³Å¼ne bity, ktÃ³re sÄ… z nim powiÄ…zane (â€œtiempoâ€ i â€œweatherâ€ powinny mieÄ‡ silne dopasowanie). OczywiÅ›cie, jeÅ›li wykonujemy tylko klasyfikacjÄ™ sekwencji, to zapytanie, klucze i wartoÅ›ci sÄ… takie same: porÃ³wnujemy sekwencjÄ™ z samÄ… sobÄ…, aby wzbogaciÄ‡ kaÅ¼dy token o kontekst z caÅ‚ej sekwencji.\nTo wyjaÅ›nia, dlaczego musieliÅ›my przekazaÄ‡ dane wejÅ›ciowe trzy razy do naszej warstwy layer_multi_head_attention(). Ale po co atencja typu multi-head?\n\n7.7.3 Atencja typu multi-head\n\nMulti-head attention to dodatkowe ulepszenie mechanizmu samo-uwagi, wprowadzone w â€œAttention Is All You Needâ€. Przydomek multi-head odnosi siÄ™ do faktu, Å¼e przestrzeÅ„ wyjÅ›ciowa warstwy samo-uwagi jest podzielona na zestaw niezaleÅ¼nych podprzestrzeni, uczonych oddzielnie: poczÄ…tkowe zapytanie, klucz i wartoÅ›Ä‡ sÄ… wysyÅ‚ane przez trzy niezaleÅ¼ne zestawy gÄ™stych projekcji, co skutkuje trzema oddzielnymi wektorami. KaÅ¼dy wektor jest przetwarzany przez warstwÄ™ atencji, a trzy wyjÅ›cia sÄ… Å‚Ä…czone z powrotem w jednÄ… sekwencjÄ™ wyjÅ›ciowÄ…. KaÅ¼da taka podprzestrzeÅ„ nazywana jest â€œgÅ‚owÄ…â€. PeÅ‚ny obraz zostaÅ‚ przedstawiony na (ig-attention5?).\n\n\n\n\n\nRys.Â 7.9: Multi-head attention\n\n\nObecnoÅ›Ä‡ gÄ™stych projekcji, ktÃ³rych moÅ¼na uczyÄ‡, pozwala warstwie faktycznie siÄ™ czegoÅ› nauczyÄ‡, w przeciwieÅ„stwie do bycia czysto bezstanowÄ… transformacjÄ…, ktÃ³ra wymagaÅ‚aby dodatkowych warstw przed lub po niej, aby byÅ‚a uÅ¼yteczna. Ponadto posiadanie niezaleÅ¼nych â€œgÅ‚Ã³wâ€ pomaga warstwie uczyÄ‡ siÄ™ rÃ³Å¼nych grup cech dla kaÅ¼dego tokena, gdzie cechy w jednej grupie sÄ… ze sobÄ… skorelowane, ale sÄ… w wiÄ™kszoÅ›ci niezaleÅ¼ne od cech w innej grupie.\nJest to zasadniczo podobne do tego, co robiÄ… konwolucje separowalne. W konwolucji separowalnej przestrzeÅ„ wyjÅ›ciowa konwolucji jest podzielona na wiele podprzestrzeni (po jednej na kanaÅ‚ wejÅ›ciowy), ktÃ³re sÄ… uczone niezaleÅ¼nie. ArtykuÅ‚ â€œAttention Is All You Needâ€ zostaÅ‚ napisany w czasie, gdy wykazano, Å¼e idea faktoryzacji przestrzeni cech na niezaleÅ¼ne podprzestrzenie zapewnia ogromne korzyÅ›ci dla komputerowych modeli wizyjnych, zarÃ³wno w przypadku konwolucji separowalnych, jak i w przypadku blisko spokrewnionego podejÅ›cia, konwolucji grupowych. Multi-head attention jest po prostu zastosowaniem tego samego pomysÅ‚u do samo-atencji.\n\n7.7.4 Enkoder transformera\nJeÅ›li dodanie dodatkowych gÄ™stych projekcji jest tak przydatne, dlaczego nie zastosujemy jednej lub dwÃ³ch do wyjÅ›cia mechanizmu uwagi? Nasz model zaczyna realizowaÄ‡ wiele funkcji, wiÄ™c moÅ¼emy chcieÄ‡ dodaÄ‡ poÅ‚Ä…czenia resztkowe, aby upewniÄ‡ siÄ™, Å¼e po drodze nie utracimy Å¼adnych cennych informacji. Dodatkowo aby przyspieszyÄ‡ proces uczenia moÅ¼na dodaÄ‡ warstwy normalizacji.\nTaki proces myÅ›lowy, ktÃ³ry, rozwijaÅ‚ siÄ™ w umysÅ‚ach twÃ³rcÃ³w architektury Transformer. Dzielenie danych wyjÅ›ciowych na wiele niezaleÅ¼nych przestrzeni, dodawanie poÅ‚Ä…czeÅ„ resztkowych, dodawanie warstw normalizacji - wszystko to sÄ… standardowe wzorce architektury, ktÃ³re warto wykorzystaÄ‡ w kaÅ¼dym zÅ‚oÅ¼onym modelu.\n\n\n\n\n\nRys.Â 7.10: Transformer-Encoder Å‚Ä…czy layer_multi_head_attention() z gÄ™stÄ… projekcjÄ… i dodaje normalizacjÄ™ oraz poÅ‚Ä…czenia resztkowe.\n\n\nOryginalna architektura Transformer skÅ‚ada siÄ™ z dwÃ³ch czÄ™Å›ci: kodera transformera, ktÃ³ry przetwarza sekwencjÄ™ ÅºrÃ³dÅ‚owÄ…, oraz dekodera transformera, ktÃ³ry wykorzystuje sekwencjÄ™ ÅºrÃ³dÅ‚owÄ… do wygenerowania przetÅ‚umaczonej wersji. Co najwaÅ¼niejsze, czÄ™Å›Ä‡ kodera moÅ¼e byÄ‡ uÅ¼ywana do klasyfikacji tekstu. Jest to bardzo ogÃ³lny moduÅ‚, ktÃ³ry przyjmuje sekwencjÄ™ i uczy siÄ™ przeksztaÅ‚caÄ‡ jÄ… w bardziej uÅ¼ytecznÄ… reprezentacjÄ™. Zaimplementujmy koder Transformer (taki jak na Rys.Â 7.10) i wyprÃ³bujemy go w zadaniu klasyfikacji sentymentu recenzji filmowej.\n\nKodlayer_transformer_encoder &lt;- new_layer_class(\n  classname = \"TransformerEncoder\",\n  initialize = function(embed_dim, dense_dim, num_heads, ...) {\n    super$initialize(...)\n    self$embed_dim &lt;- embed_dim # rozmiar tokenÃ³w wejÅ›ciowych\n    self$dense_dim &lt;- dense_dim # liczba neuronÃ³w w sieci gÄ™stej\n    self$num_heads &lt;- num_heads # liczba gÅ‚Ã³w w warstwie atencji\n    self$attention &lt;-\n      layer_multi_head_attention(num_heads = num_heads,\n                                 key_dim = embed_dim)\n\n    self$dense_proj &lt;- keras_model_sequential() %&gt;%\n      layer_dense(dense_dim, activation = \"relu\") %&gt;%\n      layer_dense(embed_dim)\n\n    self$layernorm_1 &lt;- layer_layer_normalization()\n    self$layernorm_2 &lt;- layer_layer_normalization()\n  },\n\n  call = function(inputs, mask = NULL) { # okreÅ›lenie wywoÅ‚ania\n    if (!is.null(mask)) # dostosowanie rozmiaru maski z 2D do 3D lub 4D\n      mask &lt;- mask[, tf$newaxis, ]\n\n    inputs %&gt;%\n      { self$attention(., ., attention_mask = mask) + . } %&gt;% # poÅ‚Ä…czenie rezydualne\n      self$layernorm_1() %&gt;%\n      { self$dense_proj(.) + . } %&gt;% # poÅ‚Ä…czenie projekcji gÄ™stej z rezydualnÄ…\n      self$layernorm_2()\n  },\n\n  get_config = function() { # implementacja serializacji potrzebna do zapisu modelu\n    config &lt;- super$get_config()\n    for(name in c(\"embed_dim\", \"num_heads\", \"dense_dim\"))\n      config[[name]] &lt;- self[[name]]\n    config\n  }\n)\n\n\n\n\n\n\n\n\nWskazÃ³wka\n\n\n\nPodczas pisania niestandardowych warstw naleÅ¼y zaimplementowaÄ‡ metodÄ™ get_config(). UmoÅ¼liwia to ponowne utworzenie warstwy z jej konfiguracji, co jest przydatne podczas zapisywania i Å‚adowania modelu. Metoda powinna zwracaÄ‡ nazwanÄ… listÄ™ R, ktÃ³ra zawiera wartoÅ›ci argumentÃ³w konstruktora uÅ¼ytych do utworzenia warstwy.\nWszystkie warstwy Keras mogÄ… byÄ‡ serializowane i deserializowane w nastÄ™pujÄ…cy sposÃ³b:\n\nKodconfig &lt;- layer$get_config()\nnew_layer &lt;- do.call(layer_&lt;type&gt;, config)\n\n\ngdzie layer_&lt;type&gt; jest oryginalnym konstruktorem warstwy. Na przykÅ‚ad:\n\nKodlayer &lt;- layer_dense(units = 10)\nconfig &lt;- layer$get_config()\nnew_layer &lt;- do.call(layer_dense, config)\n\n\nMoÅ¼na rÃ³wnieÅ¼ uzyskaÄ‡ dostÄ™p do rozpakowanego oryginalnego konstruktora warstwy z dowolnej istniejÄ…cej warstwy bezpoÅ›rednio za pomocÄ… specjalnego symbolu __class__ (choÄ‡ rzadko trzeba to robiÄ‡):\n\nKodlayer$`__class__`\nnew_layer &lt;- layer$`__class__`$from_config(config)\n\n\nZdefiniowanie metody get_config() w niestandardowych klasach warstw umoÅ¼liwia okreÅ›lenie przepÅ‚ywu pracy. Na przykÅ‚ad:\n\nKodlayer &lt;- layer_transformer_encoder(embed_dim = 256, dense_dim = 32,\n                                   num_heads = 2)\nconfig &lt;- layer$get_config()\n\nnew_layer &lt;- do.call(layer_transformer_encoder, config)\n# -- lub --\nnew_layer &lt;- layer$`__class__`$from_config(config)\n\n\nPodczas zapisywania modelu zawierajÄ…cego niestandardowe warstwy, zapisany plik bÄ™dzie zawieraÅ‚ te konfiguracje. Podczas Å‚adowania modelu z pliku naleÅ¼y dostarczyÄ‡ niestandardowe klasy warstw do procesu Å‚adowania, aby mÃ³gÅ‚ on zrozumieÄ‡ obiekty konfiguracyjne:\n\nKodmodel &lt;- save_model_tf(model, filename)\nmodel &lt;- load_model_tf(filename,\n                       custom_objects = list(layer_transformer_encoder))\n\n\nZwrÃ³Ä‡my uwagÄ™, Å¼e jeÅ›li lista dostarczona do custom_objects jest nazwana, to nazwy sÄ… dopasowywane do argumentu classname, ktÃ³ry zostaÅ‚ podany podczas konstruowania niestandardowego obiektu:\n\nKodmodel &lt;- load_model_tf(\n  filename,\n  custom_objects = list(TransformerEncoder = layer_transformer_encoder))\n\n\n\n\nZauwaÅ¼yliÅ›my juÅ¼ pewnie, Å¼e warstwy normalizacji, ktÃ³rych tu uÅ¼ywamy, nie sÄ… layer_batch_normalization(), jak te, ktÃ³rych uÅ¼ywaliÅ›my wczeÅ›niej w modelach do obrazÃ³w. To dlatego, Å¼e layer_batch_normalization() nie dziaÅ‚a dobrze w przypadku danych sekwencyjnych. Zamiast tego uÅ¼ywamy layer_layer_normalization(), ktÃ³ra normalizuje kaÅ¼dÄ… sekwencjÄ™ niezaleÅ¼nie od innych sekwencji w partii. Tak to wyglÄ…da w pseudokodzie R:\n\nKodlayer_normalization &lt;- function(batch_of_sequences) {\n  c(batch_size, sequence_length, embedding_dim) %&lt;-% dim(batch_of_sequences)\n  means &lt;- variances &lt;-\n    array(0, dim = dim(batch_of_sequences))\n  for (b in seq(batch_size))\n    for (s in seq(sequence_length)) { # obliczanie statystyk po ostatniej osi osadzeÅ„\n      embedding &lt;- batch_of_sequences[b, s, ]\n      means[b, s, ] &lt;- mean(embedding)\n      variances[b, s, ] &lt;- var(embedding)\n    }\n  (batch_of_sequences - means) / variances\n}\n\n\nMoÅ¼na to porÃ³wnaÄ‡ z klasycznÄ… layer_batch_normalization():\n\nKodbatch_normalization &lt;- function(batch_of_images) {\n  c(batch_size, height, width, channels) %&lt;-% dim(batch_of_images)\n  means &lt;- variances &lt;-\n    array(0, dim = dim(batch_of_images))\n  for (ch in seq(channels)) { # statystyki liczone sÄ… po partiach\n    channel &lt;- batch_of_images[, , , ch] # dla kaÅ¼dego kanaÅ‚u liczone oddzielnie\n    means[, , , ch] &lt;- mean(channel)\n    variances[, , , ch] &lt;- var(channel)\n  }\n  (batch_of_images - means) / variances\n}\n\n\nFunkcja batch_normalization() zbiera informacje z wielu prÃ³bek w celu uzyskania statystyk Å›rednich i wariancji cech, a funkcja layer_normalization() gromadzi dane w kaÅ¼dej sekwencji osobno, co jest bardziej wÅ‚aÅ›ciwe dla danych sekwencyjnych.\nTeraz wykorzystamy zbudowany transformer do naszego zadania:\n\nKodvocab_size &lt;- 20000\nembed_dim &lt;- 256\nnum_heads &lt;- 2\ndense_dim &lt;- 32\n\ninputs &lt;- layer_input(shape(NA), dtype = \"int64\")\noutputs &lt;- inputs %&gt;%\n  layer_embedding(vocab_size, embed_dim) %&gt;%\n  layer_transformer_encoder(embed_dim, dense_dim, num_heads) %&gt;%\n  layer_global_average_pooling_1d() %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\nmodel &lt;-  keras_model(inputs, outputs)\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"binary_crossentropy\",\n                  metrics = \"accuracy\")\nmodel\n\nModel: \"model_5\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_7 (InputLayer)               [(None, None)]                  0           \n embedding_5 (Embedding)            (None, None, 256)               5120000     \n transformer_encoder (TransformerE  (None, None, 256)               543776      \n ncoder)                                                                        \n global_average_pooling1d (GlobalA  (None, 256)                     0           \n veragePooling1D)                                                               \n dropout_4 (Dropout)                (None, 256)                     0           \n dense_4 (Dense)                    (None, 1)                       257         \n================================================================================\nTotal params: 5664033 (21.61 MB)\nTrainable params: 5664033 (21.61 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks = list(callback_model_checkpoint(\"models/transformer_encoder.keras\",\n                                           save_best_only = TRUE))\n\n\n\nKodmodel %&gt;% fit(\n  int_train_ds,\n  validation_data = int_val_ds,\n  epochs = 20,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/transformer_encoder.keras\",\n                       custom_objects = list(layer_transformer_encoder))\n\nsprintf(\"Test acc: %.3f\", evaluate(model, int_test_ds)[\"accuracy\"])\n\n782/782 - 150s - loss: 0.2902 - accuracy: 0.8876 - 150s/epoch - 191ms/step\n\n\n[1] \"Test acc: 0.888\"\n\n\nOsiÄ…gamy dopasowanie na poziomie 88,8% dla zbioru testowego. W tym momencie powinniÅ›my zaczÄ…Ä‡ odczuwaÄ‡ lekki niepokÃ³j. CoÅ› tu nie gra. Co jest nie tak?\nTen rozdziaÅ‚ rzekomo dotyczy â€œmodeli sekwencjiâ€. ZaczÄ™liÅ›my od podkreÅ›lenia znaczenia kolejnoÅ›ci sÅ‚Ã³w. PowiedzieliÅ›my, Å¼e transformer to architektura przetwarzania sekwencji, pierwotnie opracowana na potrzeby tÅ‚umaczenia maszynowego. A jednakâ€¦ koder transformera, ktÃ³ry wÅ‚aÅ›nie zobaczyliÅ›my w akcji, wcale nie byÅ‚ modelem sekwencyjnym. SkÅ‚ada siÄ™ on z gÄ™stych warstw, ktÃ³re przetwarzajÄ… tokeny sekwencji niezaleÅ¼nie od siebie, oraz warstwy uwagi, ktÃ³ra traktuje tokeny jako zestaw. MoÅ¼esz zmieniÄ‡ kolejnoÅ›Ä‡ tokenÃ³w w sekwencji, a otrzymamy dokÅ‚adnie takie same wyniki wzajemnej uwagi i dokÅ‚adnie takie same reprezentacje kontekstu. GdybyÅ›my caÅ‚kowicie wymieszali sÅ‚owa w kaÅ¼dej recenzji filmu, model by tego nie zauwaÅ¼yÅ‚ i nadal uzyskaÅ‚byÅ› dokÅ‚adnie takÄ… samÄ… dokÅ‚adnoÅ›Ä‡. Samoatencja jest mechanizmem przetwarzania zbiorÃ³w, skoncentrowanym na relacjach miÄ™dzy parami elementÃ³w sekwencji (patrz Tab.Â 7.1) - jest Å›lepa na to, czy elementy te wystÄ™pujÄ… na poczÄ…tku, na koÅ„cu czy w Å›rodku sekwencji. Dlaczego wiÄ™c mÃ³wimy, Å¼e Transformer jest modelem sekwencji? I w jaki sposÃ³b moÅ¼e on byÄ‡ dobry dla tÅ‚umaczenia maszynowego, jeÅ›li nie bierze pod uwagÄ™ kolejnoÅ›ci sÅ‚Ã³w?\nWskazaliÅ›my rozwiÄ…zanie wczeÅ›niej w tym rozdziale. Mimochodem wspominaliÅ›my, Å¼e transformer jest podejÅ›ciem hybrydowym, ktÃ³re jest technicznie niezaleÅ¼ne od kolejnoÅ›ci, ale rÄ™cznie wprowadza informacje o kolejnoÅ›ci do przetwarzanych reprezentacji. To jest brakujÄ…cy skÅ‚adnik! Nazywa siÄ™ to kodowaniem pozycyjnym. Przyjrzyjmy siÄ™ temu.\n\n\nTab.Â 7.1: Cechy modeli wykorzystywanych do NLP\n\n\n\n\nÅšwiadomoÅ›Ä‡ kolejnoÅ›ci sÅ‚Ã³w\nÅšwiadomoÅ›Ä‡ kontekstu\n\n\n\nBag-of-unigrams\nNie\nNie\n\n\nBag-of-bigrams\nBardzo ograniczona\nNie\n\n\nRNN\nTak\nNie\n\n\nSelf-attention\nNie\nTak\n\n\nTransformer\nTak\nTak\n\n\n\n\n\n\n\n7.7.5 Kodowanie pozycyjne\nIdea kodowania pozycyjnego jest bardzo prosta. Aby zapewniÄ‡ modelowi dostÄ™p do informacji o kolejnoÅ›ci sÅ‚Ã³w, zamierzamy dodaÄ‡ pozycjÄ™ sÅ‚owa w zdaniu do kaÅ¼dego osadzenia sÅ‚owa. Nasze wejÅ›ciowe osadzenia sÅ‚Ã³w bÄ™dÄ… miaÅ‚y dwa komponenty: zwykÅ‚y wektor sÅ‚Ã³w, ktÃ³ry reprezentuje sÅ‚owo niezaleÅ¼nie od konkretnego kontekstu, oraz wektor pozycji, ktÃ³ry reprezentuje pozycjÄ™ sÅ‚owa w bieÅ¼Ä…cym zdaniu.\nNajprostszym schematem, jaki moÅ¼na wymyÅ›liÄ‡, byÅ‚oby poÅ‚Ä…czenie pozycji sÅ‚owa z jego wektorem osadzania. DodalibyÅ›my oÅ› â€œpozycjaâ€ do wektora i wypeÅ‚nili jÄ… wartoÅ›ciÄ… 0 dla pierwszego sÅ‚owa w sekwencji, 1 dla drugiego i tak dalej. MoÅ¼e to jednak nie byÄ‡ idealne rozwiÄ…zanie, poniewaÅ¼ pozycje mogÄ… byÄ‡ potencjalnie bardzo duÅ¼ymi liczbami caÅ‚kowitymi, co zakÅ‚Ã³ci zakres wartoÅ›ci w wektorze osadzania. Jak wiadomo, sieci neuronowe nie lubiÄ… bardzo duÅ¼ych wartoÅ›ci wejÅ›ciowych ani dyskretnych rozkÅ‚adÃ³w danych wejÅ›ciowych.\nW oryginalnym artykule â€œAttention Is All You Needâ€ zastosowano interesujÄ…cÄ… sztuczkÄ™ do kodowania pozycji sÅ‚Ã³w: dodano do osadzania sÅ‚Ã³w wektor zawierajÄ…cy wartoÅ›ci z zakresu [-1, 1], ktÃ³re zmieniaÅ‚y siÄ™ cyklicznie w zaleÅ¼noÅ›ci od pozycji (wykorzystano do tego funkcje cosinusowe). Ta sztuczka pozwala jednoznacznie scharakteryzowaÄ‡ dowolnÄ… liczbÄ™ caÅ‚kowitÄ… w duÅ¼ym zakresie za pomocÄ… wektora maÅ‚ych wartoÅ›ci. To sprytne, ale nie tego zamierzamy uÅ¼yÄ‡ w naszym przypadku. Zrobimy coÅ› prostszego i bardziej efektywnego: nauczymy siÄ™ wektorÃ³w osadzania pozycji w ten sam sposÃ³b, w jaki uczymy siÄ™ osadzaÄ‡ indeksy sÅ‚Ã³w. NastÄ™pnie dodamy nasze osadzenia pozycji do odpowiednich osadzeÅ„ sÅ‚Ã³w, aby uzyskaÄ‡ osadzenie sÅ‚Ã³w uwzglÄ™dniajÄ…ce pozycjÄ™. Technika ta nazywana jest â€œosadzaniem pozycyjnymâ€.\n\nKodlayer_positional_embedding &lt;- new_layer_class(\n  classname = \"PositionalEmbedding\",\n\n  initialize = function(sequence_length, input_dim, output_dim, ...) {\n    super$initialize(...)\n    self$token_embeddings &lt;-\n      layer_embedding(input_dim = input_dim,\n                      output_dim = output_dim)\n    self$position_embeddings &lt;-\n      layer_embedding(input_dim = sequence_length,\n                      output_dim = output_dim)\n    self$sequence_length &lt;- sequence_length\n    self$input_dim &lt;- input_dim\n    self$output_dim &lt;- output_dim\n  },\n\n  call = function(inputs) {\n    length &lt;- tf$shape(inputs)[-1]\n    positions &lt;- tf$range(start = 0L, limit = length, delta = 1L)\n    embedded_tokens &lt;- self$token_embeddings(inputs)\n    embedded_positions &lt;- self$position_embeddings(positions)\n    embedded_tokens + embedded_positions\n  },\n\n  compute_mask = function(inputs, mask = NULL) {\n    inputs != 0\n  },\n\n  get_config = function() {\n    config &lt;- super$get_config()\n    for(name in c(\"output_dim\", \"sequence_length\", \"input_dim\"))\n      config[[name]] &lt;- self[[name]]\n    config\n  }\n) \n\n\nMoÅ¼na teraz uÅ¼yÄ‡ funkcji layer_positional_embedding() tak jak zwykÅ‚ej layer_embedding().\n\nKodvocab_size &lt;- 20000\nsequence_length &lt;- 600\nembed_dim &lt;- 256\nnum_heads &lt;- 2\ndense_dim &lt;- 32\n\ninputs &lt;- layer_input(shape(NULL), dtype = \"int64\")\n\noutputs &lt;- inputs %&gt;%\n  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %&gt;%\n  layer_transformer_encoder(embed_dim, dense_dim, num_heads) %&gt;%\n  layer_global_average_pooling_1d() %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel &lt;-\n  keras_model(inputs, outputs) %&gt;%\n  compile(optimizer = \"rmsprop\",\n          loss = \"binary_crossentropy\",\n          metrics = \"accuracy\")\n\nmodel\n\nModel: \"model_6\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_8 (InputLayer)               [(None, None)]                  0           \n positional_embedding (PositionalE  (None, None, 256)               5273600     \n mbedding)                                                                      \n transformer_encoder_1 (Transforme  (None, None, 256)               543776      \n rEncoder)                                                                      \n global_average_pooling1d_1 (Globa  (None, 256)                     0           \n lAveragePooling1D)                                                             \n dropout_5 (Dropout)                (None, 256)                     0           \n dense_9 (Dense)                    (None, 1)                       257         \n================================================================================\nTotal params: 5817633 (22.19 MB)\nTrainable params: 5817633 (22.19 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks &lt;- list(\n  callback_model_checkpoint(\"models/full_transformer_encoder.keras\",\n                            save_best_only = TRUE)\n)\n\n\n\nKodmodel %&gt;% fit(\n  int_train_ds,\n  validation_data = int_val_ds,\n  epochs = 20,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\n  \"models/full_transformer_encoder.keras\",\n  custom_objects = list(layer_transformer_encoder,\n                        layer_positional_embedding))\n\ncat(sprintf(\n  \"Test acc: %.3f\\n\", evaluate(model, int_test_ds)[\"accuracy\"]))\n\n782/782 - 149s - loss: 0.2802 - accuracy: 0.8857 - 149s/epoch - 191ms/step\nTest acc: 0.886\n\n\nOsiÄ…gnÄ™liÅ›my dokÅ‚adnoÅ›Ä‡ testowÄ… na poziomie 88,6% - to nieco gorzej niÅ¼ w przypadku transformera bez osadzeÅ„ pozycyjnych. Jest on jednak wciÄ…Å¼ nieco gorszy od podejÅ›cia opartego na bag-of-words.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#wskazÃ³wki-praktyczne-dotyczÄ…ce-wyboru-modelu",
    "href": "llm.html#wskazÃ³wki-praktyczne-dotyczÄ…ce-wyboru-modelu",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.8 WskazÃ³wki praktyczne dotyczÄ…ce wyboru modelu",
    "text": "7.8 WskazÃ³wki praktyczne dotyczÄ…ce wyboru modelu\nCzasami moÅ¼na usÅ‚yszeÄ‡, Å¼e metody bag-of-words sÄ… przestarzaÅ‚e i Å¼e modele sekwencji oparte na transformatach sÄ… najlepszym rozwiÄ…zaniem, niezaleÅ¼nie od zadania lub zbioru danych. Zdecydowanie tak nie jest. MaÅ‚y stos gÄ™stych warstw na szczycie bag-of-bigrams pozostaje caÅ‚kowicie poprawnym i odpowiednim podejÅ›ciem w wielu przypadkach. W rzeczywistoÅ›ci, spoÅ›rÃ³d rÃ³Å¼nych technik, ktÃ³re wyprÃ³bowaliÅ›my na zbiorze danych IMDB w tym rozdziale, jak dotÄ…d najlepiej sprawdzaÅ‚ siÄ™ bag-of-bigrams! Kiedy wiÄ™c naleÅ¼y preferowaÄ‡ jedno podejÅ›cie nad drugim?\nW 2017 roku zespÃ³Å‚ pod kierownictwem Cholleta przeprowadziÅ‚ systematycznÄ… analizÄ™ wydajnoÅ›ci rÃ³Å¼nych technik klasyfikacji tekstu w wielu rÃ³Å¼nych typach zbiorÃ³w danych tekstowych i odkryÅ‚ niezwykÅ‚Ä… i zaskakujÄ…cÄ… zasadÄ™ podejmowania decyzji, czy wybraÄ‡ model bag-of-words, czy model sekwencji (http://mng.bz/AOzK) - swego rodzaju zÅ‚otÄ… proporcjÄ™. Okazuje siÄ™, Å¼e zabierajÄ…c siÄ™ do nowego zadania klasyfikacji tekstu, naleÅ¼y zwrÃ³ciÄ‡ szczegÃ³lnÄ… uwagÄ™ na stosunek liczby prÃ³bek w danych treningowych do Å›redniej liczby sÅ‚Ã³w na prÃ³bkÄ™ (patrz Rys.Â 7.11). JeÅ›li stosunek ten jest niewielki - mniejszy niÅ¼ 1500 - wÃ³wczas model bag-of-bigrams bÄ™dzie dziaÅ‚aÅ‚ lepiej (i jako bonus, bÄ™dzie znacznie szybszy do trenowania). JeÅ›li wspÃ³Å‚czynnik ten jest wyÅ¼szy niÅ¼ 1500, naleÅ¼y wybraÄ‡ model sekwencyjny. Innymi sÅ‚owy, modele sekwencyjne dziaÅ‚ajÄ… najlepiej, gdy dostÄ™pnych jest wiele danych szkoleniowych i gdy kaÅ¼da prÃ³bka jest stosunkowo krÃ³tka.\n\n\n\n\n\nRys.Â 7.11\n\n\nTak wiÄ™c, jeÅ›li klasyfikujemy dokumenty o dÅ‚ugoÅ›ci 1000 sÅ‚Ã³w i mamy ich 100 000 (stosunek 100), powinniÅ›my wybraÄ‡ model bigramowy. JeÅ›li klasyfikujemy tweety o Å›redniej dÅ‚ugoÅ›ci 40 sÅ‚Ã³w i mamy ich 50 000 (stosunek 1250), powinniÅ›my rÃ³wnieÅ¼ wybraÄ‡ model bigramowy. Ale jeÅ›li zwiÄ™kszymy rozmiar zbioru danych do 500 000 tweetÃ³w (stosunek 12 500), wybierzmy wÃ³wczas transformery. A co z zadaniem klasyfikacji recenzji filmÃ³w IMDB? MieliÅ›my 20 000 prÃ³bek treningowych i Å›redniÄ… liczbÄ™ sÅ‚Ã³w wynoszÄ…cÄ… 233, wiÄ™c nasza zasada kciuka wskazuje na model bigramowy, co potwierdza to, co znaleÅºliÅ›my w praktyce.\nIntuicyjnie ma to sens: dane wejÅ›ciowe modelu sekwencji reprezentujÄ… bogatszÄ… i bardziej zÅ‚oÅ¼onÄ… przestrzeÅ„, a zatem potrzeba wiÄ™cej danych, aby jÄ… odwzorowaÄ‡; tymczasem zwykÅ‚y zestaw terminÃ³w jest przestrzeniÄ… tak prostÄ…, Å¼e moÅ¼na na niej trenowaÄ‡ regresjÄ™ logistycznÄ… przy uÅ¼yciu zaledwie kilkuset lub tysiÄ™cy prÃ³bek. Ponadto, im krÃ³tsza jest prÃ³bka, tym mniej model moÅ¼e sobie pozwoliÄ‡ na odrzucenie jakichkolwiek zawartych w niej informacji - w szczegÃ³lnoÅ›ci kolejnoÅ›Ä‡ sÅ‚Ã³w staje siÄ™ waÅ¼niejsza, a odrzucenie jej moÅ¼e powodowaÄ‡ niejednoznacznoÅ›Ä‡. Zdania â€œten film jest bombÄ…â€ i â€œten film byÅ‚ bombÄ…â€ majÄ… bardzo zbliÅ¼one reprezentacje unigramÃ³w, co moÅ¼e zmyliÄ‡ model bag-of-words, ale model sekwencji moÅ¼e stwierdziÄ‡, ktÃ³ry z nich jest negatywny, a ktÃ³ry pozytywny. Przy dÅ‚uÅ¼szej prÃ³bce statystyki sÅ‚Ã³w staÅ‚yby siÄ™ bardziej wiarygodne, a temat lub sentyment byÅ‚yby bardziej widoczne na podstawie samego histogramu sÅ‚Ã³w.\nNaleÅ¼y pamiÄ™taÄ‡, Å¼e ta heurystyczna reguÅ‚a zostaÅ‚a opracowana specjalnie dla klasyfikacji tekstu. Niekoniecznie musi siÄ™ ona sprawdzaÄ‡ w innych zadaniach NLP - na przykÅ‚ad, jeÅ›li chodzi o tÅ‚umaczenie maszynowe, Transformer wyrÃ³Å¼nia siÄ™ szczegÃ³lnie w przypadku bardzo dÅ‚ugich sekwencji, w porÃ³wnaniu do sieci RNN. Nasza heurystyka jest rÃ³wnieÅ¼ tylko praktycznÄ… zasadÄ…, a nie naukowym prawem, wiÄ™c spodziewaj siÄ™, Å¼e bÄ™dzie dziaÅ‚aÄ‡ przez wiÄ™kszoÅ›Ä‡ czasu, ale niekoniecznie za kaÅ¼dym razem.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "llm.html#tÅ‚umaczenia-typu-sequence-to-sequence",
    "href": "llm.html#tÅ‚umaczenia-typu-sequence-to-sequence",
    "title": "\n6Â  Modele jÄ™zykowe\n",
    "section": "\n7.9 TÅ‚umaczenia typu sequence-to-sequence",
    "text": "7.9 TÅ‚umaczenia typu sequence-to-sequence\nPosiadamy juÅ¼ wszystkie narzÄ™dzia, ktÃ³rych bÄ™dziemy potrzebowaÄ‡, aby poradziÄ‡ sobie z wiÄ™kszoÅ›ciÄ… zadaÅ„ przetwarzania jÄ™zyka naturalnego. JednakÅ¼e, widzieliÅ›my te narzÄ™dzia w akcji tylko w jednym problemie: klasyfikacji tekstu. Jest to niezwykle popularny przypadek uÅ¼ycia, ale NLP to znacznie wiÄ™cej niÅ¼ klasyfikacja. W tym podrozdziale poszerzymy swojÄ… wiedzÄ™, poznajÄ…c modele sequence-to-sequence.\nModel sekwencja-sekwencja przyjmuje jednÄ… sekwencjÄ™ jako dane wejÅ›ciowe (czÄ™sto zdanie lub akapit) i tÅ‚umaczy jÄ… na drugÄ… sekwencjÄ™. Jest to zadanie leÅ¼Ä…ce u podstaw wielu najbardziej udanych zastosowaÅ„ NLP:\n\nTÅ‚umaczenie maszynowe - konwersja akapitu w jÄ™zyku ÅºrÃ³dÅ‚owym na jego odpowiednik w jÄ™zyku docelowym.\nPodsumowanie tekstu - konwersja dÅ‚ugiego dokumentu na krÃ³tszÄ… wersjÄ™, ktÃ³ra zachowuje najwaÅ¼niejsze informacje.\nOdpowiadanie na pytania - konwersja pytania wejÅ›ciowego na odpowiedÅº.\nChatboty - konwertowanie monitu dialogowego na odpowiedÅº na ten monit lub konwertowanie historii konwersacji na nastÄ™pnÄ… odpowiedÅº w konwersacji.\nGenerowanie tekstu - konwertowanie monitu tekstowego na akapit, ktÃ³ry uzupeÅ‚nia monit.\nI tak dalej.\n\n\n\n\n\n\nRys.Â 7.12: Uczenie sekwencyjne. Sekwencja ÅºrÃ³dÅ‚owa jest przetwarzana przez koder, a nastÄ™pnie przesyÅ‚ana do dekodera. Dekoder analizuje dotychczasowÄ… sekwencjÄ™ docelowÄ… i przewiduje przesuniÄ™cie sekwencji docelowej o jeden krok w przyszÅ‚oÅ›Ä‡. Podczas wnioskowania generujemy jeden token docelowy na raz i przekazujemy go z powrotem do dekodera.\n\n\nOgÃ³lny szablon modeli sekwencja-sekwencja zostaÅ‚ opisany na Rys.Â 7.12. Podczas szkolenia:\n\nModel kodera zamienia sekwencjÄ™ ÅºrÃ³dÅ‚owÄ… w reprezentacjÄ™ poÅ›redniÄ….\nDekoder jest szkolony, aby przewidzieÄ‡ nastÄ™pny token \\(i\\) w sekwencji docelowej, patrzÄ…c zarÃ³wno na poprzednie tokeny (od 1 do \\(i - 1\\)), jak i zakodowanÄ… sekwencjÄ™ ÅºrÃ³dÅ‚owÄ….\n\nPodczas wnioskowania nie mamy dostÄ™pu do sekwencji docelowej - prÃ³bujemy przewidzieÄ‡ jÄ… od zera. BÄ™dziemy musieli generowaÄ‡ jÄ… po jednym tokenie na raz:\n\nOtrzymujemy zakodowanÄ… sekwencjÄ™ ÅºrÃ³dÅ‚owÄ… z kodera.\nDekoder zaczyna od patrzenia na zakodowanÄ… sekwencjÄ™ ÅºrÃ³dÅ‚owÄ…, a takÅ¼e poczÄ…tkowy token â€œseedâ€ (taki jak ciÄ…g â€œ[start]â€) i uÅ¼ywa go do przewidzenia pierwszego prawdziwego tokena w sekwencji.\nPrzewidywana sekwencja jest przekazywana z powrotem do dekodera, ktÃ³ry generuje nastÄ™pny token i tak dalej, aÅ¼ do wygenerowania tokenu zatrzymania (takiego jak ciÄ…g â€œ[end]â€).\n\nZasadÄ™ dziaÅ‚ania takiego modelu przedstawimy na przykÅ‚adzie.\n\n7.9.1 PrzykÅ‚ad tÅ‚umaczenia maszynowego\nZademonstrujemy modelowanie sekwencja do sekwencji, w zadaniu tÅ‚umaczenia maszynowego. TÅ‚umaczenie maszynowe jest dokÅ‚adnie tym, do czego transformer zostaÅ‚ opracowany! Zaczniemy od rekurencyjnego modelu sekwencji, a nastÄ™pnie wykorzystamy peÅ‚nÄ… architekturÄ™ transformera. BÄ™dziemy pracowaÄ‡ z zestawem danych tÅ‚umaczeÅ„ z angielskiego na hiszpaÅ„ski dostÄ™pnym na stronie http://www.manythings.org/anki/. Pobierzmy go:\n\nKoddownload.file(\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n              destfile = \"data/spa-eng.zip\")\nzip::unzip(\"data/spa-eng.zip\", exdir = \"data/\")\n\n\nPlik tekstowy zawiera jeden przykÅ‚ad w wierszu: zdanie w jÄ™zyku angielskim, po ktÃ³rym nastÄ™puje znak tabulacji, a nastÄ™pnie odpowiadajÄ…ce mu zdanie w jÄ™zyku hiszpaÅ„skim. UÅ¼yjmy readr::read_tsv(), poniewaÅ¼ mamy wartoÅ›ci oddzielone tabulatorami:\n\nKodtext_file &lt;- \"data/spa-eng/spa.txt\"\ntext_pairs &lt;- text_file %&gt;%\n  readr::read_tsv(col_names = c(\"english\", \"spanish\"),\n                  col_types = c(\"cc\")) %&gt;%\n  within(spanish %&lt;&gt;% paste(\"[start]\", ., \"[end]\"))\n\n\nNasz przykÅ‚adowy tekst wyglÄ…da teraz tak:\n\nKodstr(text_pairs[sample(nrow(text_pairs), 1), ])\n\ntibble [1 Ã— 2] (S3: tbl_df/tbl/data.frame)\n $ english: chr \"That's just an excuse.\"\n $ spanish: chr \"[start] Esa solo es una excusa. [end]\"\n\n\nPrzetasujmy je i podzielmy na zwykÅ‚e zestawy treningowe, walidacyjne i testowe:\n\nKodnum_test_samples &lt;- num_val_samples &lt;-\n  round(0.15 * nrow(text_pairs))\nnum_train_samples &lt;- nrow(text_pairs) - num_val_samples - num_test_samples\n\npair_group &lt;- sample(c(\n  rep(\"train\", num_train_samples),\n  rep(\"test\", num_test_samples),\n  rep(\"val\", num_val_samples)\n))\n\ntrain_pairs &lt;- text_pairs[pair_group == \"train\", ]\ntest_pairs &lt;- text_pairs[pair_group == \"test\", ]\nval_pairs &lt;- text_pairs[pair_group == \"val\", ]\n\n\nNastÄ™pnie przygotujmy dwie oddzielne warstwy TextVectorization: jednÄ… dla jÄ™zyka angielskiego i jednÄ… dla hiszpaÅ„skiego. BÄ™dziemy musieli dostosowaÄ‡ sposÃ³b wstÄ™pnego przetwarzania ciÄ…gÃ³w znakÃ³w:\n\nMusimy zachowaÄ‡ wstawione przez nas tokeny â€œ[start]â€ i â€œ[end]â€. DomyÅ›lnie znaki [ i ] zostanÄ… usuniÄ™te, ale chcemy je zachowaÄ‡, abyÅ›my mogli odrÃ³Å¼niÄ‡ sÅ‚owo â€œstartâ€ od tokenu startowego â€œ[start]â€.\nInterpunkcja rÃ³Å¼ni siÄ™ w zaleÅ¼noÅ›ci od jÄ™zyka! W hiszpaÅ„skiej warstwie wektoryzacji tekstu, jeÅ›li zamierzamy usunÄ…Ä‡ znaki interpunkcyjne, musimy rÃ³wnieÅ¼ usunÄ…Ä‡ znak Â¿.\n\nZauwaÅ¼my, Å¼e w przypadku modelu tÅ‚umaczenia innego niÅ¼ toy-model, traktowalibyÅ›my znaki interpunkcyjne jako oddzielne tokeny, zamiast je usuwaÄ‡, poniewaÅ¼ chcielibyÅ›my byÄ‡ w stanie generowaÄ‡ poprawnie interpunkcyjne zdania. W naszym przypadku, dla uproszczenia, pozbÄ™dziemy siÄ™ caÅ‚ej interpunkcji.\nPrzygotowujemy niestandardowÄ… funkcjÄ™ standaryzacji ciÄ…gÃ³w znakÃ³w dla hiszpaÅ„skiej warstwy TextVectorization - zachowuje ona [ i ], ale usuwa Â¿, Â¡ i wszystkie inne znaki z klasy [:punct:].\n\nKodpunctuation_regex &lt;- \"[^[:^punct:][\\\\]]|[Â¡Â¿]\"\n\nlibrary(tensorflow)\ncustom_standardization &lt;- function(input_string) {\n  input_string %&gt;%\n    tf$strings$lower() %&gt;%\n    tf$strings$regex_replace(punctuation_regex, \"\")\n}\n\ninput_string &lt;- as_tensor(\"[start] Â¡corre! [end]\")\ncustom_standardization(input_string)\n\ntf.Tensor(b'[start] corre [end]', shape=(), dtype=string)\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nWyraÅ¼enia regularne TensorFlow rÃ³Å¼niÄ… siÄ™ nieznacznie w stosunku do silnika wyraÅ¼eÅ„ regularnych R. WiÄ™cej szczegÃ³Å‚Ã³w moÅ¼na znaleÅºÄ‡ pod adresem https://github.com/google/re2/wiki/Syntax.\n\n\n\nKodvocab_size &lt;- 15000\nsequence_length &lt;- 20\n\nsource_vectorization &lt;- layer_text_vectorization(\n  max_tokens = vocab_size,\n  output_mode = \"int\",\n  output_sequence_length = sequence_length\n)\n\ntarget_vectorization &lt;- layer_text_vectorization(\n  max_tokens = vocab_size,\n  output_mode = \"int\",\n  output_sequence_length = sequence_length + 1,\n  standardize = custom_standardization\n)\n\nadapt(source_vectorization, train_pairs$english)\nadapt(target_vectorization, train_pairs$spanish)\n\n\nNa koniec moÅ¼emy przeksztaÅ‚ciÄ‡ nasze dane w potok TF Dataset. Chcemy, aby zwracaÅ‚ on parÄ™ (inputs, target), gdzie inputs jest nazwanÄ… listÄ… z dwoma wpisami, angielskim zdaniem (wejÅ›cie kodera) i hiszpaÅ„skim zdaniem (wejÅ›cie dekodera), a target jest hiszpaÅ„skim zdaniem przesuniÄ™tym o jeden krok do przodu.\n\nKodformat_pair &lt;- function(pair) {\n  eng &lt;- source_vectorization(pair$english)\n  spa &lt;- target_vectorization(pair$spanish)\n\n  inputs &lt;- list(english = eng,\n                 spanish = spa[NA:-2])\n  targets &lt;- spa[2:NA]\n  list(inputs, targets)\n}\n\n\nbatch_size &lt;- 64\n\nlibrary(tfdatasets)\nmake_dataset &lt;- function(pairs) {\n    tensor_slices_dataset(pairs) %&gt;%\n    dataset_map(format_pair, num_parallel_calls = 4) %&gt;%\n    dataset_cache() %&gt;%\n    dataset_shuffle(2048) %&gt;%\n    dataset_batch(batch_size) %&gt;%\n    dataset_prefetch(16)\n}\ntrain_ds &lt;-  make_dataset(train_pairs)\nval_ds &lt;- make_dataset(val_pairs)\n\n\nOto jak wyglÄ…dajÄ… dane wyjÅ›ciowe naszego zestawu danych:\n\nKodc(inputs, targets) %&lt;-% iter_next(as_iterator(train_ds))\nstr(inputs)\n\nList of 2\n $ english:&lt;tf.Tensor: shape=(64, 20), dtype=int64, numpy=â€¦&gt;\n $ spanish:&lt;tf.Tensor: shape=(64, 20), dtype=int64, numpy=â€¦&gt;\n\nKodstr(targets)\n\n&lt;tf.Tensor: shape=(64, 20), dtype=int64, numpy=â€¦&gt;\n\n\nDane sÄ… teraz gotowe - czas zbudowaÄ‡ kilka modeli. Zaczniemy od rekurencyjnego modelu sekwencja-sekwencja, zanim przejdziemy do transformera.\n\n7.9.2 Uczenie sekwencyjne z wykorzystaniem sieci RNN\nRekurencyjne sieci neuronowe zdominowaÅ‚y uczenie sekwencyjne w latach 2015-2017, zanim zostaÅ‚y wyprzedzone przez transformery. ByÅ‚y one podstawÄ… wielu rzeczywistych systemÃ³w tÅ‚umaczenia maszynowego. Google Translate okoÅ‚o 2017 roku byÅ‚ zasilany przez stos siedmiu duÅ¼ych warstw LSTM. DziÅ› rÃ³wnieÅ¼ warto zapoznaÄ‡ siÄ™ z tym podejÅ›ciem, poniewaÅ¼ stanowi ono Å‚atwy punkt wejÅ›cia do zrozumienia modeli sekwencja-sekwencja.\nNajprostszym, naiwnym sposobem wykorzystania RNN do przeksztaÅ‚cenia sekwencji w innÄ… sekwencjÄ™ jest zachowanie danych wyjÅ›ciowych RNN w kaÅ¼dym kroku czasowym. W jÄ™zyku keras wyglÄ…daÅ‚oby to nastÄ™pujÄ…co:\n\nKodinputs &lt;- layer_input(shape = c(sequence_length), dtype = \"int64\")\noutputs &lt;- inputs %&gt;%\n  layer_embedding(input_dim = vocab_size, output_dim = 128) %&gt;%\n  layer_lstm(32, return_sequences = TRUE) %&gt;%\n  layer_dense(vocab_size, activation = \"softmax\")\nmodel &lt;- keras_model(inputs, outputs)\n\n\nPodejÅ›cie to ma jednak dwa gÅ‚Ã³wne problemy:\n\nSekwencja docelowa musi byÄ‡ zawsze tej samej dÅ‚ugoÅ›ci co sekwencja ÅºrÃ³dÅ‚owa. W praktyce rzadko ma to miejsce. Z technicznego punktu widzenia nie jest to istotne, poniewaÅ¼ zawsze moÅ¼na uzupeÅ‚niÄ‡ sekwencjÄ™ ÅºrÃ³dÅ‚owÄ… lub docelowÄ…, aby ich dÅ‚ugoÅ›ci siÄ™ zgadzaÅ‚y.\nZe wzglÄ™du na charakter przetwarzania zadania przez sieÄ‡ RNN krok po kroku, model bÄ™dzie patrzyÅ‚ tylko na tokeny \\(1\\ldots N\\) w sekwencji ÅºrÃ³dÅ‚owej, aby przewidzieÄ‡ token \\(N\\) w sekwencji docelowej. To ograniczenie sprawia, Å¼e ta konfiguracja nie nadaje siÄ™ do wiÄ™kszoÅ›ci zadaÅ„, w szczegÃ³lnoÅ›ci do tÅ‚umaczenia. RozwaÅ¼my tÅ‚umaczenie â€œThe weather is nice todayâ€ na francuski - byÅ‚oby to â€œIl fait beau aujourdâ€™huiâ€. MusiaÅ‚byÅ› byÄ‡ w stanie przewidzieÄ‡ â€œIlâ€ z samego â€œTheâ€, â€œIl faitâ€ z samego â€œThe weatherâ€ i tak dalej, co jest po prostu niemoÅ¼liwe.\n\nJeÅ›li jesteÅ›my tÅ‚umaczami, zaczynamy od przeczytania caÅ‚ego zdania ÅºrÃ³dÅ‚owego, zanim zaczniemy je tÅ‚umaczyÄ‡. Jest to szczegÃ³lnie waÅ¼ne, jeÅ›li mamy do czynienia z jÄ™zykami, ktÃ³re majÄ… bardzo rÃ³Å¼nÄ… kolejnoÅ›Ä‡ sÅ‚Ã³w, jak angielski i japoÅ„ski. I dokÅ‚adnie to robiÄ… standardowe modele sekwencja-sekwencja.\nW prawidÅ‚owej konfiguracji sekwencja-sekwencja (patrz Rys.Â 7.13), najpierw naleÅ¼y uÅ¼yÄ‡ RNN (kodera), aby przeksztaÅ‚ciÄ‡ caÅ‚Ä… sekwencjÄ™ ÅºrÃ³dÅ‚owÄ… w pojedynczy wektor (lub zestaw wektorÃ³w). MoÅ¼e to byÄ‡ ostatnie wyjÅ›cie RNN lub alternatywnie jego koÅ„cowe wektory stanu wewnÄ™trznego. NastÄ™pnie naleÅ¼y uÅ¼yÄ‡ tego wektora (lub wektorÃ³w) jako stanu poczÄ…tkowego innej sieci RNN (dekodera), ktÃ³ra przyjrzy siÄ™ elementom \\(1\\ldots N\\) w sekwencji docelowej i sprÃ³buje przewidzieÄ‡ krok \\(N+1\\) w sekwencji docelowej.\n\n\n\n\n\nRys.Â 7.13: Model RNN sekwencja-sekwencja. Koder RNN jest uÅ¼ywany do tworzenia wektora, ktÃ³ry koduje caÅ‚Ä… sekwencjÄ™ ÅºrÃ³dÅ‚owÄ…, ktÃ³ra jest uÅ¼ywana jako stan poczÄ…tkowy dla dekodera RNN.\n\n\nZaimplementujmy to w Keras za pomocÄ… koderÃ³w i dekoderÃ³w opartych na GRU. WybÃ³r GRU zamiast LSTM nieco upraszcza sprawÄ™, poniewaÅ¼ GRU ma tylko jeden wektor stanu, podczas gdy LSTM ma ich wiele. Zacznijmy od kodera.\n\nKodembed_dim &lt;- 256\nlatent_dim &lt;- 1024\n\nsource &lt;- layer_input(c(NA), dtype = \"int64\", name = \"english\")\nencoded_source &lt;- source %&gt;%\n  layer_embedding(vocab_size, embed_dim, mask_zero = TRUE) %&gt;%\n  bidirectional(layer_gru(units = latent_dim), merge_mode = \"sum\")\n\n\nNastÄ™pnie dodajmy dekoder - prostÄ… warstwÄ™ GRU, ktÃ³ra jako stan poczÄ…tkowy przyjmuje zakodowane zdanie ÅºrÃ³dÅ‚owe. Na wierzchu dodajemy layer_dense(), ktÃ³ra tworzy dla kaÅ¼dego kroku wyjÅ›ciowego rozkÅ‚ad prawdopodobieÅ„stwa w hiszpaÅ„skim sÅ‚ownictwie.\n\nKoddecoder_gru &lt;- layer_gru(units = latent_dim, return_sequences = TRUE)\n\npast_target &lt;- layer_input(shape = c(NA), dtype = \"int64\", name = \"spanish\")\ntarget_next_step &lt;- past_target %&gt;%\n  layer_embedding(vocab_size, embed_dim, mask_zero = TRUE) %&gt;%\n  decoder_gru(initial_state = encoded_source) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(vocab_size, activation = \"softmax\")\nseq2seq_rnn &lt;- keras_model(inputs = list(source, past_target),\n                           outputs = target_next_step)\n\n\nPodczas treningu dekoder przyjmuje jako dane wejÅ›ciowe caÅ‚Ä… sekwencjÄ™ docelowÄ…, ale dziÄ™ki krokowej naturze RNN, patrzy tylko na tokeny \\(1\\ldots N\\) na wejÅ›ciu, aby przewidzieÄ‡ token \\(N\\) na wyjÅ›ciu (co odpowiada nastÄ™pnemu tokenowi w sekwencji, poniewaÅ¼ wyjÅ›cie ma byÄ‡ przesuniÄ™te o jeden krok). Oznacza to, Å¼e uÅ¼ywamy tylko informacji z przeszÅ‚oÅ›ci do przewidywania przyszÅ‚oÅ›ci, tak jak powinniÅ›my; w przeciwnym razie oszukiwalibyÅ›my, a nasz model nie dziaÅ‚aÅ‚by w czasie wnioskowania.\n\nKodseq2seq_rnn %&gt;% compile(optimizer = \"rmsprop\",\n                        loss = \"sparse_categorical_crossentropy\",\n                        metrics = \"accuracy\")\n\nseq2seq_rnn %&gt;% fit(train_ds, epochs = 15, validation_data = val_ds)\n\n\nWybraliÅ›my accuracy jako niezbyt fortunny sposÃ³b monitorowania wydajnoÅ›ci zestawu walidacyjnego podczas treningu. Åšrednio model przewiduje nastÄ™pne sÅ‚owo w hiszpaÅ„skim zdaniu poprawnie w 63,2% przypadkÃ³w. Jednak w praktyce dokÅ‚adnoÅ›Ä‡ nastÄ™pnego tokenu nie jest optymalnÄ… miarÄ… dla modeli tÅ‚umaczenia maszynowego, w szczegÃ³lnoÅ›ci dlatego, Å¼e zakÅ‚ada, Å¼e prawidÅ‚owe tokeny docelowe od \\(0\\) do \\(N\\) sÄ… juÅ¼ znane podczas przewidywania tokenu \\(N+1\\). W rzeczywistoÅ›ci podczas wnioskowania generujemy zdanie docelowe od zera i nie moÅ¼emy polegaÄ‡ na tym, Å¼e wczeÅ›niej wygenerowane tokeny sÄ… w 100% poprawne. JeÅ›li pracujemy nad rzeczywistym systemem tÅ‚umaczenia maszynowego, prawdopodobnie uÅ¼yjemy metryki BLEU6 do oceny swoich modeli - metryki, ktÃ³ra analizuje caÅ‚e wygenerowane sekwencje i wydaje siÄ™ dobrze korelowaÄ‡ z ludzkim postrzeganiem jakoÅ›ci tÅ‚umaczenia.\n6Â BLEU (ang. BiLingual Evaluation Understudy) to wskaÅºnik sÅ‚uÅ¼Ä…cy do automatycznej oceny tekstu przetÅ‚umaczonego maszynowo. Wynik BLEU to liczba od zera do jednego, ktÃ³ra mierzy podobieÅ„stwo tekstu przetÅ‚umaczonego maszynowo do zestawu wysokiej jakoÅ›ci tÅ‚umaczeÅ„ referencyjnych. WartoÅ›Ä‡ 0 oznacza, Å¼e tekst przetÅ‚umaczony maszynowo nie pokrywa siÄ™ z tÅ‚umaczeniem referencyjnym (niska jakoÅ›Ä‡), podczas gdy wartoÅ›Ä‡ 1 oznacza, Å¼e tekst idealnie pokrywa siÄ™ z tÅ‚umaczeniem referencyjnym (wysoka jakoÅ›Ä‡).Na koniec uÅ¼yjmy naszego modelu do wnioskowania. Wybierzemy kilka zdaÅ„ z zestawu testowego i sprawdzimy, jak nasz model je tÅ‚umaczy. Zaczniemy od tokenu poczÄ…tkowego â€œ[start]â€ i wprowadzimy go do modelu dekodera wraz z zakodowanym angielskim zdaniem ÅºrÃ³dÅ‚owym. Pobierzemy predykcjÄ™ nastÄ™pnego tokena i ponownie wprowadzimy jÄ… do dekodera, prÃ³bkujÄ…c jeden nowy token docelowy w kaÅ¼dej iteracji, aÅ¼ dojdziemy do â€œ[end]â€ lub osiÄ…gniemy maksymalnÄ… dÅ‚ugoÅ›Ä‡ zdania.\n\nKodspa_vocab &lt;- get_vocabulary(target_vectorization)\nmax_decoded_sentence_length &lt;- 20\n\ndecode_sequence &lt;- function(input_sentence) {\n  tokenized_input_sentence &lt;-\n    source_vectorization(array(input_sentence, dim = c(1, 1)))\n  decoded_sentence &lt;- \"[start]\"\n  for (i in seq(max_decoded_sentence_length)) {\n    tokenized_target_sentence &lt;-\n      target_vectorization(array(decoded_sentence, dim = c(1, 1)))\n    next_token_predictions &lt;- seq2seq_rnn %&gt;%\n      predict(list(tokenized_input_sentence,\n                   tokenized_target_sentence))\n    sampled_token_index &lt;- which.max(next_token_predictions[1, i, ])\n    sampled_token &lt;- spa_vocab[sampled_token_index]\n    decoded_sentence &lt;- paste(decoded_sentence, sampled_token)\n    if (sampled_token == \"[end]\")\n      break\n  }\n  decoded_sentence\n}\n\nfor (i in seq(5)) {\n    input_sentence &lt;- sample(test_pairs$english, 1)\n    print(\"-\")\n    print(input_sentence)\n    print(decode_sequence(input_sentence))\n}\n\n[1] \"-\"\n[1] \"He went to school by car.\"\n1/1 - 1s - 1s/epoch - 1s/step\n1/1 - 0s - 33ms/epoch - 33ms/step\n1/1 - 0s - 34ms/epoch - 34ms/step\n1/1 - 0s - 33ms/epoch - 33ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 33ms/epoch - 33ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n[1] \"[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos\"\n[1] \"-\"\n[1] \"That is just typical of him.\"\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 26ms/epoch - 26ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 26ms/epoch - 26ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 26ms/epoch - 26ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n[1] \"[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos\"\n[1] \"-\"\n[1] \"Tom did well for a beginner.\"\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 26ms/epoch - 26ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n[1] \"[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos\"\n[1] \"-\"\n[1] \"It started a chain reaction.\"\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 42ms/epoch - 42ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n[1] \"[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos\"\n[1] \"-\"\n[1] \"Tomorrow is a holiday.\"\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n[1] \"[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos\"\n\n\ndecode_sequence() dziaÅ‚a dobrze, choÄ‡ byÄ‡ moÅ¼e nieco wolniej niÅ¼ byÅ›my chcieli. Jednym z Å‚atwych sposobÃ³w na przyspieszenie dziaÅ‚ania takiego kodu jest uÅ¼ycie funkcji tf_function(). Przepiszmy funkcjÄ™ decode_sentence() tak, aby byÅ‚a kompilowana przez tf_function(). Oznacza to, Å¼e zamiast uÅ¼ywaÄ‡ natywnych funkcji R, takich jak seq(), predict() i which.max(), uÅ¼yjemy odpowiednikÃ³w TensorFlow, takich jak tf$range(), wywoÅ‚ujÄ…c bezpoÅ›rednio model() i tf$argmax().\nPoniewaÅ¼ tf$range() i tf$argmax() zwracajÄ… wartoÅ›Ä‡ rÃ³wnÄ… 0, ustawimy lokalnÄ… opcjÄ™ funkcji: option(tensorflow.extract.style = \"python\"). Zmieni to zachowanie [ dla tensorÃ³w, aby rÃ³wnieÅ¼ startowaÅ‚y od 0.\n\nKodtf_decode_sequence &lt;- tf_function(function(input_sentence) {\n\n  withr::local_options(tensorflow.extract.style = \"python\")\n\n  tokenized_input_sentence &lt;- input_sentence %&gt;%\n    as_tensor(shape = c(1, 1)) %&gt;%\n    source_vectorization()\n\n  spa_vocab &lt;- as_tensor(spa_vocab)\n\n  decoded_sentence &lt;- as_tensor(\"[start]\", shape = c(1, 1))\n\n  for (i in tf$range(as.integer(max_decoded_sentence_length))) {\n\n    tokenized_target_sentence &lt;- decoded_sentence %&gt;%\n      target_vectorization()\n\n    next_token_predictions &lt;-\n      seq2seq_rnn(list(tokenized_input_sentence,\n                       tokenized_target_sentence))\n\n    sampled_token_index &lt;- tf$argmax(next_token_predictions[0, i, ])\n    sampled_token &lt;- spa_vocab[sampled_token_index]\n    decoded_sentence &lt;-\n      tf$strings$join(c(decoded_sentence, sampled_token),\n                      separator = \" \")\n\n    if (sampled_token == \"[end]\")\n      break\n  }\n\n  decoded_sentence\n\n})\n\nfor (i in seq(5)) {\n    input_sentence &lt;- sample(test_pairs$english, 1)\n    cat(\"-\\n\")\n    cat(input_sentence, \"\\n\")\n    cat(input_sentence %&gt;% as_tensor() %&gt;%\n          tf_decode_sequence() %&gt;% as.character(), \"\\n\")\n}\n\n-\nThat's amazing! \n[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos \n-\nHow many students are there in your university? \n[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos \n-\nIs everything ready? \n[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos \n-\nI like to play tennis. \n[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos \n-\nWhat kind of sports we play depends on the weather and the season. \n[start] nomÃ¡s caca matrimonio ningÃºn enormemente subas disculpe recorrimos tripa cenicienta cansada oÃ­mos composiciones promete promete colocar colocar atÃ³nito ahogarse modernos \n\n\nNasza funkcja tf_decode_sentence() jest okoÅ‚o 10 razy szybsza od wersji natywnej.\nNaleÅ¼y zauwaÅ¼yÄ‡, Å¼e ta konfiguracja wnioskowania, choÄ‡ bardzo prosta, jest raczej nieefektywna, poniewaÅ¼ ponownie przetwarzamy caÅ‚e zdanie ÅºrÃ³dÅ‚owe i caÅ‚e wygenerowane zdanie docelowe za kaÅ¼dym razem, gdy prÃ³bkujemy nowe sÅ‚owo. W praktycznym zastosowaniu koder i dekoder byÅ‚yby traktowane jako dwa oddzielne modele, a dekoder wykonywaÅ‚by tylko jeden krok w kaÅ¼dej iteracji prÃ³bkowania tokenÃ³w, ponownie wykorzystujÄ…c swÃ³j poprzedni stan wewnÄ™trzny.\nOto wyniki naszego tÅ‚umaczenia. Nasz model dziaÅ‚a przyzwoicie jak na model typu toy, choÄ‡ nadal popeÅ‚nia wiele podstawowych bÅ‚Ä™dÃ³w.\nIstnieje wiele sposobÃ³w na ulepszenie â€œzabawkowegoâ€ modelu: moglibyÅ›my uÅ¼yÄ‡ gÅ‚Ä™bokiego stosu warstw rekurencyjnych zarÃ³wno dla kodera, jak i dekodera (naleÅ¼y pamiÄ™taÄ‡, Å¼e w przypadku dekodera zarzÄ…dzanie stanem jest nieco bardziej skomplikowane). MoglibyÅ›my uÅ¼yÄ‡ LSTM zamiast GRU. I tak dalej. Poza takimi poprawkami, podejÅ›cie RNN do uczenia sekwencyjnego ma jednak kilka fundamentalnych ograniczeÅ„:\n\nReprezentacja sekwencji ÅºrÃ³dÅ‚owej musi byÄ‡ przechowywana w caÅ‚oÅ›ci w wektorze (wektorach) stanu kodera, co nakÅ‚ada znaczne ograniczenia na rozmiar i zÅ‚oÅ¼onoÅ›Ä‡ zdaÅ„, ktÃ³re moÅ¼na przetÅ‚umaczyÄ‡. To trochÄ™ tak, jakby czÅ‚owiek tÅ‚umaczyÅ‚ zdanie w caÅ‚oÅ›ci z pamiÄ™ci, nie patrzÄ…c dwa razy na zdanie ÅºrÃ³dÅ‚owe podczas tworzenia tÅ‚umaczenia.\nRNN majÄ… problem z radzeniem sobie z bardzo dÅ‚ugimi sekwencjami, poniewaÅ¼ majÄ… tendencjÄ™ do stopniowego zapominania o przeszÅ‚oÅ›ci - do czasu osiÄ…gniÄ™cia setnego tokena w dowolnej sekwencji, niewiele informacji pozostaje o poczÄ…tku sekwencji. Oznacza to, Å¼e modele oparte na RNN nie sÄ… w stanie utrzymaÄ‡ dÅ‚ugoterminowego kontekstu, ktÃ³ry moÅ¼e byÄ‡ niezbÄ™dny do tÅ‚umaczenia dÅ‚ugich dokumentÃ³w.\n\nOgraniczenia te sprawiÅ‚y, Å¼e spoÅ‚ecznoÅ›Ä‡ zajmujÄ…ca siÄ™ uczeniem maszynowym przyjÄ™Å‚a architekturÄ™ transformer do rozwiÄ…zywania problemÃ³w typu sekwencja-sekwencja. Przyjrzyjmy siÄ™ temu.\n\n7.9.3 Uczenie siÄ™ sekwencyjne z transformerem\nUczenie sekwencyjne to zadanie, w ktÃ³rym transformer naprawdÄ™ bÅ‚yszczy. Atencja neuronowa umoÅ¼liwia modelom transformer wÅ‚aÅ›ciwe przetwarzanie sekwencji, ktÃ³re sÄ… znacznie dÅ‚uÅ¼sze i bardziej zÅ‚oÅ¼one niÅ¼ te, z ktÃ³rymi radzÄ… sobie sieci RNN.\nJako czÅ‚owiek tÅ‚umaczÄ…cy z angielskiego na hiszpaÅ„ski, nie czytamy angielskiego zdania po jednym sÅ‚owie na raz, tylko zachowujemy jego znaczenie w pamiÄ™ci, a nastÄ™pnie generujemy hiszpaÅ„skie zdanie po jednym sÅ‚owie na raz. MoÅ¼e to zadziaÅ‚aÄ‡ w przypadku zdania skÅ‚adajÄ…cego siÄ™ z piÄ™ciu sÅ‚Ã³w, ale jest maÅ‚o prawdopodobne, by zadziaÅ‚aÅ‚o w przypadku caÅ‚ego akapitu. Zamiast tego, prawdopodobnie bÄ™dziemy chcieli przeskakiwaÄ‡ miÄ™dzy zdaniem ÅºrÃ³dÅ‚owym a tÅ‚umaczeniem i zwracaÄ‡ uwagÄ™ na rÃ³Å¼ne sÅ‚owa w ÅºrÃ³dle podczas zapisywania rÃ³Å¼nych czÄ™Å›ci tÅ‚umaczenia.\nDokÅ‚adnie to moÅ¼na osiÄ…gnÄ…Ä‡ za pomocÄ… atencji neuronowej i transformerÃ³w. Znamy juÅ¼ koder transformera, ktÃ³ry wykorzystuje samo-atencjÄ™ (w odmianie z wieloma gÅ‚owami) do tworzenia reprezentacji kontekstowych kaÅ¼dego tokena w sekwencji wejÅ›ciowej. W transformatorze sekwencja-sekwencja, koder transformera naturalnie odgrywaÅ‚by rolÄ™ kodera, ktÃ³ry odczytuje sekwencjÄ™ ÅºrÃ³dÅ‚owÄ… i tworzy jej zakodowanÄ… reprezentacjÄ™. Jednak w przeciwieÅ„stwie do naszego poprzedniego kodera RNN, koder transformer utrzymuje zakodowanÄ… reprezentacjÄ™ w formacie sekwencji: jest to sekwencja wektorÃ³w osadzania z uwzglÄ™dnieniem kontekstu.\nDruga czÄ™Å›Ä‡ modelu to dekoder transformera. Podobnie jak dekoder RNN, odczytuje on tokeny \\(1\\ldots N\\) w sekwencji docelowej i prÃ³buje przewidzieÄ‡ token \\(N + 1\\). Co najwaÅ¼niejsze, robiÄ…c to, wykorzystuje atencjÄ™ neuronowÄ…, aby zidentyfikowaÄ‡, ktÃ³re tokeny w zakodowanym zdaniu ÅºrÃ³dÅ‚owym sÄ… najbardziej powiÄ…zane z tokenem docelowym, ktÃ³ry obecnie prÃ³buje przewidzieÄ‡. Przypomnijmy model atencji posiadaÅ‚ query, key i value. W dekoderze transformerowym sekwencja docelowa sÅ‚uÅ¼y jako query, ktÃ³re jest uÅ¼ywane do zwracania wiÄ™kszej uwagi na rÃ³Å¼ne czÄ™Å›ci sekwencji ÅºrÃ³dÅ‚owej (sekwencja ÅºrÃ³dÅ‚owa odgrywa role zarÃ³wno key, jak i value).\n\n7.9.3.1 Dekoder transformerowy\nRys.Â 7.14 przedstawia peÅ‚ny transformator sekwencja do sekwencji. Przyjrzyjmy siÄ™ wewnÄ™trznym elementom dekodera. Rozpoznamy, Å¼e wyglÄ…da on bardzo podobnie do kodera transformera, z wyjÄ…tkiem tego, Å¼e dodatkowy blok atencji jest wstawiony pomiÄ™dzy blok samo-atencji zastosowany do sekwencji docelowej i gÄ™stych warstw bloku wyjÅ›ciowego.\n\n\n\n\n\nRys.Â 7.14: TransformerDecoder jest podobny do TransformerEncoder, z wyjÄ…tkiem tego, Å¼e zawiera dodatkowy blok atencji, w ktÃ³rym key i value sÄ… sekwencjÄ… ÅºrÃ³dÅ‚owÄ… zakodowanÄ… przez TransformerEncoder. Razem, koder i dekoder tworzÄ… transformator end-to-end.\n\n\nZaimplementujmy to. Podobnie jak w przypadku TransformerEncoder, bÄ™dziemy tworzyÄ‡ nowÄ… klasÄ™ warstw.\nMetoda call() jest prostym odwzorowaniem poÅ‚Ä…czeÅ„ diagramu z Rys.Â 7.14. Jest jednak dodatkowy szczegÃ³Å‚, ktÃ³ry musimy wziÄ…Ä‡ pod uwagÄ™ - przyczynowe wypeÅ‚nianie (ang. casual padding). WypeÅ‚nianie przyczynowe jest absolutnie krytyczne dla pomyÅ›lnego uczenia transformatora sekwencja do sekwencji. W przeciwieÅ„stwie do RNN, ktÃ³ry patrzy na swoje dane wejÅ›ciowe po jednym kroku na raz, a zatem bÄ™dzie miaÅ‚ dostÄ™p tylko do krokÃ³w \\(1\\ldots N\\), aby wygenerowaÄ‡ krok wyjÅ›ciowy \\(N\\) (ktÃ³ry jest tokenem \\(N+1\\) w sekwencji docelowej), natomiast TransformerDecoder jest niezaleÅ¼ny od kolejnoÅ›ci: patrzy na caÅ‚Ä… sekwencjÄ™ docelowÄ… naraz. Gdyby pozwoliÄ‡ mu na wykorzystanie caÅ‚ego wejÅ›cia, po prostu nauczyÅ‚by siÄ™ kopiowaÄ‡ krok wejÅ›ciowy \\(N+1\\) do lokalizacji \\(N\\) na wyjÅ›ciu. W ten sposÃ³b model osiÄ…gnÄ…Å‚by idealnÄ… dokÅ‚adnoÅ›Ä‡ treningu, ale oczywiÅ›cie podczas wnioskowania byÅ‚by caÅ‚kowicie bezuÅ¼yteczny, poniewaÅ¼ kroki wejÅ›ciowe poza \\(N\\) nie sÄ… dostÄ™pne.\nRozwiÄ…zanie jest proste - zamaskujemy gÃ³rnÄ… poÅ‚owÄ™ macierzy atencji parami, aby uniemoÅ¼liwiÄ‡ modelowi zwracanie uwagi na informacje z przyszÅ‚oÅ›ci - informacje tylko z tokenÃ³w \\(1\\ldots N\\) w sekwencji docelowej powinny byÄ‡ uÅ¼ywane podczas generowania tokena docelowego \\(N+1\\). Aby to zrobiÄ‡, dodamy metodÄ™ get_causal_attention_mask(inputs) do naszego TransformerDecoder, aby pobraÄ‡ maskÄ™ uwagi, ktÃ³rÄ… moÅ¼emy przekazaÄ‡ do naszych warstw MultiHeadAttention.\n\nKodlayer_transformer_decoder &lt;- new_layer_class(\n  classname = \"TransformerDecoder\",\n\n  initialize = function(embed_dim, dense_dim, num_heads, ...) {\n    super$initialize(...)\n    self$embed_dim &lt;- embed_dim\n    self$dense_dim &lt;- dense_dim\n    self$num_heads &lt;- num_heads\n    self$attention_1 &lt;- layer_multi_head_attention(num_heads = num_heads,\n                                                   key_dim = embed_dim)\n    self$attention_2 &lt;- layer_multi_head_attention(num_heads = num_heads,\n                                                   key_dim = embed_dim)\n    self$dense_proj &lt;- keras_model_sequential() %&gt;%\n      layer_dense(dense_dim, activation = \"relu\") %&gt;%\n      layer_dense(embed_dim)\n\n    self$layernorm_1 &lt;- layer_layer_normalization()\n    self$layernorm_2 &lt;- layer_layer_normalization()\n    self$layernorm_3 &lt;- layer_layer_normalization()\n    self$supports_masking &lt;- TRUE\n  },\n\n  get_config = function() {\n    config &lt;- super$get_config()\n    for (name in c(\"embed_dim\", \"num_heads\", \"dense_dim\"))\n      config[[name]] &lt;- self[[name]]\n    config\n  },\n\n  get_causal_attention_mask = function(inputs) {\n    c(batch_size, sequence_length, encoding_length) %&lt;-%\n      tf$unstack(tf$shape(inputs))\n\n    x &lt;- tf$range(sequence_length)\n    i &lt;- x[, tf$newaxis]\n    j &lt;- x[tf$newaxis, ]\n    mask &lt;- tf$cast(i &gt;= j, \"int32\")\n\n    tf$tile(mask[tf$newaxis, , ],\n            tf$stack(c(batch_size, 1L, 1L)))\n  },\n\n  call = function(inputs, encoder_outputs, mask = NULL) {\n\n    causal_mask &lt;- self$get_causal_attention_mask(inputs)\n\n    if (is.null(mask))\n      mask &lt;- causal_mask\n    else\n      mask %&lt;&gt;% { tf$minimum(tf$cast(.[, tf$newaxis, ], \"int32\"),\n                             causal_mask) }\n\n    inputs %&gt;%\n      { self$attention_1(query = ., value = ., key = .,\n                         attention_mask = causal_mask) + . } %&gt;%\n      self$layernorm_1() %&gt;%\n\n      { self$attention_2(query = .,\n                         value = encoder_outputs,\n                         key = encoder_outputs,\n                         attention_mask = mask) + . } %&gt;%\n      self$layernorm_2() %&gt;%\n\n      { self$dense_proj(.) + . } %&gt;%\n      self$layernorm_3()\n\n  }\n)\n\n\nBÄ™dziemy trenowaÄ‡ model transformator typu end-to-end. Mapuje on sekwencjÄ™ ÅºrÃ³dÅ‚owÄ… i docelowÄ… do sekwencji docelowej o jeden krok w przyszÅ‚oÅ›ci. W prosty sposÃ³b Å‚Ä…czy elementy, ktÃ³re zbudowaliÅ›my do tej pory: warstwy PositionalEmbedding, TransformerEncoder i TransformerDecoder. ZwrÃ³Ä‡my uwagÄ™, Å¼e zarÃ³wno TransformerEncoder, jak i TransformerDecoder sÄ… niezmienne pod wzglÄ™dem ksztaÅ‚tu7, wiÄ™c moÅ¼emy poÅ‚Ä…czyÄ‡ wiele z nich, aby stworzyÄ‡ bardziej wydajny koder lub dekoder. W naszym przykÅ‚adzie bÄ™dziemy trzymaÄ‡ siÄ™ pojedynczej instancji kaÅ¼dego z nich.\n7Â wejÅ›cie i wyjÅ›cie jest tych samych rozmiarÃ³w\nKodembed_dim &lt;- 256\ndense_dim &lt;- 2048\nnum_heads &lt;- 8\n\nencoder_inputs &lt;- layer_input(shape(NA), dtype = \"int64\", name = \"english\")\nencoder_outputs &lt;- encoder_inputs %&gt;%\n  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %&gt;%\n  layer_transformer_encoder(embed_dim, dense_dim, num_heads)\n\ntransformer_decoder &lt;-\n  layer_transformer_decoder(NULL, embed_dim, dense_dim, num_heads)\n\ndecoder_inputs &lt;-  layer_input(shape(NA), dtype = \"int64\", name = \"spanish\")\ndecoder_outputs &lt;- decoder_inputs %&gt;%\n  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %&gt;%\n  transformer_decoder(., encoder_outputs) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(vocab_size, activation=\"softmax\")\n\ntransformer &lt;- keras_model(list(encoder_inputs, decoder_inputs),\n                           decoder_outputs)\n\n\n\nKodtransformer %&gt;%\n  compile(optimizer = \"rmsprop\",\n          loss = \"sparse_categorical_crossentropy\",\n          metrics = \"accuracy\")\n\ntransformer %&gt;%\n  fit(train_ds, epochs = 30, validation_data = val_ds)\n\nsave_model_tf(transformer, filepath = \"models/end_to_end_transformer.keras\")\n\n\nOsiÄ…gamy dokÅ‚adnoÅ›Ä‡ 65,3%, czyli istotnie lepiej niÅ¼ z modelu opartego na GRU.\nNa koniec sprÃ³bujmy uÅ¼yÄ‡ naszego modelu do przetÅ‚umaczenia nigdy wczeÅ›niej nie widzianych angielskich zdaÅ„ z zestawu testowego. Konfiguracja jest identyczna z tÄ…, ktÃ³rej uÅ¼yliÅ›my do modelu RNN typu sekwencja-sekwencja. ZastÄ…pimy tylko seq2seq_rnn transformatorem i usuniemy dodatkowego tokena, ktÃ³ry skonfigurowaliÅ›my do dodania w warstwie target_vectorization().\n\nKodtransformer &lt;- load_model_tf(\n  \"models/end_to_end_transformer.keras\",\n  custom_objects = list(\n    layer_positional_embedding,\n    layer_transformer_decoder,\n    layer_transformer_encoder\n  )\n)\n\n\n\nKodtf_decode_sequence &lt;- tf_function(function(input_sentence) {\n  withr::local_options(tensorflow.extract.style = \"python\")\n\n  tokenized_input_sentence &lt;- input_sentence %&gt;%\n    as_tensor(shape = c(1, 1)) %&gt;%\n    source_vectorization()\n  spa_vocab &lt;- as_tensor(spa_vocab)\n  decoded_sentence &lt;- as_tensor(\"[start]\", shape = c(1, 1))\n\n  for (i in tf$range(as.integer(max_decoded_sentence_length))) {\n\n    tokenized_target_sentence &lt;-\n      target_vectorization(decoded_sentence)[,NA:-1]\n\n    next_token_predictions &lt;-\n      transformer(list(tokenized_input_sentence,\n                       tokenized_target_sentence))\n\n    sampled_token_index &lt;- tf$argmax(next_token_predictions[0, i, ])\n    sampled_token &lt;- spa_vocab[sampled_token_index]\n    decoded_sentence &lt;-\n      tf$strings$join(c(decoded_sentence, sampled_token),\n                      separator = \" \")\n\n    if (sampled_token == \"[end]\")\n      break\n  }\n\n  decoded_sentence\n\n})\n\nfor (i in seq(20)) {\n\n    c(input_sentence, correct_translation) %&lt;-%\n      test_pairs[sample.int(nrow(test_pairs), 1), ]\n    cat(\"-\\n\")\n    cat(input_sentence, \"\\n\")\n    cat(correct_translation, \"\\n\")\n    cat(input_sentence %&gt;% as_tensor() %&gt;%\n          tf_decode_sequence() %&gt;% as.character(), \"\\n\")\n}\n\n-\nThis book is new. \n[start] Este libro es nuevo. [end] \n[start] se ha estuvo dÃ©jame no es [end] \n-\nThe coffee was so hot that I nearly burned my tongue. \n[start] El cafÃ© estaba tan caliente que casi me quemÃ© la lengua. [end] \n[start] no sabÃ­a que nada sea vos de que [UNK] de francÃ©s [UNK] [end] \n-\nHaven't I already told you about this before? \n[start] Â¿No te lo he dicho ya antes? [end] \n[start] sin fue tengo que te habrÃ­a [end] \n-\nI thought Tom had a dog. \n[start] PensÃ© que Tom tenÃ­a un perro. [end] \n[start] quiere cuando estuvo tom [end] \n-\nI am hanging up a picture of my grandmother. \n[start] Estoy colgando una foto de mi abuela. [end] \n[start] yo pero de la habÃ­a [end] \n-\nTom didn't know that Mary liked to cook. \n[start] Tom no sabÃ­a que a Mary le gustaba cocinar. [end] \n[start] tom no sabÃ­a que para tenÃ­a estÃ¡is estaba japÃ³n [end] \n-\nIt's difficult to understand his theory. \n[start] Es difÃ­cil entender su teorÃ­a. [end] \n[start] deberÃ­as se ha estuvo a mi casi discusiÃ³n [end] \n-\nPlease say a prayer for those who were onboard. \n[start] Por favor diga una plegaria por aquellos que estuvieron a bordo. [end] \n[start] por todos equivocado en sueÃ±o de fÃ¡cil se [UNK] [end] \n-\nI am never free on Sundays. \n[start] Nunca estoy libre los domingos. [end] \n[start] yo tengo mayor diste por vi diste [end] \n-\nWhy did you call me at this unearthly hour? \n[start] Â¿Para quÃ© me llamas a esta hora imprudente? [end] \n[start] de nuestro te [UNK] antes que todo tengo usar [end] \n-\nHe drinks a lot of milk every day. \n[start] Ã‰l toma mucha leche todos los dÃ­as. [end] \n[start] su eso niÃ±o hablÃ³ he pronto tan mi tren [end] \n-\nHe's three years older than she is. \n[start] Ã‰l tiene tres aÃ±os mÃ¡s que ella. [end] \n[start] tan mi perder pie hace [end] \n-\nI feel like throwing up. \n[start] Tengo ganas de vomitar. [end] \n[start] me policÃ­a en mira [end] \n-\nTom worked as a farmer. \n[start] TomÃ¡s trabajaba como granjero. [end] \n[start] tom preguntaba a un piensas [UNK] [end] \n-\nMy father likes fishing, and so do I. \n[start] A mi padre le gusta pescar y a mÃ­ tambiÃ©n. [end] \n[start] el toda bÃ©isbol [UNK] y algunas [end] \n-\nTom is deeply in love with Mary. \n[start] Tom estÃ¡ perdidamente enamorado de Mary. [end] \n[start] tom es posible en mis estas para [end] \n-\nThe man slumped to the floor. \n[start] El hombre se derrumbÃ³ sobre el suelo. [end] \n[start] se te buscando era [end] \n-\nMove along now. \n[start] MuÃ©vase ya. [end] \n[start] dijo estÃ¡s madre ojos [end] \n-\nDo you want to kill me? \n[start] Â¿Quieres matarme? [end] \n[start] desde a este terminado [end] \n-\nThis is not enough. \n[start] No es suficiente. [end] \n[start] trabajo no se ha estuvo [end] \n\n\nNa tym koÅ„czy siÄ™ ten rozdziaÅ‚ o przetwarzaniu jÄ™zyka naturalnego. W ramach niego przeszliÅ›my od podstaw do w rozwiÄ…zania wspÃ³Å‚czeÅ›nie uÅ¼ywanego, czyli Transformera, ktÃ³ry moÅ¼e tÅ‚umaczyÄ‡ zdania z angielskiego na hiszpaÅ„ski.\n\n\n\n\nBengio, Yoshua, RÃ©jean Ducharme, Pascal Vincent, i Christian Jauvin. b.d. â€A Neural Probabilistic Language Modelâ€.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, i Illia Polosukhin. b.d. â€Attention Is All You Needâ€. https://doi.org/10.48550/arXiv.1706.03762.",
    "crumbs": [
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Modele jÄ™zykowe</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Bengio, Yoshua, RÃ©jean Ducharme, Pascal Vincent, and Christian Jauvin.\nn.d. â€œA Neural Probabilistic Language Model.â€\n\n\nBengio, Y., P. Simard, and P. Frasconi. 1994. â€œLearning Long-Term\nDependencies with Gradient Descent Is Difficult.â€ IEEE\nTransactions on Neural Networks 5 (2): 157â€“66. https://doi.org/10.1109/72.279181.\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, and Pedro LarraÃ±aga.\n2015. â€œA Survey on Multi-Output Regression.â€ WIREs Data\nMining and Knowledge Discovery 5 (5): 216â€“33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, and Charles J. Stone.\n2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua\nBengio. n.d. â€œOn the Properties of Neural Machine Translation:\nEncoder-Decoder Approaches.â€ https://doi.org/10.48550/arXiv.1409.1259.\n\n\nEvgeniou, Theodoros, and Massimiliano Pontil. 2004. â€œRegularized\nMultiâ€“Task Learning.â€ Proceedings of the Tenth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\nAugust. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, RÃ©mi Gilleron, and Fabien\nTorre. 2012. â€œLearning Multiple Tasks with Boosted Decision\nTrees.â€ In Proceedings of the 2012th European Conference on\nMachine Learning and Knowledge Discovery in Databases - Volume Part\ni, 681â€“96. ECMLPKDDâ€™12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, and Remi\nGilleron. 2010. â€œBoosting Multi-Task Weak Learners with\nApplications to Textual and Social Data.â€ In 2010 Ninth\nInternational Conference on Machine Learning and Applications,\n367â€“72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nGlocker, Ben, Olivier Pauly, Ender Konukoglu, and Antonio Criminisi.\n2012. â€œJoint Classification-Regression Forests for Spatially\nStructured Multi-Object Segmentation.â€ In Computer Vision â€“\nECCV 2012, edited by Andrew Fitzgibbon, Svetlana Lazebnik, Pietro\nPerona, Yoichi Sato, and Cordelia Schmid, 7575:870â€“81. Springer Berlin\nHeidelberg. http://link.springer.com/10.1007/978-3-642-33765-9_62.\n\n\nHochreiter, Sepp, and JÃ¼rgen Schmidhuber. 1997. â€œLong\nShort-Term Memory.â€ Neural Computation 9\n(8): 1735â€“80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nIzenman, Alan Julian. 1975. â€œReduced-Rank Regression for the\nMultivariate Linear Model.â€ Journal of Multivariate\nAnalysis 5 (2): 248â€“64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, and SaÅ¡o DÅ¾eroski. 2013.\nâ€œTree Ensembles for Predicting Structured Outputs.â€\nPattern Recognition 46 (3): 817â€“33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, and SebastiÃ¡n Ventura.\n2017. â€œMulti-Target Support Vector Regression via Correlation\nRegressor Chains.â€ Information Sciences 415â€“416\n(November): 53â€“69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning.\nMorgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello\nMastelini, Fabio Luiz Melquiades, and Sylvio Barbon Jr. 2020.\nâ€œImproved Prediction of Soil Properties with Multi-Target Stacked\nGeneralisation on EDXRF Spectra.â€ arXiv Preprint\narXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. â€œTree-Structured Methods for\nLongitudinal Data.â€ Journal of the American Statistical\nAssociation 87 (418): 407â€“18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves,\nand Ioannis Vlahavas. 2016. â€œMulti-Target Regression\nvia Input Space Expansion: Treating Targets as\nInputs.â€ Machine Learning 104 (1): 55â€“98.\nhttps://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, and SaÅ¡o DÅ¾eroski. 2006. â€œConstraint Based Induction\nof Multi-Objective Regression Trees.â€ In Knowledge Discovery\nin Inductive Databases, edited by Francesco Bonchi and\nJean-FranÃ§ois Boulicaut, 222â€“33. Lecture Notes in Computer Science.\nSpringer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, and Victor Sheng. 2013. â€œEmpirical Comparison of\nMulti-Label Classification Algorithms.â€ In Proceedings of the\nAAAI Conference on Artificial\nIntelligence, 27:1645â€“46.\n\n\nTsoumakas, Grigorios, and Ioannis Katakis. 2007. â€œMulti-Label\nClassification: An Overview.â€ International\nJournal of Data Warehousing and Mining (IJDWM) 3 (3): 1â€“13.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. n.d.\nâ€œAttention Is All You Need.â€ https://doi.org/10.48550/arXiv.1706.03762.\n\n\nVazquez, Emmanuel, and Eric Walter. 2003. â€œMulti-Output Suppport\nVector Regression.â€ IFAC Proceedings Volumes, 13th IFAC\nsymposium on system identification (SYSID 2003), rotterdam, the\nnetherlands, 27-29 august, 2003, 36 (16): 1783â€“88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, and Laisheng Wang. 2013. â€œA Twin Multi-Class\nClassification Support Vector Machine.â€ Cognitive\nComputation 5 (4): 580â€“88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, and Cheolkon Jung. n.d. â€œGBDT-MO: Gradient\nBoosted Decision Trees for Multiple Outputs.â€ https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "Literatura"
    ]
  }
]