[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaawansowane metody uczenia maszynowego",
    "section": "",
    "text": "Wstęp\nKsiążka ta jest napisana na potrzeby prowadzenia zajęć na kierunku Inżynieria i analiza danych z przedmiotu Zaawansowane metody uczenia maszynowego. Jest swego rodzaju autorskim podejściem do tematu, przedstawiającym wybrane metody uczenia maszynowego, które rzadziej występują w opracowaniach na temat uczenia maszynowego.\nUczenie maszynowe stanowi obszar intensywnego rozwoju, który obejmuje szereg technik umożliwiających bardziej skomplikowane i wydajne modele predykcyjne. Wśród tych metod warto wyróżnić głębokie sieci neuronowe, zwłaszcza konwolucyjne sieci neuronowe (CNN) i rekurencyjne sieci neuronowe (RNN). CNN są wykorzystywane w zadaniach przetwarzania obrazów, gdzie potrafią efektywnie ekstrahować hierarchiczne cechy z danych wejściowych, natomiast RNN są efektywne w analizie sekwencji danych, takich jak język naturalny. Ponadto, metody uczenia maszynowego obejmują techniki transferu wiedzy, uczenie ze wzmocnieniem, generatywne modele, takie jak generatywne sieci przeciwdziedzinowe (GAN), czy też autokodery. Te nowoczesne podejścia umożliwiają modelom uczącym się wykonywanie bardziej złożonych zadań, a także adaptację do różnorodnych danych wejściowych, co sprawia, że są one stosowane w obszarach takich jak rozpoznawanie obrazów, przetwarzanie języka naturalnego, czy nawet w autonomicznych systemach decyzyjnych.\nPonadto, zaawansowane metody uczenia maszynowego obejmują także techniki regularyzacji, optymalizacji i inżynierię cech. Regularyzacja ma na celu zapobieganie przeuczeniu poprzez kontrolowanie złożoności modelu, natomiast optymalizacja skupia się na dostosowywaniu wag modelu w celu minimalizacji funkcji straty. Inżynieria cech polega na ręcznym lub automatycznym dostosowywaniu danych wejściowych w celu uzyskania lepszych wyników modelu. Dzięki tym zaawansowanym metodom, uczenie maszynowe staje się coraz bardziej potężnym narzędziem w analizie danych i podejmowaniu skomplikowanych decyzji w różnych dziedzinach.\nModele predykcyjne dla wielu wyjść, czyli tzw. multi-target regression and classification, stanowią kolejny istotny obszar w dziedzinie uczenia maszynowego. W przypadku multi-target regression, celem jest przewidywanie wielu wartości wyjściowych dla danego zestawu wejściowego, co często spotyka się w złożonych problemach predykcyjnych, takich jak prognozowanie wielu parametrów jednocześnie. Z kolei w przypadku multi-target classification, model ma za zadanie przypisanie jednego lub więcej klas dla każdego przykładu wejściowego. Te modele są powszechnie stosowane w różnych dziedzinach, takich jak bioinformatyka, finanse czy przemysł, gdzie jednoczesne przewidywanie wielu zmiennych jest kluczowe dla skutecznego rozwiązania problemu. Wdrożenie takich zaawansowanych modeli predykcyjnych wymaga starannej obróbki danych, odpowiedniego dostosowania architektury modelu oraz precyzyjnej oceny wyników, co sprawia, że są one istotnym narzędziem w obszarze analizy danych i podejmowania decyzji.\nModele językowe stanowią jeszcze jeden kluczowy obszar w dziedzinie uczenia maszynowego, skoncentrowany na zrozumieniu i generowaniu ludzkiego języka naturalnego. Głębokie sieci neuronowe, zwłaszcza rekurencyjne sieci neuronowe (RNN) i transformery, zostały skutecznie wykorzystane do tworzenia modeli językowych o zdolnościach przetwarzania i generowania tekstu na poziomie zbliżonym do ludzkiego. Te modele zdolne są do zrozumienia kontekstu, analizy gramatyki, a także generowania spójnych i sensownych odpowiedzi. Wykorzystywane są w różnorodnych zastosowaniach, takich jak tłumaczenie maszynowe, generowanie tekstu, czy analiza nastroju w tekście. Ponadto, pre-trenowane modele językowe, takie jak BERT czy GPT (Generative Pre-trained Transformer), zdobywają popularność, umożliwiając dostosowanie ich do różnych zadań poprzez fine-tuning. W miarę postępu badań i rozwoju w tej dziedzinie, modele językowe stają się coraz bardziej zaawansowane, co przyczynia się do doskonalenia komunikacji między maszynami a ludźmi oraz do rozwijania nowych możliwości w dziedzinie przetwarzania języka naturalnego.\nWspomniane powyżej metody i modele będą stanowić treść wykładów z wspomnianego na wstępie przedmiotu.",
    "crumbs": [
      "Wstęp"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "Witam w świecie zaawansowanych metod uczenia maszynowego 🤖, prezentowanej w niniejszej publikacji. Książka ta skupia się na trzech głównych obszarach, zaczynając od wielowymiarowych problemów predykcyjnych, przechodząc przez kompleksowe modele głębokich sieci neuronowych, a kończąc na zaawansowanych modelach językowych. Koncepcyjnie rozpoczniemy od omówienia multiple target regression and classification, gdzie przedstawimy skomplikowane zadania predykcyjne wymagające jednoczesnej prognozy wielu zmiennych. Przeanalizujemy praktyczne zastosowania tych modeli w obszarach, takich jak nauki społeczne, biologia i finanse.\nNastępnie poświęcimy uwagę głębokim sieciom neuronowym, głównemu filarowi nowoczesnej sztucznej inteligencji. Omówimy ewolucję od konwolucyjnych sieci neuronowych (CNN) do rekurencyjnych sieci neuronowych (RNN), zwracając uwagę na ich zdolność do efektywnego przetwarzania obrazów, sekwencji danych i rozwiązania bardziej złożonych problemów. W ramach tego obszaru, przyjrzymy się również technikom transferu wiedzy, uczenia ze wzmocnieniem oraz generatywnym modelom, takim jak generatywne sieci przeciwdziedzinowe (GAN), które poszerzają granice możliwości maszynowego uczenia się.\n\n\n\nTrzeci kluczowy obszar, który będzie przedmiotem analizy, to modele językowe. Rozważania rozpoczniemy od głębokich sieci neuronowych, a następnie skoncentrujemy się na transformatorach, które rewolucjonizują przetwarzanie języka naturalnego 👅. Przedstawimy praktyczne zastosowania tych modeli, zwłaszcza w tłumaczeniu maszynowym, generowaniu tekstu i analizie sentymentu. Ponadto, omówimy pre-trenowane modele językowe, takie jak BERT czy GPT, jako kluczowe narzędzia adaptacyjne, zdolne do fine-tuningu w zależności od konkretnego zadania.\nKażdy podejmowany temat będzie wzbogacony o implementację analizowanych metod w realnych scenariuszach. Omówimy kroki od obróbki danych, przez dostosowywanie architektury modelu, aż po ocenę wyników. W tym kontekście poruszymy także aspekty etyczne i wyzwania związane z zastosowaniem zaawansowanych modeli uczenia maszynowego.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html",
    "href": "multi_target_models.html",
    "title": "2  Modele z wieloma wyjściami",
    "section": "",
    "text": "2.1 Typy modeli z wieloma zmiennymi wynikowymi\nWśród nadzorowanych modeli uczenia maszynowego z wieloma zmiennymi wynikowymi można wymienić zarówno te dedykowane do klasyfikacji, jak i regresji. Modele te są znane jako modele z wieloma wyjściami (klasyfikacyjne) lub modele z wieloma wyjściami (regresyjne), w zależności od rodzaju problemu, który rozwiązują.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "href": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "title": "2  Modele z wieloma wyjściami",
    "section": "",
    "text": "Modele z wieloma wyjściami (klasyfikacyjne)\nW przypadku klasyfikacji, gdy mamy wiele kategorii (klas) jako zmienną wynikową, modele te są nazywane modelami z wieloma wyjściami. Przykłady obejmują algorytmy takie jak regresja logistyczna, metoda k najbliższych sąsiadów (k-NN) czy algorytmy drzew decyzyjnych, które zostały dostosowane do obsługi wielu klas.\nPrzykładowe zadanie: Załóżmy, że mamy zbiór danych dotyczący różnych rodzajów owoców (np. jabłek, pomarańczy, bananów) i chcemy stworzyć model, który jednocześnie przewiduje gatunek owocu oraz kolor owocu. Mamy więc dwie zmienne wynikowe: gatunek (klasyfikacja wieloklasowa) i kolor (klasyfikacja wieloklasowa).\nModele z wieloma wyjściami (regresyjne).\nW przypadku regresji, gdzie zmienną wynikową jest wektor wartości numerycznych, modele te są nazywane modelami z wieloma wyjściami. Przykłady obejmują algorytmy regresji liniowej lub nieliniowej, algorytmy oparte na drzewach decyzyjnych, czy też bardziej zaawansowane modele, takie jak sieci neuronowe.\nPrzykładowe zadanie: Zakładamy, że mamy zbiór danych zawierający informacje o pracownikach, takie jak doświadczenie zawodowe, poziom wykształcenia, liczba godzin pracy tygodniowo itp. Chcemy stworzyć model, który jednocześnie przewiduje zarobki pracowników oraz ich poziom satysfakcji zawodowej.\nModele wielozadaniowe.\nModele wielozadaniowe to rodzaj nadzorowanego uczenia maszynowego, w którym model jest trenowany jednocześnie do rozwiązania kilku zadań. Te zadania mogą obejmować zarówno klasyfikację, jak i regresję. Dzięki wspólnemu trenowaniu modelu na wielu zadaniach, można uzyskać korzyści w postaci wspólnego wykorzystywania wiedzy między zadaniami.\nPrzykładowe zadanie: Załóżmy, że mamy zbiór danych dotyczący zakupów klientów w sklepie internetowym. Dla każdego klienta mamy informacje o różnych aspektach zakupów, takich jak czas dostawy, łatwość obsługi strony, jakość produktów itp. Chcemy stworzyć model, który jednocześnie przewiduje dwie zmienne wynikowe: jakość obsługi klienta (skala jakościowa, np. “Niska”, “Średnia”, “Wysoka”) oraz całkowity wydatek klienta (zmienna ilościowa, np. kwota zakupów).\nModele hierarchiczne.\nW niektórych przypadkach, szczególnie gdy mamy hierarchię zmiennych wynikowych, modele te mogą być budowane w sposób hierarchiczny. Przykładowo, w problemie klasyfikacji obrazów z hierarchią kategorii (na przykład rozpoznawanie gatunków zwierząt), model może być zaprojektowany do rozpoznawania zarówno ogólnych, jak i bardziej szczegółowych kategorii.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#różnie-podejścia-do-modelowania-z-wieloma-wyjściami",
    "href": "multi_target_models.html#różnie-podejścia-do-modelowania-z-wieloma-wyjściami",
    "title": "2  Modele z wieloma wyjściami",
    "section": "2.2 Różnie podejścia do modelowania z wieloma wyjściami",
    "text": "2.2 Różnie podejścia do modelowania z wieloma wyjściami\nIstnieją dwa ogólne podejścia do rozwiązywania problemów wieloetykietowych: transformacja problemu i adaptacja algorytmu. Transformacja problemu polega na manipulowaniu zbiorem danych w taki sposób, że problem wieloetykietowy staje się jednym lub kilkoma problemami jednoetykietowymi (Tawiah i Sheng 2013). Adaptacja algorytmu polega na tym, że sam algorytm jest w stanie poradzić sobie bezpośrednio z problemem wieloetykietowym. Okazuje się, że wiele, choć nie wszystkie, metody adaptacji algorytmów metod adaptacji algorytmów w rzeczywistości wykorzystuje transformację problemu (Tsoumakas i Katakis 2007).\n\n2.2.1 Transformacja problemu\nTechniki te przewidują stworzenie indywidualnego modelu dla każdego celu, a następnie połączenie oddzielnych modeli w celu uzyskania ogólnej prognozy. Metody transformacji problemów okazały się lepsze od metod adaptacji algorytmów pod względem dokładności (Spyromitros-Xioufis i in. 2016). Co więcej, podstawowa zasada sprawia, że metody transformacji problemu są niezależne od algorytmu. W konsekwencji, można je łatwo dostosować do danego problemu poprzez zastosowanie odpowiednich bazowych metod uczących. Punkt ten ma również szczególne znaczenie dla modeli typu ensemble, które łączą oszacowania z wielu potencjalnie różnych algorytmów w ostateczną prognozę. Niedawno Spyromitros-Xioufis i in. (2016) zaproponowali rozszerzenie znanych metod transformacji klasyfikacji wieloetykietowej, aby poradzić sobie z problemem regresji wielowynikowej i modelować zależności między celami. W szczególności wprowadzili oni dwa nowe podejścia do regresji wielocelowej, składanie regresorów wielocelowych i łańcuchy regresorów, inspirowane popularnymi i skutecznymi podejściami do klasyfikacji wieloznaczeniowej.\nPodstawową koncepcją w metodach transformacji problemów jest wykorzystanie poprzednich modeli do nowego przewidywania poprzez rozszerzoną przestrzeń cech (Borchani i in. 2015). Stacked generalization to podejście do meta-uczenia, które wykorzystuje dane wyjściowe wcześniej wyuczonych modeli do uczenia się nowego modelu. W związku z tym początkowe dane wyjściowe modelu są traktowane jako nowe cechy i są układane w stos do początkowego wektora cech przed ponownym uczeniem. W oryginalnym sformułowaniu przewidziano tylko dwuetapową procedurę, tj. początkowe modele wyuczone z początkowego wektora cech odpowiadają odpowiednio modelom i danym poziomu 0, a powiększony wektor cech i ponownie wyuczony model są określane odpowiednio jako dane poziomu 1 i generalizator. Jednakże, rozsądnie rzecz biorąc, ten proces układania pojedynczego celu (ang. Single-target Stacking - STS) może być również przeprowadzany w wielu iteracjach. Aby wdrożyć tę zasadę dla problemów z wieloma celami, w których kodowane są również możliwe korelacje między zmiennymi docelowymi, wprowadzono koncepcję układania wielu celów (ang. Multi-target Stacking - MTS) (Borchani i in. 2015). Analogicznie do STS, szkolenie modelu MTS można uznać za procedurę dwuetapową. W pierwszym etapie uczone są niezależne modele dla każdej zmiennej docelowej. Następnie uczone są meta-modele dla każdej zmiennej docelowej z rozszerzonymi wektorami cech, które zawierają początkowe wektory cech, a także oszacowania poziomu 0 pozostałych zmiennych docelowych. Podobne pomysły były również stosowane w kontekście modeli zespołowych, tj. uczenia się kilku modeli poziomu 0 dla każdej zmiennej docelowej, które są łączone w procedurze uogólniania poziomu 1 dla wielu zmiennych docelowych (Santana i in. 2020).\n\n2.2.1.1 Single-target stacking\nMetoda ta jest stosowana przede wszystkim z zadaniach regresyjnych z wieloma wyjściami. Rozważmy zbiór danych \\(D = \\left\\{\\left(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(N)}, \\mathbf{y}^{(N)}\\right)\\right\\}\\), składający się z \\(N\\) obserwacji, które są realizacjami zmiennych losowych \\(X_1,\\ldots,X_m, Y_1,\\ldots,Y_d\\). Zatem każde wejście do modelu jest charakteryzowane przez \\(m\\) zmiennych \\(\\mathbf{x}{(l)}=\\left(x_1^{(l)},\\ldots, x_j^{(l)}, \\ldots, x_m^{(l)} \\right)\\) oraz \\(d\\) odpowiadających im wyjść \\(\\mathbf{y}{(l)}=\\left(y_1^{(l)},\\ldots, y_i^{(l)}, \\ldots, y_d^{(l)} \\right)\\), gdzie \\(l\\in\\{1,\\ldots,N\\}, j\\in\\{1,\\ldots,m\\}, i\\in\\{1,\\ldots,d\\}\\). Naszym celem w zadaniu regresyjnym (MTR - Multi-target Regression) jest nauczenie takiego modelu \\(h\\), który przekształca \\(\\mathbf{x}\\) w \\(\\mathbf{y}\\).\nW podejściu STS w pierwszym kroku budowanych jest \\(d\\) niezależnych modeli przewidujących pojedyncze wyjście. Po tej czynności meta-model jest trenowany na zbiorze \\(D_i'\\), który jest wzbogaconym zbiorem \\(D_i\\) o predykcje zmiennej \\(Y_i\\), czyli\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)} \\right)\\). W zależności czy rozpatrujemy algorytm STS niekumulatywny, czy kumulatywny, drugi krok iteracji wygląda nieco inaczej:\n\nniekumulatywny\n\\[\n\\bar{D}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i'^{(l)} \\right)\\)\nkumulatywny\n\\[\n\\bar{\\bar{D}}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)},\\hat{y}_i'^{(l)} \\right)\\).\n\n\n\n\nSingle-target stacking\n\n\n\n\n2.2.1.2 Multi-target stacking\nW przeciwieństwie do STS, MTS został zaprojektowany do dzielenia się wiedzą w skorelowanych zmiennych docelowych w ramach procedury łączenia w stosy. Podobnie, najpierw uczone są modele pojedynczego celu. Następnie tworzony jest zestaw meta-modeli, które zawierają model dla każdej zmiennej docelowej \\(Y_i,\\) \\(i \\in \\{1, \\ldots, d\\}\\). W ten sposób uwzględniane są szacunki dotyczące pozostałych zmiennych docelowych z pierwszego etapu, tj. model jest uczony z przekształconego zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_1^{(l)},\\ldots,\\hat{y}_d^{(l)} \\right)\\). W metodzie MTS istnieją również dwa sposoby składania kolejnych iteracji. Przebiegają one w podobny sposób jak w przypadku STS.\n\n\n\nMulti-target stacking\n\n\nIstnieje jeszcze trzecia metoda powszechnie stosowana do predykcji wielowyniowej zwana Regressor Chains lub Classifier Chains w zależności od celu zadania. Idę działania tej metody przedstawię na przykładzie modelu regresyjnego.\n\n\n2.2.1.3 Regressor Chains\nRC opierają się na idei dopasowywania modeli pojedynczego celu wzdłuż wybranej permutacji, tj. łańcucha. Najpierw losowana jest permutacja w odniesieniu do zmiennych docelowych. Proces ten można przeprowadzić w sposób losowy (Spyromitros-Xioufis i in. 2016) lub uporządkowany (Melki i in. 2017). Wybrana permutacja jest wykorzystywana do zbudowania oddzielnego modelu regresji dla zmiennych docelowych zgodnie z kolejnością permutacji. Aby wykorzystać tę strukturę do MTR, rzeczywiste wartości zmiennych docelowych są dostarczane do kolejnych modeli podczas uczenia się wzdłuż łańcucha. Na podstawie pełnego łańcucha lub wybranego zestawu \\(C = (Y_1,\\ldots,Y_d)\\), pierwszy model jest ograniczony do ustalenia predykcji dla \\(Y_1\\). Następnie, kolejno dla \\(Y_i\\) uczone są modele na podstawie zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, y_1^{(l)},\\ldots, y_{i-1}^{(l)} \\right)\\). Ten algorytm ma również dwie odmiany (niekumulatywną i kumulatywną) w zależności od kształtu kolejnych iteracji.\n\n\n\nRegressor chains\n\n\nPonieważ, jak można się spodziewać wyniki modelowania w znaczny sposób zależą od wylosowanej permutacji, to w metodzie zaproponowanej przez Melki i in. (2017) aby uniknąć tego efektu buduje się \\(k\\) modeli dla różnych permutacji i łączy się wyniki w podobny sposób jak w lasach losowych.\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nSłowo komentarza jeśli chodzi o dostępność tych metod w językach programowania. Niestety wspomniane metody w R nie są zaimplementowane w sposób, który pozwalałby na bezpieczne używanie przygotowanych rozwiązań. Istnieje kilka wzmianek (na dzień dzisiejszy, czyli początek 2024 roku) na ten temat. Twórcy dwóch głównych frameworków do uczenia maszynowego, czyli mlr3 oraz tidymodels przygotowują implementacje tych metod. Dodatkowo istnieje rozwiązanie w wersji eksperymentalnej mtr-toolkit, które pozwala na wykonanie modelowania z wieloma wyjściami, którym można się posiłkować. Na potrzeby klasyfikacji istnieje również pakiet mldr i ultim, które pozwalają na uczenie modeli klasyfikacyjnymi z wieloma wyjściami.\nNiestety w przypadku Python-a nie jest dużo lepiej. Wprawdzie w pakiecie scikit-learn istnieją implementacje pozwalające na predykcje wielowyjściowe w obu typach zadań poprzez MultiOutputRegressor i MultiOutputClassifier, ale dokonują one predykcji naiwnej poprzez złożenie w listę wyników pojedynczych modeli dla każdej zmiennej. Nieco lepiej sprawa wygląda w przypadku metod łańcuchowych, ponieważ zarówno dla klasyfikacji, jak i regresji są metody to realizujące (ClassifierChain i RegressorChain).\n\n\n\n\n\n2.2.2 Adaptacja algorytmu\nProstota podejścia transformacji problemu sprawia, że jest ono odpowiednie dla problemów, w których jego wady mają niewielki lub żaden wpływ - jednak dla złożonych problemów podejście adaptacji algorytmu może okazać się bardziej efektywne. Dodatkowo, dowody empiryczne sugerują, że uczenie się powiązanych zadań jednocześnie, a nie niezależnie, może poprawić wyniki predykcyjne (Evgeniou i Pontil 2004). Z drugiej strony, jeśli zadania są bardzo odmienne, wydajność predykcyjna może ucierpieć, gdy zadania są uczone razem, a nie niezależnie (Faddoul i in. 2010). W związku z tym możemy wyciągnąć następujące wnioski:\n\njeśli zadania, których nasz predyktor ma się nauczyć, są powiązane, powinniśmy dążyć do znalezienia odpowiedniej metody adaptacji algorytmu;\njeśli zadania, których chcemy się nauczyć, nie są powiązane, powinniśmy zamiast tego dążyć do znalezienia odpowiedniej metody transformacji problemu.\n\nWreszcie, powinniśmy wziąć pod uwagę rozmiar problemu i zdać sobie sprawę, że gdy zadania są niepowiązane, istnieje potencjalny kompromis między efektywnością czasową a wydajnością predykcyjną przy wyborze metody transformacji problemu lub metody adaptacji algorytmu. W przypadku niepowiązanych ze sobą zadań, metody transformacji problemu mogą zwiększać skuteczność predykcyjną, ale zmniejszać wydajność czasową w przypadku dużych problemów i odwrotnie.\nNiestety tej metody nie da się zastosować do każdego typu modelu. Rodzina modeli, których adaptacja jest wykonana cały czas rośnie. Adaptacja modelu polega na przekształceniu go do postaci, w której da się wykonać predykcję dla wielu wyjść. Wśród modeli, których wersje native multi-target istnieją należy wymienić:\n\nregresja wieloraka (Izenman 1975)\nkNN\ndrzewo decyzyjne (Struyf i Džeroski 2006)\nlas losowy (Kocev i in. 2013)\nbagging (Kocev i in. 2013)\ngradient boosting (Zhang i Jung, b.d.; Faddoul i in. 2012)\nSVM (Xu, Guo, i Wang 2013; Vazquez i Walter 2003)\nno i oczywiście sieci neuronowe.\n\nNie sposób przedstawić w jaki sposób wprowadzone zostały zmiany we wszystkich algorytmach. Skupię się jednak na pokazaniu adaptacji drzew decyzyjnych do predykcji wielu wyjść jednocześnie, ponieważ jest to meta-model modeli takich jak lasy losowe, bagging czy boosting.\n\n2.2.2.1 Adaptacja klasyfikacyjnego drzewa decyzyjnego\nFaddoul i in. (2012) zaproponowali zmodyfikowaną wersję algorytmu drzewa decyzyjnego C4.5 (Quinlan 1993), która bezpośrednio obsługuje problemy klasyfikacji wielowyjściowej. Zmodyfikowana wersja (nazwana MT-DT) różni się od standardowej implementacji C4.5 w dwóch krytycznych aspektach: kryteriach podziału węzłów i procesie decyzyjnym. Faddoul i in. (2012) proponują trzy różne podejścia do łączenia wielu miar przyrostu informacji w jedną miarę: wspólny przyrost informacji, suma nieważona i maksymalny przyrost informacji. Wspólny przyrost informacyjny jest definiowany przy użyciu konkatenacji wszystkich poszczególnych zadań, tj. względnej różnicy w entropii mierzonej we wszystkich zadaniach decyzyjnych. Autorzy pokazują, że nieważona suma (Równanie 2.1) indywidualnych przyrostów informacyjnych wszystkich zadań jest równoważna wspólnemu przyrostowi informacyjnemu.\n\\[\nIG_U=\\sum_YIG_Y\n\\tag{2.1}\\]\nMaksymalny przyrost informacyjny, zgodnie z propozycją autorów jest definiowany po prostu jako maksymalny przyrost informacyjny wszystkich zadań:\n\\[\nIG_M=\\max_YIG_Y\n\\tag{2.2}\\]\nBadania eksperymentalne pokazały, że maksymalny przyrost informacyjny wykorzystany do budowania reguł podziału, charakteryzuje się wyższym poziomem dopasowania modeli, niż przy zastosowaniu \\(IG_U\\) i \\(IG_J\\).\nW przypadku klasyfikacji z jedną etykietą, algorytm indukcji drzewa decyzyjnego (taki jak C4.5) rekurencyjnie dzieli węzły, dodając (zazwyczaj dwa) elementy potomne, aż możliwe jest utworzenie liścia takiego, że znaczna większość (lub nawet wszystkie) jego przykładowych instancji należy do tej samej klasy. W przypadku wielu wyjść, indukcja drzewa niekoniecznie jest tak prosta. Rozważmy problem klasyfikacji wielowyjściowej z dwoma wyjściami binarnymi \\(\\nu_1\\) i \\(\\nu_2\\); możliwe jest, że po \\(t\\) podziałach, węzeł zawiera tylko wartości pozytywne dla \\(\\nu_1\\), ale mieszankę wartości pozytywnych i negatywnych dla \\(\\nu_2\\) - stąd, podczas konstruowania drzew decyzyjnych dla wielu jednoczesnych zadań, należy pamiętać, że proces decyzyjny dla pewnego zadania może wymagać krótszej ścieżki decyzyjnej niż inne zadania w ramach tego samego problemu wielowyjściowego. MT-DT radzi sobie z tym, sprawdzając w każdym węźle, czy możliwe jest utworzenie węzła terminalnego dla któregokolwiek z zadań - w powyższym przykładzie spowodowałoby to utworzenie drzewa, w którym wewnętrzny węzeł \\(t_1\\) jest oznaczony jako węzeł zatrzymania dla \\(\\nu_1\\), oznaczony klasą pozytywną. Ponieważ celem jest prognozowanie dla obu wyjść binarnych, \\(t_1\\) nie jest węzłem liścia - zamiast tego rekurencyjne dzielenie jest kontynuowane od \\(t_1\\), aż do znalezienia węzła \\(t_2\\) takiego, że \\(t_2\\) jest wystarczająco czysty w odniesieniu do \\(\\nu_2\\), aby można było utworzyć regułę klasyfikacji dla drugiego zadania binarnego. W tym momencie węzły decyzyjne (węzły wewnętrzne lub liście) zostały znalezione dla wszystkich wyników (\\(\\nu_1\\) i \\(\\nu_2\\)), a algorytm indukcji drzewa rekurencyjnego może zostać zakończony.\nNic dziwnego, że klasyfikacja przy użyciu już zbudowanego modelu MT-DT przebiega według tej samej formuły, co jego indukcja - podczas przechodzenia przez drzewo każdy węzeł jest sprawdzany w celu ustalenia, czy można podjąć decyzję dla któregokolwiek z aktualnie nierozstrzygniętych zadań. W przykładzie \\(\\nu_1\\), \\(\\nu_2\\), klasyfikacja zostanie dokonana dla \\(\\nu_1\\) w węźle \\(t_1\\), ponieważ jest on oznaczony jako węzeł zatrzymania dla \\(\\nu_1\\); następnie przejście jest kontynuowane do momentu napotkania \\(t_2\\) i klasyfikacja może zostać dokonana dla \\(\\nu_2\\). W tym momencie wszystkie wyjścia zostały sklasyfikowane, a przechodzenie może się zakończyć, zwracając dwie wartości w \\(t_1\\) i \\(t_2\\) jako klasyfikacje odpowiednio dla \\(\\nu_1\\) i \\(\\nu_2\\).\n\n\n2.2.2.2 Adaptacja regresyjnego drzewa decyzyjnego\nSegal (1992) zaproponował rozwiązanie dla drzew regresyjnych o wielu wyjściach (MRT), które są w stanie przewidywać wyniki dla wielu powiązanych zadań regresyjnych; te wielowyjściowe drzewa regresyjne są oparte na funkcji podziału najmniejszych kwadratów zaproponowanej w ramach CART (Breiman i in. 2017). W przypadku drzewa regresyjnego o jednej odpowiedzi celem jest minimalizacja następującej funkcji celu:\n\\[\n\\phi(t) = SS(t)-SS(t_L)-SS(t_R)\n\\]\ngdzie \\(SS(t)\\) jest zdefiniowana następująco\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))^2.\n\\]\nSegal (1992) dodał ważenie macierzą kowariancji do błędu kwadratowego, co prowadzi algorytm drzewa do tworzenia węzłów potomnych, które reprezentują jednorodne klastry w odniesieniu do zestawu odpowiedzi wyjściowych:\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))'V^{-1}(t)(y_i-\\bar{y}(t)),\n\\]\ngdzie \\(V(t)\\) oznacza macierz kowariancji w węźle \\(t\\).\n\n\n2.2.2.3 Adaptacja drzew decyzyjnych do realizacji obu zadań\nJak wspomniano wcześniej, jedną z kluczowych motywacji do podejmowania prób rozwiązywania problemów rozpoznawania wzorców z wieloma wyjściami przy użyciu metod adaptacji algorytmów jest oczekiwanie, że pojedynczy model wytrenowany na zestawie powiązanych zadań wykaże poprawę wydajności predykcyjnej w porównaniu do zestawu indywidualnych modeli, z których każdy został wytrenowany na pojedynczym zadaniu. Rodzi to pytanie: co jeśli problem wielowynikowy zawiera zarówno zadania klasyfikacji, jak i regresji? Jeśli zadania są niepowiązane, rozwiązanie takiego wspólnego problemu klasyfikacyjno-regresyjnego nie musi być trudniejsze niż szkolenie zestawu klasyfikatorów i regresorów dla poszczególnych zadań; jeśli jednak zadania są powiązane, oczekujemy, że metoda adaptacji algorytmu zapewni lepsze wyniki pod względem wydajności predykcyjnej.\nGlocker i in. (2012) zaproponował algorytm indukcji drzewa, który jednocześnie rozwiązuje jedno zadanie klasyfikacji i jedno zadanie regresji. Podobnie jak MT-DT i MRT, wspólne drzewo klasyfikacyjno-regresyjne (JCRT) rozwiązuje wiele jednoczesnych zadań predykcji poprzez modyfikację funkcji podziału węzła w kroku indukcyjnym i oznaczenie węzłów końcowych odpowiednimi wartościami dla każdego zadania. Ze względu na charakter wspólnych problemów klasyfikacyjno-regresyjnych, zmodyfikowana funkcja podziału jest wymagana do jednoczesnego uwzględnienia błędu zarówno części klasyfikacyjnej, jak i regresyjnej. Funkcja podziału zaproponowana przez Glocker i in. (2012) wykorzystuje funkcję entropii składającą się z trzech części:\n\npo pierwsze, entropia Shannona jest obliczana dla części klasyfikacji;\npo drugie, ważona entropia różnicowa jest obliczana dla części regresji;\npo trzecie, ze względu na fakt, że entropia Shannona i entropia różnicowa istnieją w różnych zakresach, stosuje się krok normalizacji w celu połączenia dwóch entropii. Entropia Shannona jest obliczana tak, jak opisano wcześniej:\n\n\\[\nH_c(t) = \\sum_{c\\in C}p(c|x)\\log p(c|x).\n\\]\nMiara entropii różniczkowej stosowana przez Glocker i in. (2012) dla regresyjnej części problemu jest obliczana w podobny sposób, z dwiema kluczowymi różnicami:\n\nzamiast sumowania prawdopodobieństw wartości nominalnych, entropia jest definiowana przez różniczkę funkcji prawdopodobieństwa wyjścia o wartości rzeczywistej;\ndodatkowo funkcja prawdopodobieństwa jest ważona w klasach:\n\n\\[\nH_{r|c}(t) = \\sum_{c\\in C}p(c|x)\\int_{r\\in \\mathbb{R}^n}-p(r|c,x)\\log p(r|c,x)dr.\n\\]\nNastępnie dokonywana jest normalizacja ze względu na oba zadania, gdzie punktem odniesienie jest entropia w korzeniu:\n\\[\nH(t) = \\frac12\\left(\\frac{H_c(t)}{H_c(t_0)}+\\frac{H_{r|c}(t)}{H_{r|c}(t_0)}\\right).\n\\]\n\n\n\n\n\n\nAdnotacja\n\n\n\nJedyne implementacje, które znalazłem dla obu języków programowania (R i Python) dotyczyły lasów losowych. W R pakiet nazywa się randomForestSRC, a w Pythonie morfist. Pozwalają one zarówno na wykonywanie wielowyjściowych zadań klasyfikacyjnych i regresyjnych, jak również zadań mieszanych. Oczywiście wspomniane wyżej typy zadań można realizować przy użyciu sieci neuronowych w obu językach programowania.\n\n\n\n\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, i Pedro Larrañaga. 2015. „A Survey on Multi-Output Regression”. WIREs Data Mining and Knowledge Discovery 5 (5): 216–33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, i Charles J. Stone. 2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, i Massimiliano Pontil. 2004. „Regularized multi–task learning”. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, sierpień. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Rémi Gilleron, i Fabien Torre. 2012. „Learning multiple tasks with boosted decision trees”. W Proceedings of the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I, 681–96. ECMLPKDD’12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, i Remi Gilleron. 2010. „Boosting Multi-Task Weak Learners with Applications to Textual and Social Data”. W 2010 Ninth International Conference on Machine Learning and Applications, 367–72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nGlocker, Ben, Olivier Pauly, Ender Konukoglu, i Antonio Criminisi. 2012. „Joint Classification-Regression Forests for Spatially Structured Multi-object Segmentation”. W Computer Vision – ECCV 2012, zredagowane przez Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, i Cordelia Schmid, 7575:870–81. Springer Berlin Heidelberg. http://link.springer.com/10.1007/978-3-642-33765-9_62.\n\n\nIzenman, Alan Julian. 1975. „Reduced-rank regression for the multivariate linear model”. Journal of multivariate analysis 5 (2): 248–64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, i Sašo Džeroski. 2013. „Tree ensembles for predicting structured outputs”. Pattern Recognition 46 (3): 817–33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, i Sebastián Ventura. 2017. „Multi-Target Support Vector Regression via Correlation Regressor Chains”. Information Sciences 415–416 (listopad): 53–69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Morgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello Mastelini, Fabio Luiz Melquiades, i Sylvio Barbon Jr. 2020. „Improved Prediction of Soil Properties with Multi-Target Stacked Generalisation on EDXRF Spectra”. arXiv preprint arXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. „Tree-Structured Methods for Longitudinal Data”. Journal of the American Statistical Association 87 (418): 407–18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves, i Ioannis Vlahavas. 2016. „Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs”. Machine Learning 104 (1): 55–98. https://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, i Sašo Džeroski. 2006. „Constraint Based Induction of Multi-objective Regression Trees”. W Knowledge Discovery in Inductive Databases, zredagowane przez Francesco Bonchi i Jean-François Boulicaut, 222–33. Lecture Notes w Computer Science. Springer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, i Victor Sheng. 2013. „Empirical Comparison of Multi-Label Classification Algorithms”. W Proceedings of the AAAI Conference on Artificial Intelligence, 27:1645–46.\n\n\nTsoumakas, Grigorios, i Ioannis Katakis. 2007. „Multi-Label Classification: An Overview”. International Journal of Data Warehousing and Mining (IJDWM) 3 (3): 1–13.\n\n\nVazquez, Emmanuel, i Eric Walter. 2003. „Multi-Output Suppport Vector Regression”. IFAC Proceedings Volumes, 13th IFAC Symposium on System Identification (SYSID 2003), Rotterdam, The Netherlands, 27-29 August, 2003, 36 (16): 1783–88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, i Laisheng Wang. 2013. „A Twin Multi-Class Classification Support Vector Machine”. Cognitive Computation 5 (4): 580–88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, i Cheolkon Jung. b.d. „GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputs”. https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "3  Przykłady - metody klasyczne",
    "section": "",
    "text": "3.1 Przykład 1\nNajpierw sformułujemy problem badawczy wymagający zastosowania modeli z wieloma wyjściami.\nKod\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_regression\nKod\n# generowanie danych do zadania\nX, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)\nKod\n# łączenie ich w ramki danych\nX_df = pd.DataFrame(i for i in X)\nX_df.columns = [\"X\"+ str(i) for i in range(1,11)]\ny_df = pd.DataFrame(i for i in y)\ny_df.columns = [\"y\"+str(i) for i in range(1,4)]\n\ndf = pd.concat([X_df,y_df], axis=1)\ndf.head()\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nX9\nX10\ny1\ny2\ny3\n\n\n\n\n0\n-0.547331\n0.426522\n-1.693585\n-0.740282\n0.277445\n-1.910679\n-0.320635\n1.449172\n-0.469619\n0.371273\n-92.924393\n-116.819352\n-32.117311\n\n\n1\n2.314630\n-0.936016\n-1.833034\n-0.394507\n-0.902981\n-1.089381\n1.005442\n-0.351289\n1.218223\n0.350258\n88.577211\n182.003040\n67.579488\n\n\n2\n-0.025423\n0.684189\n1.208964\n1.325672\n0.328946\n-0.354083\n-0.566556\n0.671359\n-0.560768\n0.327379\n115.652472\n63.462207\n43.519697\n\n\n3\n-0.044533\n0.603030\n-1.495716\n-0.507870\n-0.268485\n-0.140194\n-0.246658\n-0.758946\n2.567979\n1.808345\n-12.092227\n65.181041\n96.138770\n\n\n4\n2.083679\n0.318852\n-0.080982\n-1.284608\n0.281687\n0.792470\n-0.560598\n-1.368963\n0.718059\n-1.741815\n-18.573726\n-103.219393\n11.559200\nKod\ndf_X_boxplot = pd.melt(X_df)\nsns.boxplot(data = df_X_boxplot, x = \"variable\", y = \"value\")\nplt.show()\nKod\ndf_y_boxplot = pd.melt(y_df)\nsns.boxplot(data = df_y_boxplot, x = \"variable\", y = \"value\")\nplt.show()\nZarówno zmienne X, jak i y mają zbliżone rozkłady.\nKod\nsns.set_theme(style=\"white\")\ng = sns.PairGrid(y_df, diag_sharey=False)\ng.map_upper(sns.scatterplot, s=7)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.histplot)\nplt.show()\nJak widać z powyższego wykresu zmienne y1,y2,y3 wykazują pewne wzajemne zależności, dlatego budowa oddzielnych modeli dla każdej ze zmiennych powinna dawać gorsze predykcje niż modele wiążące wszystkie zmiennej w jednym modelu. Można też zauważyć, że rozkłady są zbliżone do normalnego.\nKod\ncor = df.corr()\nplt.figure(figsize = (12,10))\nsns.heatmap(cor, \n            xticklabels=cor.columns.values,\n            yticklabels=cor.columns.values,\n            annot = True, fmt = '.2f')\nplt.show()\nJak widać z powyższej macierzy korelacji, przynajmniej 8 spośród 10 zmiennych X koreluje istotnie ze zmiennymi y.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przykłady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#przykład-1",
    "href": "examples.html#przykład-1",
    "title": "3  Przykłady - metody klasyczne",
    "section": "",
    "text": "Przykład 3.1 Dane do zadania wygenerujemy za pomocą funkcji make_regression() z pakietu sklern.dataset1. Wygenerujemy 700 obserwacji z 10 predyktorami i 3 zmiennymi zależnymi.\n1 Nie znam R-owego odpowiednika",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przykłady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#rozwiązanie",
    "href": "examples.html#rozwiązanie",
    "title": "3  Przykłady - metody klasyczne",
    "section": "3.2 Rozwiązanie",
    "text": "3.2 Rozwiązanie\nNa potrzeby porównania różnych rozwiązań zbudujemy następujące konfiguracje modeli:\n\ntrzy lasy losowe dla każdej zmiennej y niezależnie;\nmodel lasu losowego z wykorzystaniem reguły Regression Chains jako transformacji modelu;\nlas losowy dla trzech zmiennych wynikowych jednocześnie (adaptacja modelu)\n\nRozwiązania te porównamy pod względem dopasowania.\n\n3.2.1 Niezależne lasy losowe\nFunkcja MultiOutputRegressor nałożona na model lasu losowego takie rozwiązanie tworzy.\n\n\nKod\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)\n\n\n\n\nKod\nrf_meta = RandomForestRegressor(random_state=4)\nrf_indep = MultiOutputRegressor(rf_meta)\nprint(rf_indep)\n\n\nMultiOutputRegressor(estimator=RandomForestRegressor(random_state=4))\n\n\n\n\nKod\nrf_indep.fit(X_train, y_train)\nr2_indep= rf_indep.score(X_test, y_test)\npred_indep = rf_indep.predict(X_test)\nrmse_indep = mean_squared_error(y_test, pred_indep, squared = False)\n\nprint(f'R2 on test samples: {r2_indep:.2f}')\nprint(f'RMSE on test samples: {rmse_indep:.1f}')\n\n\nR2 on test samples: 0.82\nRMSE on test samples: 71.0\n\n\n\n\n3.2.2 Regressor Chains RF\n\n\nKod\nrf_chains = RegressorChain(rf_meta)\nprint(rf_chains)\n\n\nRegressorChain(base_estimator=RandomForestRegressor(random_state=4))\n\n\n\n\nKod\nrf_chains.fit(X_train, y_train)\nr2_chains= rf_chains.score(X_test, y_test)\npred_chains = rf_chains.predict(X_test)\nrmse_chains = mean_squared_error(y_test, pred_chains, squared = False)\n\nprint(f'R2 on test samples: {r2_chains:.2f}')\nprint(f'RMSE on test samples: {rmse_chains:.1f}')\n\n\nR2 on test samples: 0.80\nRMSE on test samples: 72.3\n\n\n\n\n3.2.3 Adaptacja modelu RF\n\n\nKod\nprint(rf_meta)\n\n\nRandomForestRegressor(random_state=4)\n\n\n\n\nKod\nrf_meta.fit(X_train, y_train)\nr2_meta= rf_meta.score(X_test, y_test)\npred_meta = rf_meta.predict(X_test)\nrmse_meta = mean_squared_error(y_test, pred_meta, squared = False)\n\nprint(f'R2 on test samples: {r2_meta:.2f}')\nprint(f'RMSE on test samples: {rmse_meta:.1f}')\n\n\nR2 on test samples: 0.77\nRMSE on test samples: 77.9\n\n\n\n\n3.2.4 Podsumowanie\nBiorąc pod uwagę miary dopasowania najlepiej poradził sobie z tym zadaniem model składający się z trzech niezależnych modeli RF, potem model RF w wersji Regressor Chains, a najgorzej (o dziwo) radzi sobie z predykcją model korzystający adaptacji drzew decyzyjnych do wersji wielowyjściowej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przykłady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#przykład-2",
    "href": "examples.html#przykład-2",
    "title": "3  Przykłady - metody klasyczne",
    "section": "3.3 Przykład 2",
    "text": "3.3 Przykład 2\nTym razem sformułujemy problem z wieloma wyjściami ale klasyfikacyjny i również dopasujemy trzy wersje modelu lasu losowego:\n\nlasy losowe dla każdej zmiennej y niezależnie;\nmodel lasu losowego z wykorzystaniem reguły Classifier Chains jako transformacji modelu\nlas losowy dla wszystkich zmiennych wynikowych jednocześnie (adaptacja modelu).\n\nAnalizowany problem będzie prawdziwy i będzie dotyczył klasyfikacji enzymów na podstawie cech charakterystycznych substratów. Pierwszych 31 zmiennych stanowi zmienne objaśniające, a 6 pozostałych zmienne kodujące przynależność do danej grupy enzymów.\n\n\nKod\ndt = pd.read_csv(\"data/original.csv\", index_col = \"id\")\ndt.head()\n\n\n\n\n\n\n\n\n\nBertzCT\nChi1\nChi1n\nChi1v\nChi2n\nChi2v\nChi3v\nChi4n\nEState_VSA1\nEState_VSA2\n...\nSlogP_VSA3\nVSA_EState9\nfr_COO\nfr_COO2\nEC1\nEC2\nEC3\nEC4\nEC5\nEC6\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC00009\n49.783540\n2.000000\n0.782574\n2.347723\n0.513277\n1.539831\n0.000000\n0.000000\n7.822697\n0.000000\n...\n4.565048\n16.923611\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00013\n147.355172\n3.707107\n1.530297\n4.590890\n1.062804\n3.678309\n1.914534\n0.138556\n15.645394\n0.000000\n...\n13.440728\n20.899028\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00014\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.150546\n...\n0.000000\n0.000000\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00017\n172.720106\n4.947265\n2.081214\n2.081214\n1.157830\n1.157830\n0.489278\n0.180980\n12.514062\n12.451936\n...\n9.589074\n35.105740\n1\n1\n0\n1\n1\n0\n0\n0\n\n\nC00022\n72.039100\n2.642734\n1.381855\n1.381855\n0.861339\n0.861339\n0.301176\n0.000000\n11.752550\n0.000000\n...\n9.589074\n25.333333\n1\n1\n1\n1\n1\n1\n0\n1\n\n\n\n\n5 rows × 37 columns\n\n\n\n\n\nKod\ndt.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1039 entries, C00009 to C22220\nData columns (total 37 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   BertzCT            1039 non-null   float64\n 1   Chi1               1039 non-null   float64\n 2   Chi1n              1039 non-null   float64\n 3   Chi1v              1039 non-null   float64\n 4   Chi2n              1039 non-null   float64\n 5   Chi2v              1039 non-null   float64\n 6   Chi3v              1039 non-null   float64\n 7   Chi4n              1039 non-null   float64\n 8   EState_VSA1        1039 non-null   float64\n 9   EState_VSA2        1039 non-null   float64\n 10  ExactMolWt         1039 non-null   float64\n 11  FpDensityMorgan1   1039 non-null   float64\n 12  FpDensityMorgan2   1039 non-null   float64\n 13  FpDensityMorgan3   1039 non-null   float64\n 14  HallKierAlpha      1039 non-null   float64\n 15  HeavyAtomMolWt     1039 non-null   float64\n 16  Kappa3             1039 non-null   float64\n 17  MaxAbsEStateIndex  1039 non-null   float64\n 18  MinEStateIndex     1039 non-null   float64\n 19  NumHeteroatoms     1039 non-null   int64  \n 20  PEOE_VSA10         1039 non-null   float64\n 21  PEOE_VSA14         1039 non-null   float64\n 22  PEOE_VSA6          1039 non-null   float64\n 23  PEOE_VSA7          1039 non-null   float64\n 24  PEOE_VSA8          1039 non-null   float64\n 25  SMR_VSA10          1039 non-null   float64\n 26  SMR_VSA5           1039 non-null   float64\n 27  SlogP_VSA3         1039 non-null   float64\n 28  VSA_EState9        1039 non-null   float64\n 29  fr_COO             1039 non-null   int64  \n 30  fr_COO2            1039 non-null   int64  \n 31  EC1                1039 non-null   int64  \n 32  EC2                1039 non-null   int64  \n 33  EC3                1039 non-null   int64  \n 34  EC4                1039 non-null   int64  \n 35  EC5                1039 non-null   int64  \n 36  EC6                1039 non-null   int64  \ndtypes: float64(28), int64(9)\nmemory usage: 308.5+ KB\n\n\nSprawdzimy na ile niezbalansowane są poszczególne klasy wynikowe.\n\n\nKod\ndt_deps = dt.loc[:,'EC1':'EC6']\ndt_deps = dt_deps.melt()\ndt_deps = dt_deps[dt_deps.value == 1]\nsns.countplot(data = dt_deps, x = 'variable')\n\n\n\n\n\n\n\n\n\nNiestety nie wszystkie klasy występują jednakowo często i może pojawić się zjawisko, że kombinacja enzymów będzie występowała bardzo rzadko (np. raz). Jak widać z poniższej tabeli faktycznie tak się dzieje. To nie pozwala przeprowadzić uczenia. Dlatego musimy połączyć pewne klasy enzymów aby uniemożliwić taką sytuację.\n\n\nKod\ndt.loc[:,'EC1':'EC6'].value_counts()\n\n\nEC1  EC2  EC3  EC4  EC5  EC6\n1    0    0    0    0    0      178\n0    1    0    0    0    0      136\n1    1    0    0    0    0      112\n0    1    1    0    0    0       90\n     0    1    0    0    0       69\n1    1    0    1    0    0       38\n          1    0    0    0       37\n     0    0    1    0    0       33\n0    0    0    1    0    0       30\n     1    0    1    0    0       27\n               0    1    0       21\n1    0    1    0    0    0       20\n     1    0    0    0    1       19\n               1    0    1       15\n0    0    0    0    1    0       15\n1    1    0    0    1    0       14\n     0    0    0    1    0       12\n0    0    1    1    0    0       12\n          0    0    0    1       11\n1    1    0    1    1    1       11\n0    1    0    0    0    1       10\n          1    1    0    0        9\n1    0    0    1    1    0        8\n     1    1    1    0    1        8\n     0    0    1    0    1        7\n     1    1    0    0    1        7\n     0    0    0    0    1        7\n0    1    1    0    1    0        7\n                    0    1        7\n1    1    1    1    1    0        6\n          0    1    1    0        6\n0    1    0    1    0    1        6\n1    1    1    0    1    0        5\n0    0    0    1    1    0        5\n     1    1    1    1    0        4\n1    1    0    0    1    1        4\n          1    1    0    0        4\n0    0    1    0    0    1        4\n                    1    0        3\n1    1    1    1    1    1        3\n     0    1    1    0    0        3\n               0    0    1        3\n0    1    0    1    1    0        2\n                         1        2\n     0    0    1    0    1        2\n1    0    0    0    1    1        1\n0    0    1    1    0    1        1\n               0    1    1        1\n1    0    1    1    0    1        1\n               0    1    0        1\n0    1    1    1    0    1        1\n     0    1    1    1    0        1\nName: count, dtype: int64\n\n\nUsuniemy zatem takie kombinacje, które występują bardzo rzadko w danych.\n\n\nKod\ncombinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()\nidx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()\nbad_combinations = combinations[idx&lt;10]\nidx = []\nfor i in range(len(dt)):\n  idx.append(True)\n  for j in range(len(bad_combinations)):\n    if all(dt.iloc[i, 31:] == bad_combinations[j]):\n      idx[i]=False\n\ndt = dt.iloc[idx,:]\n\n\n\n\nKod\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier, ClassifierChain\ny = dt.iloc[:,31:].to_numpy()\nX = dt.iloc[:,:31].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n\n\n\n3.3.1 Niezależne lasy losowe\n\n\nKod\nrf_meta = RandomForestClassifier(random_state=4)\nrf_indep = MultiOutputClassifier(rf_meta)\nprint(rf_indep)\n\nrf_indep.fit(X_train, y_train)\nacc_indep= rf_indep.score(X_test, y_test)\npred_indep = rf_indep.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_indep:.3f}')\n\n\nMultiOutputClassifier(estimator=RandomForestClassifier(random_state=4))\nAccuracy on test samples: 0.172\n\n\n\n\n3.3.2 Classifier Chains\n\n\nKod\nrf_chains = ClassifierChain(rf_meta)\nprint(rf_chains)\n\nrf_chains.fit(X_train, y_train)\nacc_chains= rf_chains.score(X_test, y_test)\npred_chains = rf_chains.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_chains:.3f}')\n\n\nClassifierChain(base_estimator=RandomForestClassifier(random_state=4))\nAccuracy on test samples: 0.187\n\n\n\n\n3.3.3 Adaptacja modelu RF\n\n\nKod\nrf_meta.fit(X_train, y_train)\nacc_meta= rf_meta.score(X_test, y_test)\npred_meta = rf_meta.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_meta:.3f}')\n\n\nAccuracy on test samples: 0.190\n\n\n\n\n3.3.4 Podsumowanie\nRównież i tym razem nie widać wyraźnych różnic pomiędzy jakością dopasowania modeli. Model adaptowanego lasu losowego poradził sobie z zadanie najlepiej (19% poprawności trafień), Classifier Chains dał 18,7%, a niezależne lasy losowe 17,2%.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przykłady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples2.html",
    "href": "examples2.html",
    "title": "4  Przykłady NN",
    "section": "",
    "text": "4.1 Przykład 1\nRozwiązanie przykładu z poprzedniego rozdziału można dokonać z dużo lepszą precyzją wykorzystując sieci neuronowe. W tym przykładzie po raz kolejny wygenerujemy dane do uczenia1, a następnie wytrenujemy sieć (dosyć płytką) MLP.\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, ReLU, LeakyReLU, ELU\nfrom keras.callbacks import EarlyStopping\nimport keras\nKod\n# generowanie danych do zadania\nX, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)\n\nn_neurons = [10,20,50]\nModel będzie bardzo prosty, składający się z tylko jednej warstwy ukrytej.\nKod\ndef get_model(n_inputs, n_outputs, n_neurons):\n  model = Sequential()\n  model.add(Dense(int(n_neurons), input_dim=n_inputs, activation='relu'))\n  model.add(Dense(n_outputs, activation='linear'))\n  model.compile(loss='mse', optimizer='adam')\n  return model\nW tym przykładzie chciałem również pokazać jak wykonywać trenowanie sieci z użyciem sprawdzianu krzyżowego, który pomoże nam ustalić optymalną liczę neuronów w warstwie ukrytej.\nKod\n# ocena dopasowania modelu z wykorzystaniem CV\ndef evaluate_model(X, y, n_neurons):\n    results = list()\n    n_inputs, n_outputs = X.shape[1], y.shape[1]\n    # definicja CV\n    cv = RepeatedKFold(n_splits=5, random_state=1)\n    # pętla po foldach\n    for train_ix, test_ix in cv.split(X):\n        # przygotowanie danych\n        X_tr, X_te = X[train_ix], X[test_ix]\n        y_tr, y_te = y[train_ix], y[test_ix]\n        # określenie modelu\n        model = get_model(n_inputs, n_outputs, n_neurons)\n        # dopasowanie modelu\n        model.fit(X_tr, y_tr, verbose=0, epochs=100)\n        # ocena dopasowania na foldzie testowym\n        mae = model.evaluate(X_te, y_te, verbose=0)\n        results.append(mae)\n    return results\nKod\nresults = []\nfor i in n_neurons:\n  # dopasuj i oceń model na zbiorze uczącym\n  results.append(np.mean(evaluate_model(X_train, y_train, i)))\nKod\nresults = np.load(\"./data/mlp_eval.npz\")\nresults = results['arr_0'].tolist()\nfor i in range(len(n_neurons)):\n  print(f\"Dla {n_neurons[i]} neuronów MAE: {results[i]:.0f}\")\n\n\nDla 10 neuronów MAE: 16409\nDla 20 neuronów MAE: 9806\nDla 50 neuronów MAE: 3779\nNajlepszy rezultat osiągamy dla 50 neuronów i taki parametr dobierzemy w ostatecznym modelu.\nKod\nmy_callbacks = [\n    EarlyStopping(patience=2)\n]\n\nmodel = get_model(X_train.shape[1], y_train.shape[1], 50)\nhistory = model.fit(X_train, y_train, \n                    verbose=0, epochs=1000, \n                    validation_split=0.2, callbacks=my_callbacks)\n\n\nMetal device set to: Apple M1\nProces uczenia przebiegał prawidłowo i osiągnięto niski poziom funkcji straty.\nKod\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\nWyniki dopasowania znacznie przekraczają wyniki uzyskane metodami z poprzedniego rozdziału.\nKod\ny_pred = model.predict(X_test, verbose=0)\nrmse_mlp = mean_squared_error(y_test, y_pred, squared=False)\nr2_mlp = r2_score(y_test, y_pred)\nprint(f\"R2 on test sample: {r2_mlp:.2f}\")\nprint(f\"RMSE on test sample: {rmse_mlp:.1f}\")\n\n\nR2 on test sample: 1.00\nRMSE on test sample: 5.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Przykłady NN</span>"
    ]
  },
  {
    "objectID": "examples2.html#przykład-1",
    "href": "examples2.html#przykład-1",
    "title": "4  Przykłady NN",
    "section": "",
    "text": "1 te same co z przykładu regresyjnego z poprzedniego wykładu",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Przykłady NN</span>"
    ]
  },
  {
    "objectID": "examples2.html#przykład-2",
    "href": "examples2.html#przykład-2",
    "title": "4  Przykłady NN",
    "section": "4.2 Przykład 2",
    "text": "4.2 Przykład 2\nW tym przykładzie jeszcze raz rozpatrzymy zadanie klasyfikacyjne z wieloma wyjściami z poprzedniego rozdziału. Przeprowadzimy czynności preprocessingu podobne jak w poprzednim rozdziale, dodając jeszcze standaryzację, która dla sieci neuronowych jest bardzo ważna.\n\n\nKod\nfrom sklearn.preprocessing import StandardScaler\ndt = pd.read_csv(\"./data/original.csv\", index_col = \"id\")\ncombinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()\nidx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()\nbad_combinations = combinations[idx&lt;10]\nidx = []\nfor i in range(len(dt)):\n  idx.append(True)\n  for j in range(len(bad_combinations)):\n    if all(dt.iloc[i, 31:] == bad_combinations[j]):\n      idx[i]=False\n\ndt = dt.iloc[idx,:]\n\ny = dt.iloc[:,31:].to_numpy()\nX = dt.iloc[:,:31].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n\nscaler = StandardScaler().fit(X_train)\nX_test = scaler.transform(X_test)\nX_train = scaler.transform(X_train)\n\n\nNastępnie przygotujemy model sieci neuronowej, która pozwoli na właściwe klasyfikacje obiektów.\n\n\nKod\nmodel = Sequential()\nmodel.add(Dense(15, input_dim=X_train.shape[1]))\nmodel.add(ReLU())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y_train.shape[1], activation='sigmoid'))\n\nopt = keras.optimizers.Nadam(0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=opt)\nmodel.summary()\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 15)                480       \n                                                                 \n re_lu (ReLU)                (None, 15)                0         \n                                                                 \n dropout (Dropout)           (None, 15)                0         \n                                                                 \n dense_3 (Dense)             (None, 6)                 96        \n                                                                 \n=================================================================\nTotal params: 576\nTrainable params: 576\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nKod\nmy_callbacks = [\n    EarlyStopping(patience=10)\n]\nhistory = model.fit(X_train, y_train, epochs=1000,  validation_split=0.4, verbose=1,\ncallbacks=my_callbacks)\n\n\nEpoch 1/1000\n 1/12 [=&gt;............................] - ETA: 20s - loss: 0.7379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/12 [===========&gt;..................] - ETA: 0s - loss: 0.7970 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/12 [========================&gt;.....] - ETA: 0s - loss: 0.7768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - ETA: 0s - loss: 0.7780\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 2s 60ms/step - loss: 0.7780 - val_loss: 0.7309\nEpoch 2/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.7156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/12 [===========&gt;..................] - ETA: 0s - loss: 0.7571\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/12 [========================&gt;.....] - ETA: 0s - loss: 0.7412\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 16ms/step - loss: 0.7343 - val_loss: 0.6912\nEpoch 3/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.7013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.6760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.6764 - val_loss: 0.6594\nEpoch 4/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.6821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.6556\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.6498 - val_loss: 0.6347\nEpoch 5/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.6335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6372\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.6300\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.6276 - val_loss: 0.6154\nEpoch 6/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5903\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.6064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.6021 - val_loss: 0.6002\nEpoch 7/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5887 - val_loss: 0.5887\nEpoch 8/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.6046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5850\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5850 - val_loss: 0.5795\nEpoch 9/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5808\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5718 - val_loss: 0.5715\nEpoch 10/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5631\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5521\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5510 - val_loss: 0.5647\nEpoch 11/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5435\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5482 - val_loss: 0.5592\nEpoch 12/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5441\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5418\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5432 - val_loss: 0.5544\nEpoch 13/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5437\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5454 - val_loss: 0.5503\nEpoch 14/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5451\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5386\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5391 - val_loss: 0.5466\nEpoch 15/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5341\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5283\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5292 - val_loss: 0.5436\nEpoch 16/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5668\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.5345 - val_loss: 0.5405\nEpoch 17/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5348\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5317 - val_loss: 0.5377\nEpoch 18/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5282\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5473\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5349\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5292 - val_loss: 0.5352\nEpoch 19/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5197\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.5194 - val_loss: 0.5329\nEpoch 20/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5193\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5226 - val_loss: 0.5314\nEpoch 21/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5733\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5302\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5197 - val_loss: 0.5296\nEpoch 22/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5432\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5118\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5103 - val_loss: 0.5279\nEpoch 23/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5075\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5114\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5139 - val_loss: 0.5267\nEpoch 24/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5005\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5069\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5112 - val_loss: 0.5257\nEpoch 25/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5170\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5045\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5087\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5084 - val_loss: 0.5244\nEpoch 26/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4575\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5066\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5017 - val_loss: 0.5232\nEpoch 27/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5069\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5038\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5072\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5097 - val_loss: 0.5222\nEpoch 28/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5287\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5075\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5077 - val_loss: 0.5213\nEpoch 29/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4988 - val_loss: 0.5204\nEpoch 30/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4713\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4968\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5038 - val_loss: 0.5198\nEpoch 31/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5009\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5047 - val_loss: 0.5193\nEpoch 32/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5056\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4993 - val_loss: 0.5186\nEpoch 33/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4913\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4906 - val_loss: 0.5178\nEpoch 34/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.4993 - val_loss: 0.5168\nEpoch 35/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4936\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4942 - val_loss: 0.5162\nEpoch 36/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4924\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4925 - val_loss: 0.5156\nEpoch 37/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4778\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4706\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4898 - val_loss: 0.5149\nEpoch 38/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4793\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4896\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4883 - val_loss: 0.5145\nEpoch 39/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4940\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4933 - val_loss: 0.5139\nEpoch 40/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4927 - val_loss: 0.5134\nEpoch 41/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4682\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4830\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4817 - val_loss: 0.5126\nEpoch 42/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5112\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4823\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4880 - val_loss: 0.5121\nEpoch 43/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4785\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4861 - val_loss: 0.5116\nEpoch 44/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4405\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4808\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4816 - val_loss: 0.5111\nEpoch 45/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4840\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4880 - val_loss: 0.5107\nEpoch 46/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.3993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4767 - val_loss: 0.5103\nEpoch 47/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4710\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4794\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4834 - val_loss: 0.5103\nEpoch 48/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4770\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4824 - val_loss: 0.5099\nEpoch 49/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4817\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4811\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4760 - val_loss: 0.5095\nEpoch 50/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4803\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4794 - val_loss: 0.5090\nEpoch 51/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4792 - val_loss: 0.5087\nEpoch 52/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4982\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4655\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4749 - val_loss: 0.5083\nEpoch 53/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4812 - val_loss: 0.5079\nEpoch 54/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4747 - val_loss: 0.5076\nEpoch 55/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5205\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4736 - val_loss: 0.5073\nEpoch 56/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4894\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4754 - val_loss: 0.5069\nEpoch 57/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4827\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4708 - val_loss: 0.5068\nEpoch 58/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4765 - val_loss: 0.5067\nEpoch 59/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4315\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4777\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.4784 - val_loss: 0.5067\nEpoch 60/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4711 - val_loss: 0.5070\nEpoch 61/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4228\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4630\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4675 - val_loss: 0.5066\nEpoch 62/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4196\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4544\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.4690 - val_loss: 0.5061\nEpoch 63/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4761\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4722 - val_loss: 0.5062\nEpoch 64/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4270\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4745 - val_loss: 0.5064\nEpoch 65/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4676\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4697 - val_loss: 0.5062\nEpoch 66/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5521\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4681 - val_loss: 0.5061\nEpoch 67/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4781\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4706 - val_loss: 0.5062\nEpoch 68/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4986\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4670 - val_loss: 0.5067\nEpoch 69/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4416\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4515\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4683 - val_loss: 0.5066\nEpoch 70/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4828\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4611\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4697 - val_loss: 0.5064\nEpoch 71/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4687\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4665 - val_loss: 0.5062\nEpoch 72/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4557\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4668\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4699 - val_loss: 0.5061\nEpoch 73/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.3911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4660\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4655 - val_loss: 0.5060\nEpoch 74/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4714 - val_loss: 0.5061\nEpoch 75/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5306\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4737\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4688 - val_loss: 0.5065\nEpoch 76/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4492\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4646 - val_loss: 0.5063\nEpoch 77/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4649 - val_loss: 0.5062\nEpoch 78/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4633 - val_loss: 0.5060\nEpoch 79/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4653 - val_loss: 0.5060\nEpoch 80/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4529\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4586\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4627\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4602 - val_loss: 0.5057\nEpoch 81/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4433\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4621 - val_loss: 0.5059\nEpoch 82/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4350\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4644 - val_loss: 0.5061\nEpoch 83/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4153\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4621\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4608 - val_loss: 0.5063\nEpoch 84/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.3970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4599\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4600 - val_loss: 0.5065\nEpoch 85/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4619 - val_loss: 0.5066\nEpoch 86/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4633 - val_loss: 0.5066\nEpoch 87/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4572\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.4593 - val_loss: 0.5064\nEpoch 88/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5105\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4767\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4596 - val_loss: 0.5065\nEpoch 89/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4657\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4618 - val_loss: 0.5065\nEpoch 90/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4611\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4626 - val_loss: 0.5066\n\n\n\n\nKod\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nKod\nfrom sklearn.metrics import accuracy_score\ny_pred = model.predict(X_test, verbose=0)\ny_class = y_pred.round()\nacc = accuracy_score(y_test, y_class)\nprint(f\"ACC on test sample: {acc:.3f}\")\n\n\nACC on test sample: 0.183\n\n\n\n4.2.1 Podsumowanie\nModel prostej sieci neuronowej nie poprawił znacząco jakości dopasowania w stosunku do modelu lasu losowego adaptowanego do zadania z wieloma wyjściami.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Przykłady NN</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Borchani, Hanen, Gherardo Varando, Concha Bielza, and Pedro Larrañaga.\n2015. “A Survey on Multi-Output Regression.” WIREs Data\nMining and Knowledge Discovery 5 (5): 216–33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, and Charles J. Stone.\n2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, and Massimiliano Pontil. 2004. “Regularized\nMulti–Task Learning.” Proceedings of the Tenth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\nAugust. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Rémi Gilleron, and Fabien\nTorre. 2012. “Learning Multiple Tasks with Boosted Decision\nTrees.” In Proceedings of the 2012th European Conference on\nMachine Learning and Knowledge Discovery in Databases - Volume Part\ni, 681–96. ECMLPKDD’12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, and Remi\nGilleron. 2010. “Boosting Multi-Task Weak Learners with\nApplications to Textual and Social Data.” In 2010 Ninth\nInternational Conference on Machine Learning and Applications,\n367–72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nGlocker, Ben, Olivier Pauly, Ender Konukoglu, and Antonio Criminisi.\n2012. “Joint Classification-Regression Forests for Spatially\nStructured Multi-Object Segmentation.” In Computer Vision –\nECCV 2012, edited by Andrew Fitzgibbon, Svetlana Lazebnik, Pietro\nPerona, Yoichi Sato, and Cordelia Schmid, 7575:870–81. Springer Berlin\nHeidelberg. http://link.springer.com/10.1007/978-3-642-33765-9_62.\n\n\nIzenman, Alan Julian. 1975. “Reduced-Rank Regression for the\nMultivariate Linear Model.” Journal of Multivariate\nAnalysis 5 (2): 248–64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, and Sašo Džeroski. 2013.\n“Tree Ensembles for Predicting Structured Outputs.”\nPattern Recognition 46 (3): 817–33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, and Sebastián Ventura.\n2017. “Multi-Target Support Vector Regression via Correlation\nRegressor Chains.” Information Sciences 415–416\n(November): 53–69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning.\nMorgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello\nMastelini, Fabio Luiz Melquiades, and Sylvio Barbon Jr. 2020.\n“Improved Prediction of Soil Properties with Multi-Target Stacked\nGeneralisation on EDXRF Spectra.” arXiv Preprint\narXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. “Tree-Structured Methods for\nLongitudinal Data.” Journal of the American Statistical\nAssociation 87 (418): 407–18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves,\nand Ioannis Vlahavas. 2016. “Multi-Target Regression\nvia Input Space Expansion: Treating Targets as\nInputs.” Machine Learning 104 (1): 55–98.\nhttps://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, and Sašo Džeroski. 2006. “Constraint Based Induction\nof Multi-Objective Regression Trees.” In Knowledge Discovery\nin Inductive Databases, edited by Francesco Bonchi and\nJean-François Boulicaut, 222–33. Lecture Notes in Computer Science.\nSpringer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, and Victor Sheng. 2013. “Empirical Comparison of\nMulti-Label Classification Algorithms.” In Proceedings of the\nAAAI Conference on Artificial\nIntelligence, 27:1645–46.\n\n\nTsoumakas, Grigorios, and Ioannis Katakis. 2007. “Multi-Label\nClassification: An Overview.” International\nJournal of Data Warehousing and Mining (IJDWM) 3 (3): 1–13.\n\n\nVazquez, Emmanuel, and Eric Walter. 2003. “Multi-Output Suppport\nVector Regression.” IFAC Proceedings Volumes, 13th IFAC\nsymposium on system identification (SYSID 2003), rotterdam, the\nnetherlands, 27-29 august, 2003, 36 (16): 1783–88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, and Laisheng Wang. 2013. “A Twin Multi-Class\nClassification Support Vector Machine.” Cognitive\nComputation 5 (4): 580–88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, and Cheolkon Jung. n.d. “GBDT-MO: Gradient\nBoosted Decision Trees for Multiple Outputs.” https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "Literatura"
    ]
  }
]