[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaawansowane metody uczenia maszynowego",
    "section": "",
    "text": "WstÄ™p\nKsiÄ…Å¼ka ta jest napisana na potrzeby prowadzenia zajÄ™Ä‡ na kierunku InÅ¼ynieria i analiza danych z przedmiotu Zaawansowane metody uczenia maszynowego. Jest swego rodzaju autorskim podejÅ›ciem do tematu, przedstawiajÄ…cym wybrane metody uczenia maszynowego, ktÃ³re rzadziej wystÄ™pujÄ… w opracowaniach na temat uczenia maszynowego.\nUczenie maszynowe stanowi obszar intensywnego rozwoju, ktÃ³ry obejmuje szereg technik umoÅ¼liwiajÄ…cych bardziej skomplikowane i wydajne modele predykcyjne. WÅ›rÃ³d tych metod warto wyrÃ³Å¼niÄ‡ gÅ‚Ä™bokie sieci neuronowe, zwÅ‚aszcza konwolucyjne sieci neuronowe (CNN) i rekurencyjne sieci neuronowe (RNN). CNN sÄ… wykorzystywane w zadaniach przetwarzania obrazÃ³w, gdzie potrafiÄ… efektywnie ekstrahowaÄ‡ hierarchiczne cechy z danych wejÅ›ciowych, natomiast RNN sÄ… efektywne w analizie sekwencji danych, takich jak jÄ™zyk naturalny. Ponadto, metody uczenia maszynowego obejmujÄ… techniki transferu wiedzy, uczenie ze wzmocnieniem, generatywne modele, takie jak generatywne sieci przeciwdziedzinowe (GAN), czy teÅ¼ autokodery. Te nowoczesne podejÅ›cia umoÅ¼liwiajÄ… modelom uczÄ…cym siÄ™ wykonywanie bardziej zÅ‚oÅ¼onych zadaÅ„, a takÅ¼e adaptacjÄ™ do rÃ³Å¼norodnych danych wejÅ›ciowych, co sprawia, Å¼e sÄ… one stosowane w obszarach takich jak rozpoznawanie obrazÃ³w, przetwarzanie jÄ™zyka naturalnego, czy nawet w autonomicznych systemach decyzyjnych.\nPonadto, zaawansowane metody uczenia maszynowego obejmujÄ… takÅ¼e techniki regularyzacji, optymalizacji i inÅ¼ynieriÄ™ cech. Regularyzacja ma na celu zapobieganie przeuczeniu poprzez kontrolowanie zÅ‚oÅ¼onoÅ›ci modelu, natomiast optymalizacja skupia siÄ™ na dostosowywaniu wag modelu w celu minimalizacji funkcji straty. InÅ¼ynieria cech polega na rÄ™cznym lub automatycznym dostosowywaniu danych wejÅ›ciowych w celu uzyskania lepszych wynikÃ³w modelu. DziÄ™ki tym zaawansowanym metodom, uczenie maszynowe staje siÄ™ coraz bardziej potÄ™Å¼nym narzÄ™dziem w analizie danych i podejmowaniu skomplikowanych decyzji w rÃ³Å¼nych dziedzinach.\nModele predykcyjne dla wielu wyjÅ›Ä‡, czyli tzw. multi-target regression and classification, stanowiÄ… kolejny istotny obszar w dziedzinie uczenia maszynowego. W przypadku multi-target regression, celem jest przewidywanie wielu wartoÅ›ci wyjÅ›ciowych dla danego zestawu wejÅ›ciowego, co czÄ™sto spotyka siÄ™ w zÅ‚oÅ¼onych problemach predykcyjnych, takich jak prognozowanie wielu parametrÃ³w jednoczeÅ›nie. Z kolei w przypadku multi-target classification, model ma za zadanie przypisanie jednego lub wiÄ™cej klas dla kaÅ¼dego przykÅ‚adu wejÅ›ciowego. Te modele sÄ… powszechnie stosowane w rÃ³Å¼nych dziedzinach, takich jak bioinformatyka, finanse czy przemysÅ‚, gdzie jednoczesne przewidywanie wielu zmiennych jest kluczowe dla skutecznego rozwiÄ…zania problemu. WdroÅ¼enie takich zaawansowanych modeli predykcyjnych wymaga starannej obrÃ³bki danych, odpowiedniego dostosowania architektury modelu oraz precyzyjnej oceny wynikÃ³w, co sprawia, Å¼e sÄ… one istotnym narzÄ™dziem w obszarze analizy danych i podejmowania decyzji.\nModele jÄ™zykowe stanowiÄ… jeszcze jeden kluczowy obszar w dziedzinie uczenia maszynowego, skoncentrowany na zrozumieniu i generowaniu ludzkiego jÄ™zyka naturalnego. GÅ‚Ä™bokie sieci neuronowe, zwÅ‚aszcza rekurencyjne sieci neuronowe (RNN) i transformery, zostaÅ‚y skutecznie wykorzystane do tworzenia modeli jÄ™zykowych o zdolnoÅ›ciach przetwarzania i generowania tekstu na poziomie zbliÅ¼onym do ludzkiego. Te modele zdolne sÄ… do zrozumienia kontekstu, analizy gramatyki, a takÅ¼e generowania spÃ³jnych i sensownych odpowiedzi. Wykorzystywane sÄ… w rÃ³Å¼norodnych zastosowaniach, takich jak tÅ‚umaczenie maszynowe, generowanie tekstu, czy analiza nastroju w tekÅ›cie. Ponadto, pre-trenowane modele jÄ™zykowe, takie jak BERT czy GPT (Generative Pre-trained Transformer), zdobywajÄ… popularnoÅ›Ä‡, umoÅ¼liwiajÄ…c dostosowanie ich do rÃ³Å¼nych zadaÅ„ poprzez fine-tuning. W miarÄ™ postÄ™pu badaÅ„ i rozwoju w tej dziedzinie, modele jÄ™zykowe stajÄ… siÄ™ coraz bardziej zaawansowane, co przyczynia siÄ™ do doskonalenia komunikacji miÄ™dzy maszynami a ludÅºmi oraz do rozwijania nowych moÅ¼liwoÅ›ci w dziedzinie przetwarzania jÄ™zyka naturalnego.\nWspomniane powyÅ¼ej metody i modele bÄ™dÄ… stanowiÄ‡ treÅ›Ä‡ wykÅ‚adÃ³w z wspomnianego na wstÄ™pie przedmiotu.",
    "crumbs": [
      "WstÄ™p"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1Â  Wprowadzenie",
    "section": "",
    "text": "Witam w Å›wiecie zaawansowanych metod uczenia maszynowego ğŸ¤–, prezentowanej w niniejszej publikacji. KsiÄ…Å¼ka ta skupia siÄ™ na trzech gÅ‚Ã³wnych obszarach, zaczynajÄ…c od wielowymiarowych problemÃ³w predykcyjnych, przechodzÄ…c przez kompleksowe modele gÅ‚Ä™bokich sieci neuronowych, a koÅ„czÄ…c na zaawansowanych modelach jÄ™zykowych. Koncepcyjnie rozpoczniemy od omÃ³wienia multiple target regression and classification, gdzie przedstawimy skomplikowane zadania predykcyjne wymagajÄ…ce jednoczesnej prognozy wielu zmiennych. Przeanalizujemy praktyczne zastosowania tych modeli w obszarach, takich jak nauki spoÅ‚eczne, biologia i finanse.\nNastÄ™pnie poÅ›wiÄ™cimy uwagÄ™ gÅ‚Ä™bokim sieciom neuronowym, gÅ‚Ã³wnemu filarowi nowoczesnej sztucznej inteligencji. OmÃ³wimy ewolucjÄ™ od konwolucyjnych sieci neuronowych (CNN) do rekurencyjnych sieci neuronowych (RNN), zwracajÄ…c uwagÄ™ na ich zdolnoÅ›Ä‡ do efektywnego przetwarzania obrazÃ³w, sekwencji danych i rozwiÄ…zania bardziej zÅ‚oÅ¼onych problemÃ³w. W ramach tego obszaru, przyjrzymy siÄ™ rÃ³wnieÅ¼ technikom transferu wiedzy, uczenia ze wzmocnieniem oraz generatywnym modelom, takim jak generatywne sieci przeciwdziedzinowe (GAN), ktÃ³re poszerzajÄ… granice moÅ¼liwoÅ›ci maszynowego uczenia siÄ™.\n\n\n\nTrzeci kluczowy obszar, ktÃ³ry bÄ™dzie przedmiotem analizy, to modele jÄ™zykowe. RozwaÅ¼ania rozpoczniemy od gÅ‚Ä™bokich sieci neuronowych, a nastÄ™pnie skoncentrujemy siÄ™ na transformatorach, ktÃ³re rewolucjonizujÄ… przetwarzanie jÄ™zyka naturalnego ğŸ‘…. Przedstawimy praktyczne zastosowania tych modeli, zwÅ‚aszcza w tÅ‚umaczeniu maszynowym, generowaniu tekstu i analizie sentymentu. Ponadto, omÃ³wimy pre-trenowane modele jÄ™zykowe, takie jak BERT czy GPT, jako kluczowe narzÄ™dzia adaptacyjne, zdolne do fine-tuningu w zaleÅ¼noÅ›ci od konkretnego zadania.\nKaÅ¼dy podejmowany temat bÄ™dzie wzbogacony o implementacjÄ™ analizowanych metod w realnych scenariuszach. OmÃ³wimy kroki od obrÃ³bki danych, przez dostosowywanie architektury modelu, aÅ¼ po ocenÄ™ wynikÃ³w. W tym kontekÅ›cie poruszymy takÅ¼e aspekty etyczne i wyzwania zwiÄ…zane z zastosowaniem zaawansowanych modeli uczenia maszynowego.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html",
    "href": "multi_target_models.html",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "",
    "text": "2.1 Typy modeli z wieloma zmiennymi wynikowymi\nWÅ›rÃ³d nadzorowanych modeli uczenia maszynowego z wieloma zmiennymi wynikowymi moÅ¼na wymieniÄ‡ zarÃ³wno te dedykowane do klasyfikacji, jak i regresji. Modele te sÄ… znane jako modele z wieloma wyjÅ›ciami (klasyfikacyjne) lub modele z wieloma wyjÅ›ciami (regresyjne), w zaleÅ¼noÅ›ci od rodzaju problemu, ktÃ³ry rozwiÄ…zujÄ….",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "href": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "",
    "text": "Modele z wieloma wyjÅ›ciami (klasyfikacyjne)\nW przypadku klasyfikacji, gdy mamy wiele kategorii (klas) jako zmiennÄ… wynikowÄ…, modele te sÄ… nazywane modelami z wieloma wyjÅ›ciami. PrzykÅ‚ady obejmujÄ… algorytmy takie jak regresja logistyczna, metoda k najbliÅ¼szych sÄ…siadÃ³w (k-NN) czy algorytmy drzew decyzyjnych, ktÃ³re zostaÅ‚y dostosowane do obsÅ‚ugi wielu klas.\nPrzykÅ‚adowe zadanie: ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych dotyczÄ…cy rÃ³Å¼nych rodzajÃ³w owocÃ³w (np. jabÅ‚ek, pomaraÅ„czy, bananÃ³w) i chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje gatunek owocu oraz kolor owocu. Mamy wiÄ™c dwie zmienne wynikowe: gatunek (klasyfikacja wieloklasowa) i kolor (klasyfikacja wieloklasowa).\nModele z wieloma wyjÅ›ciami (regresyjne).\nW przypadku regresji, gdzie zmiennÄ… wynikowÄ… jest wektor wartoÅ›ci numerycznych, modele te sÄ… nazywane modelami z wieloma wyjÅ›ciami. PrzykÅ‚ady obejmujÄ… algorytmy regresji liniowej lub nieliniowej, algorytmy oparte na drzewach decyzyjnych, czy teÅ¼ bardziej zaawansowane modele, takie jak sieci neuronowe.\nPrzykÅ‚adowe zadanie: ZakÅ‚adamy, Å¼e mamy zbiÃ³r danych zawierajÄ…cy informacje o pracownikach, takie jak doÅ›wiadczenie zawodowe, poziom wyksztaÅ‚cenia, liczba godzin pracy tygodniowo itp. Chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje zarobki pracownikÃ³w oraz ich poziom satysfakcji zawodowej.\nModele wielozadaniowe.\nModele wielozadaniowe to rodzaj nadzorowanego uczenia maszynowego, w ktÃ³rym model jest trenowany jednoczeÅ›nie do rozwiÄ…zania kilku zadaÅ„. Te zadania mogÄ… obejmowaÄ‡ zarÃ³wno klasyfikacjÄ™, jak i regresjÄ™. DziÄ™ki wspÃ³lnemu trenowaniu modelu na wielu zadaniach, moÅ¼na uzyskaÄ‡ korzyÅ›ci w postaci wspÃ³lnego wykorzystywania wiedzy miÄ™dzy zadaniami.\nPrzykÅ‚adowe zadanie: ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych dotyczÄ…cy zakupÃ³w klientÃ³w w sklepie internetowym. Dla kaÅ¼dego klienta mamy informacje o rÃ³Å¼nych aspektach zakupÃ³w, takich jak czas dostawy, Å‚atwoÅ›Ä‡ obsÅ‚ugi strony, jakoÅ›Ä‡ produktÃ³w itp. Chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje dwie zmienne wynikowe: jakoÅ›Ä‡ obsÅ‚ugi klienta (skala jakoÅ›ciowa, np. â€œNiskaâ€, â€œÅšredniaâ€, â€œWysokaâ€) oraz caÅ‚kowity wydatek klienta (zmienna iloÅ›ciowa, np. kwota zakupÃ³w).\nModele hierarchiczne.\nW niektÃ³rych przypadkach, szczegÃ³lnie gdy mamy hierarchiÄ™ zmiennych wynikowych, modele te mogÄ… byÄ‡ budowane w sposÃ³b hierarchiczny. PrzykÅ‚adowo, w problemie klasyfikacji obrazÃ³w z hierarchiÄ… kategorii (na przykÅ‚ad rozpoznawanie gatunkÃ³w zwierzÄ…t), model moÅ¼e byÄ‡ zaprojektowany do rozpoznawania zarÃ³wno ogÃ³lnych, jak i bardziej szczegÃ³Å‚owych kategorii.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#rÃ³Å¼nie-podejÅ›cia-do-modelowania-z-wieloma-wyjÅ›ciami",
    "href": "multi_target_models.html#rÃ³Å¼nie-podejÅ›cia-do-modelowania-z-wieloma-wyjÅ›ciami",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "2.2 RÃ³Å¼nie podejÅ›cia do modelowania z wieloma wyjÅ›ciami",
    "text": "2.2 RÃ³Å¼nie podejÅ›cia do modelowania z wieloma wyjÅ›ciami\nIstniejÄ… dwa ogÃ³lne podejÅ›cia do rozwiÄ…zywania problemÃ³w wieloetykietowych: transformacja problemu i adaptacja algorytmu. Transformacja problemu polega na manipulowaniu zbiorem danych w taki sposÃ³b, Å¼e problem wieloetykietowy staje siÄ™ jednym lub kilkoma problemami jednoetykietowymi (Tawiah i Sheng 2013). Adaptacja algorytmu polega na tym, Å¼e sam algorytm jest w stanie poradziÄ‡ sobie bezpoÅ›rednio z problemem wieloetykietowym. Okazuje siÄ™, Å¼e wiele, choÄ‡ nie wszystkie, metody adaptacji algorytmÃ³w metod adaptacji algorytmÃ³w w rzeczywistoÅ›ci wykorzystuje transformacjÄ™ problemu (Tsoumakas i Katakis 2007).\n\n2.2.1 Transformacja problemu\nTechniki te przewidujÄ… stworzenie indywidualnego modelu dla kaÅ¼dego celu, a nastÄ™pnie poÅ‚Ä…czenie oddzielnych modeli w celu uzyskania ogÃ³lnej prognozy. Metody transformacji problemÃ³w okazaÅ‚y siÄ™ lepsze od metod adaptacji algorytmÃ³w pod wzglÄ™dem dokÅ‚adnoÅ›ci (Spyromitros-Xioufis i in. 2016). Co wiÄ™cej, podstawowa zasada sprawia, Å¼e metody transformacji problemu sÄ… niezaleÅ¼ne od algorytmu. W konsekwencji, moÅ¼na je Å‚atwo dostosowaÄ‡ do danego problemu poprzez zastosowanie odpowiednich bazowych metod uczÄ…cych. Punkt ten ma rÃ³wnieÅ¼ szczegÃ³lne znaczenie dla modeli typu ensemble, ktÃ³re Å‚Ä…czÄ… oszacowania z wielu potencjalnie rÃ³Å¼nych algorytmÃ³w w ostatecznÄ… prognozÄ™. Niedawno Spyromitros-Xioufis i in. (2016) zaproponowali rozszerzenie znanych metod transformacji klasyfikacji wieloetykietowej, aby poradziÄ‡ sobie z problemem regresji wielowynikowej i modelowaÄ‡ zaleÅ¼noÅ›ci miÄ™dzy celami. W szczegÃ³lnoÅ›ci wprowadzili oni dwa nowe podejÅ›cia do regresji wielocelowej, skÅ‚adanie regresorÃ³w wielocelowych i Å‚aÅ„cuchy regresorÃ³w, inspirowane popularnymi i skutecznymi podejÅ›ciami do klasyfikacji wieloznaczeniowej.\nPodstawowÄ… koncepcjÄ… w metodach transformacji problemÃ³w jest wykorzystanie poprzednich modeli do nowego przewidywania poprzez rozszerzonÄ… przestrzeÅ„ cech (Borchani i in. 2015). Stacked generalization to podejÅ›cie do meta-uczenia, ktÃ³re wykorzystuje dane wyjÅ›ciowe wczeÅ›niej wyuczonych modeli do uczenia siÄ™ nowego modelu. W zwiÄ…zku z tym poczÄ…tkowe dane wyjÅ›ciowe modelu sÄ… traktowane jako nowe cechy i sÄ… ukÅ‚adane w stos do poczÄ…tkowego wektora cech przed ponownym uczeniem. W oryginalnym sformuÅ‚owaniu przewidziano tylko dwuetapowÄ… procedurÄ™, tj. poczÄ…tkowe modele wyuczone z poczÄ…tkowego wektora cech odpowiadajÄ… odpowiednio modelom i danym poziomu 0, a powiÄ™kszony wektor cech i ponownie wyuczony model sÄ… okreÅ›lane odpowiednio jako dane poziomu 1 i generalizator. JednakÅ¼e, rozsÄ…dnie rzecz biorÄ…c, ten proces ukÅ‚adania pojedynczego celu (ang. Single-target Stacking - STS) moÅ¼e byÄ‡ rÃ³wnieÅ¼ przeprowadzany w wielu iteracjach. Aby wdroÅ¼yÄ‡ tÄ™ zasadÄ™ dla problemÃ³w z wieloma celami, w ktÃ³rych kodowane sÄ… rÃ³wnieÅ¼ moÅ¼liwe korelacje miÄ™dzy zmiennymi docelowymi, wprowadzono koncepcjÄ™ ukÅ‚adania wielu celÃ³w (ang. Multi-target Stacking - MTS) (Borchani i in. 2015). Analogicznie do STS, szkolenie modelu MTS moÅ¼na uznaÄ‡ za procedurÄ™ dwuetapowÄ…. W pierwszym etapie uczone sÄ… niezaleÅ¼ne modele dla kaÅ¼dej zmiennej docelowej. NastÄ™pnie uczone sÄ… meta-modele dla kaÅ¼dej zmiennej docelowej z rozszerzonymi wektorami cech, ktÃ³re zawierajÄ… poczÄ…tkowe wektory cech, a takÅ¼e oszacowania poziomu 0 pozostaÅ‚ych zmiennych docelowych. Podobne pomysÅ‚y byÅ‚y rÃ³wnieÅ¼ stosowane w kontekÅ›cie modeli zespoÅ‚owych, tj. uczenia siÄ™ kilku modeli poziomu 0 dla kaÅ¼dej zmiennej docelowej, ktÃ³re sÄ… Å‚Ä…czone w procedurze uogÃ³lniania poziomu 1 dla wielu zmiennych docelowych (Santana i in. 2020).\n\n2.2.1.1 Single-target stacking\nMetoda ta jest stosowana przede wszystkim z zadaniach regresyjnych z wieloma wyjÅ›ciami. RozwaÅ¼my zbiÃ³r danych \\(D = \\left\\{\\left(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(N)}, \\mathbf{y}^{(N)}\\right)\\right\\}\\), skÅ‚adajÄ…cy siÄ™ z \\(N\\) obserwacji, ktÃ³re sÄ… realizacjami zmiennych losowych \\(X_1,\\ldots,X_m, Y_1,\\ldots,Y_d\\). Zatem kaÅ¼de wejÅ›cie do modelu jest charakteryzowane przez \\(m\\) zmiennych \\(\\mathbf{x}{(l)}=\\left(x_1^{(l)},\\ldots, x_j^{(l)}, \\ldots, x_m^{(l)} \\right)\\) oraz \\(d\\) odpowiadajÄ…cych im wyjÅ›Ä‡ \\(\\mathbf{y}{(l)}=\\left(y_1^{(l)},\\ldots, y_i^{(l)}, \\ldots, y_d^{(l)} \\right)\\), gdzie \\(l\\in\\{1,\\ldots,N\\}, j\\in\\{1,\\ldots,m\\}, i\\in\\{1,\\ldots,d\\}\\). Naszym celem w zadaniu regresyjnym (MTR - Multi-target Regression) jest nauczenie takiego modelu \\(h\\), ktÃ³ry przeksztaÅ‚ca \\(\\mathbf{x}\\) w \\(\\mathbf{y}\\).\nW podejÅ›ciu STS w pierwszym kroku budowanych jest \\(d\\) niezaleÅ¼nych modeli przewidujÄ…cych pojedyncze wyjÅ›cie. Po tej czynnoÅ›ci meta-model jest trenowany na zbiorze \\(D_i'\\), ktÃ³ry jest wzbogaconym zbiorem \\(D_i\\) o predykcje zmiennej \\(Y_i\\), czyli\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)} \\right)\\). W zaleÅ¼noÅ›ci czy rozpatrujemy algorytm STS niekumulatywny, czy kumulatywny, drugi krok iteracji wyglÄ…da nieco inaczej:\n\nniekumulatywny\n\\[\n\\bar{D}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i'^{(l)} \\right)\\)\nkumulatywny\n\\[\n\\bar{\\bar{D}}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)},\\hat{y}_i'^{(l)} \\right)\\).\n\n\n\n\nSingle-target stacking\n\n\n\n\n2.2.1.2 Multi-target stacking\nW przeciwieÅ„stwie do STS, MTS zostaÅ‚ zaprojektowany do dzielenia siÄ™ wiedzÄ… w skorelowanych zmiennych docelowych w ramach procedury Å‚Ä…czenia w stosy. Podobnie, najpierw uczone sÄ… modele pojedynczego celu. NastÄ™pnie tworzony jest zestaw meta-modeli, ktÃ³re zawierajÄ… model dla kaÅ¼dej zmiennej docelowej \\(Y_i,\\) \\(i \\in \\{1, \\ldots, d\\}\\). W ten sposÃ³b uwzglÄ™dniane sÄ… szacunki dotyczÄ…ce pozostaÅ‚ych zmiennych docelowych z pierwszego etapu, tj. model jest uczony z przeksztaÅ‚conego zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_1^{(l)},\\ldots,\\hat{y}_d^{(l)} \\right)\\). W metodzie MTS istniejÄ… rÃ³wnieÅ¼ dwa sposoby skÅ‚adania kolejnych iteracji. PrzebiegajÄ… one w podobny sposÃ³b jak w przypadku STS.\n\n\n\nMulti-target stacking\n\n\nIstnieje jeszcze trzecia metoda powszechnie stosowana do predykcji wielowyniowej zwana Regressor Chains lub Classifier Chains w zaleÅ¼noÅ›ci od celu zadania. IdÄ™ dziaÅ‚ania tej metody przedstawiÄ™ na przykÅ‚adzie modelu regresyjnego.\n\n\n2.2.1.3 Regressor Chains\nRC opierajÄ… siÄ™ na idei dopasowywania modeli pojedynczego celu wzdÅ‚uÅ¼ wybranej permutacji, tj. Å‚aÅ„cucha. Najpierw losowana jest permutacja w odniesieniu do zmiennych docelowych. Proces ten moÅ¼na przeprowadziÄ‡ w sposÃ³b losowy (Spyromitros-Xioufis i in. 2016) lub uporzÄ…dkowany (Melki i in. 2017). Wybrana permutacja jest wykorzystywana do zbudowania oddzielnego modelu regresji dla zmiennych docelowych zgodnie z kolejnoÅ›ciÄ… permutacji. Aby wykorzystaÄ‡ tÄ™ strukturÄ™ do MTR, rzeczywiste wartoÅ›ci zmiennych docelowych sÄ… dostarczane do kolejnych modeli podczas uczenia siÄ™ wzdÅ‚uÅ¼ Å‚aÅ„cucha. Na podstawie peÅ‚nego Å‚aÅ„cucha lub wybranego zestawu \\(C = (Y_1,\\ldots,Y_d)\\), pierwszy model jest ograniczony do ustalenia predykcji dla \\(Y_1\\). NastÄ™pnie, kolejno dla \\(Y_i\\) uczone sÄ… modele na podstawie zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, y_1^{(l)},\\ldots, y_{i-1}^{(l)} \\right)\\). Ten algorytm ma rÃ³wnieÅ¼ dwie odmiany (niekumulatywnÄ… i kumulatywnÄ…) w zaleÅ¼noÅ›ci od ksztaÅ‚tu kolejnych iteracji.\n\n\n\nRegressor chains\n\n\nPoniewaÅ¼, jak moÅ¼na siÄ™ spodziewaÄ‡ wyniki modelowania w znaczny sposÃ³b zaleÅ¼Ä… od wylosowanej permutacji, to w metodzie zaproponowanej przez Melki i in. (2017) aby uniknÄ…Ä‡ tego efektu buduje siÄ™ \\(k\\) modeli dla rÃ³Å¼nych permutacji i Å‚Ä…czy siÄ™ wyniki w podobny sposÃ³b jak w lasach losowych.\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nSÅ‚owo komentarza jeÅ›li chodzi o dostÄ™pnoÅ›Ä‡ tych metod w jÄ™zykach programowania. Niestety wspomniane metody w R nie sÄ… zaimplementowane w sposÃ³b, ktÃ³ry pozwalaÅ‚by na bezpieczne uÅ¼ywanie przygotowanych rozwiÄ…zaÅ„. Istnieje kilka wzmianek (na dzieÅ„ dzisiejszy, czyli poczÄ…tek 2024 roku) na ten temat. TwÃ³rcy dwÃ³ch gÅ‚Ã³wnych frameworkÃ³w do uczenia maszynowego, czyli mlr3 oraz tidymodels przygotowujÄ… implementacje tych metod. Dodatkowo istnieje rozwiÄ…zanie w wersji eksperymentalnej mtr-toolkit, ktÃ³re pozwala na wykonanie modelowania z wieloma wyjÅ›ciami, ktÃ³rym moÅ¼na siÄ™ posiÅ‚kowaÄ‡. Na potrzeby klasyfikacji istnieje rÃ³wnieÅ¼ pakiet mldr i ultim, ktÃ³re pozwalajÄ… na uczenie modeli klasyfikacyjnymi z wieloma wyjÅ›ciami.\nNiestety w przypadku Python-a nie jest duÅ¼o lepiej. Wprawdzie w pakiecie scikit-learn istniejÄ… implementacje pozwalajÄ…ce na predykcje wielowyjÅ›ciowe w obu typach zadaÅ„ poprzez MultiOutputRegressor i MultiOutputClassifier, ale dokonujÄ… one predykcji naiwnej poprzez zÅ‚oÅ¼enie w listÄ™ wynikÃ³w pojedynczych modeli dla kaÅ¼dej zmiennej. Nieco lepiej sprawa wyglÄ…da w przypadku metod Å‚aÅ„cuchowych, poniewaÅ¼ zarÃ³wno dla klasyfikacji, jak i regresji sÄ… metody to realizujÄ…ce (ClassifierChain i RegressorChain).\n\n\n\n\n\n2.2.2 Adaptacja algorytmu\nProstota podejÅ›cia transformacji problemu sprawia, Å¼e jest ono odpowiednie dla problemÃ³w, w ktÃ³rych jego wady majÄ… niewielki lub Å¼aden wpÅ‚yw - jednak dla zÅ‚oÅ¼onych problemÃ³w podejÅ›cie adaptacji algorytmu moÅ¼e okazaÄ‡ siÄ™ bardziej efektywne. Dodatkowo, dowody empiryczne sugerujÄ…, Å¼e uczenie siÄ™ powiÄ…zanych zadaÅ„ jednoczeÅ›nie, a nie niezaleÅ¼nie, moÅ¼e poprawiÄ‡ wyniki predykcyjne (Evgeniou i Pontil 2004). Z drugiej strony, jeÅ›li zadania sÄ… bardzo odmienne, wydajnoÅ›Ä‡ predykcyjna moÅ¼e ucierpieÄ‡, gdy zadania sÄ… uczone razem, a nie niezaleÅ¼nie (Faddoul i in. 2010). W zwiÄ…zku z tym moÅ¼emy wyciÄ…gnÄ…Ä‡ nastÄ™pujÄ…ce wnioski:\n\njeÅ›li zadania, ktÃ³rych nasz predyktor ma siÄ™ nauczyÄ‡, sÄ… powiÄ…zane, powinniÅ›my dÄ…Å¼yÄ‡ do znalezienia odpowiedniej metody adaptacji algorytmu;\njeÅ›li zadania, ktÃ³rych chcemy siÄ™ nauczyÄ‡, nie sÄ… powiÄ…zane, powinniÅ›my zamiast tego dÄ…Å¼yÄ‡ do znalezienia odpowiedniej metody transformacji problemu.\n\nWreszcie, powinniÅ›my wziÄ…Ä‡ pod uwagÄ™ rozmiar problemu i zdaÄ‡ sobie sprawÄ™, Å¼e gdy zadania sÄ… niepowiÄ…zane, istnieje potencjalny kompromis miÄ™dzy efektywnoÅ›ciÄ… czasowÄ… a wydajnoÅ›ciÄ… predykcyjnÄ… przy wyborze metody transformacji problemu lub metody adaptacji algorytmu. W przypadku niepowiÄ…zanych ze sobÄ… zadaÅ„, metody transformacji problemu mogÄ… zwiÄ™kszaÄ‡ skutecznoÅ›Ä‡ predykcyjnÄ…, ale zmniejszaÄ‡ wydajnoÅ›Ä‡ czasowÄ… w przypadku duÅ¼ych problemÃ³w i odwrotnie.\nNiestety tej metody nie da siÄ™ zastosowaÄ‡ do kaÅ¼dego typu modelu. Rodzina modeli, ktÃ³rych adaptacja jest wykonana caÅ‚y czas roÅ›nie. Adaptacja modelu polega na przeksztaÅ‚ceniu go do postaci, w ktÃ³rej da siÄ™ wykonaÄ‡ predykcjÄ™ dla wielu wyjÅ›Ä‡. WÅ›rÃ³d modeli, ktÃ³rych wersje native multi-target istniejÄ… naleÅ¼y wymieniÄ‡:\n\nregresja wieloraka (Izenman 1975)\nkNN\ndrzewo decyzyjne (Struyf i DÅ¾eroski 2006)\nlas losowy (Kocev i in. 2013)\nbagging (Kocev i in. 2013)\ngradient boosting (Zhang i Jung, b.d.; Faddoul i in. 2012)\nSVM (Xu, Guo, i Wang 2013; Vazquez i Walter 2003)\nno i oczywiÅ›cie sieci neuronowe.\n\nNie sposÃ³b przedstawiÄ‡ w jaki sposÃ³b wprowadzone zostaÅ‚y zmiany we wszystkich algorytmach. SkupiÄ™ siÄ™ jednak na pokazaniu adaptacji drzew decyzyjnych do predykcji wielu wyjÅ›Ä‡ jednoczeÅ›nie, poniewaÅ¼ jest to meta-model modeli takich jak lasy losowe, bagging czy boosting.\n\n2.2.2.1 Adaptacja klasyfikacyjnego drzewa decyzyjnego\nFaddoul i in. (2012) zaproponowali zmodyfikowanÄ… wersjÄ™ algorytmu drzewa decyzyjnego C4.5 (Quinlan 1993), ktÃ³ra bezpoÅ›rednio obsÅ‚uguje problemy klasyfikacji wielowyjÅ›ciowej. Zmodyfikowana wersja (nazwana MT-DT) rÃ³Å¼ni siÄ™ od standardowej implementacji C4.5 w dwÃ³ch krytycznych aspektach: kryteriach podziaÅ‚u wÄ™zÅ‚Ã³w i procesie decyzyjnym. Faddoul i in. (2012) proponujÄ… trzy rÃ³Å¼ne podejÅ›cia do Å‚Ä…czenia wielu miar przyrostu informacji w jednÄ… miarÄ™: wspÃ³lny przyrost informacji, suma niewaÅ¼ona i maksymalny przyrost informacji. WspÃ³lny przyrost informacyjny jest definiowany przy uÅ¼yciu konkatenacji wszystkich poszczegÃ³lnych zadaÅ„, tj. wzglÄ™dnej rÃ³Å¼nicy w entropii mierzonej we wszystkich zadaniach decyzyjnych. Autorzy pokazujÄ…, Å¼e niewaÅ¼ona suma (RÃ³wnanieÂ 2.1) indywidualnych przyrostÃ³w informacyjnych wszystkich zadaÅ„ jest rÃ³wnowaÅ¼na wspÃ³lnemu przyrostowi informacyjnemu.\n\\[\nIG_U=\\sum_YIG_Y\n\\tag{2.1}\\]\nMaksymalny przyrost informacyjny, zgodnie z propozycjÄ… autorÃ³w jest definiowany po prostu jako maksymalny przyrost informacyjny wszystkich zadaÅ„:\n\\[\nIG_M=\\max_YIG_Y\n\\tag{2.2}\\]\nBadania eksperymentalne pokazaÅ‚y, Å¼e maksymalny przyrost informacyjny wykorzystany do budowania reguÅ‚ podziaÅ‚u, charakteryzuje siÄ™ wyÅ¼szym poziomem dopasowania modeli, niÅ¼ przy zastosowaniu \\(IG_U\\) i \\(IG_J\\).\nW przypadku klasyfikacji z jednÄ… etykietÄ…, algorytm indukcji drzewa decyzyjnego (taki jak C4.5) rekurencyjnie dzieli wÄ™zÅ‚y, dodajÄ…c (zazwyczaj dwa) elementy potomne, aÅ¼ moÅ¼liwe jest utworzenie liÅ›cia takiego, Å¼e znaczna wiÄ™kszoÅ›Ä‡ (lub nawet wszystkie) jego przykÅ‚adowych instancji naleÅ¼y do tej samej klasy. W przypadku wielu wyjÅ›Ä‡, indukcja drzewa niekoniecznie jest tak prosta. RozwaÅ¼my problem klasyfikacji wielowyjÅ›ciowej z dwoma wyjÅ›ciami binarnymi \\(\\nu_1\\) i \\(\\nu_2\\); moÅ¼liwe jest, Å¼e po \\(t\\) podziaÅ‚ach, wÄ™zeÅ‚ zawiera tylko wartoÅ›ci pozytywne dla \\(\\nu_1\\), ale mieszankÄ™ wartoÅ›ci pozytywnych i negatywnych dla \\(\\nu_2\\) - stÄ…d, podczas konstruowania drzew decyzyjnych dla wielu jednoczesnych zadaÅ„, naleÅ¼y pamiÄ™taÄ‡, Å¼e proces decyzyjny dla pewnego zadania moÅ¼e wymagaÄ‡ krÃ³tszej Å›cieÅ¼ki decyzyjnej niÅ¼ inne zadania w ramach tego samego problemu wielowyjÅ›ciowego. MT-DT radzi sobie z tym, sprawdzajÄ…c w kaÅ¼dym wÄ™Åºle, czy moÅ¼liwe jest utworzenie wÄ™zÅ‚a terminalnego dla ktÃ³regokolwiek z zadaÅ„ - w powyÅ¼szym przykÅ‚adzie spowodowaÅ‚oby to utworzenie drzewa, w ktÃ³rym wewnÄ™trzny wÄ™zeÅ‚ \\(t_1\\) jest oznaczony jako wÄ™zeÅ‚ zatrzymania dla \\(\\nu_1\\), oznaczony klasÄ… pozytywnÄ…. PoniewaÅ¼ celem jest prognozowanie dla obu wyjÅ›Ä‡ binarnych, \\(t_1\\) nie jest wÄ™zÅ‚em liÅ›cia - zamiast tego rekurencyjne dzielenie jest kontynuowane od \\(t_1\\), aÅ¼ do znalezienia wÄ™zÅ‚a \\(t_2\\) takiego, Å¼e \\(t_2\\) jest wystarczajÄ…co czysty w odniesieniu do \\(\\nu_2\\), aby moÅ¼na byÅ‚o utworzyÄ‡ reguÅ‚Ä™ klasyfikacji dla drugiego zadania binarnego. W tym momencie wÄ™zÅ‚y decyzyjne (wÄ™zÅ‚y wewnÄ™trzne lub liÅ›cie) zostaÅ‚y znalezione dla wszystkich wynikÃ³w (\\(\\nu_1\\) i \\(\\nu_2\\)), a algorytm indukcji drzewa rekurencyjnego moÅ¼e zostaÄ‡ zakoÅ„czony.\nNic dziwnego, Å¼e klasyfikacja przy uÅ¼yciu juÅ¼ zbudowanego modelu MT-DT przebiega wedÅ‚ug tej samej formuÅ‚y, co jego indukcja - podczas przechodzenia przez drzewo kaÅ¼dy wÄ™zeÅ‚ jest sprawdzany w celu ustalenia, czy moÅ¼na podjÄ…Ä‡ decyzjÄ™ dla ktÃ³regokolwiek z aktualnie nierozstrzygniÄ™tych zadaÅ„. W przykÅ‚adzie \\(\\nu_1\\), \\(\\nu_2\\), klasyfikacja zostanie dokonana dla \\(\\nu_1\\) w wÄ™Åºle \\(t_1\\), poniewaÅ¼ jest on oznaczony jako wÄ™zeÅ‚ zatrzymania dla \\(\\nu_1\\); nastÄ™pnie przejÅ›cie jest kontynuowane do momentu napotkania \\(t_2\\) i klasyfikacja moÅ¼e zostaÄ‡ dokonana dla \\(\\nu_2\\). W tym momencie wszystkie wyjÅ›cia zostaÅ‚y sklasyfikowane, a przechodzenie moÅ¼e siÄ™ zakoÅ„czyÄ‡, zwracajÄ…c dwie wartoÅ›ci w \\(t_1\\) i \\(t_2\\) jako klasyfikacje odpowiednio dla \\(\\nu_1\\) i \\(\\nu_2\\).\n\n\n2.2.2.2 Adaptacja regresyjnego drzewa decyzyjnego\nSegal (1992) zaproponowaÅ‚ rozwiÄ…zanie dla drzew regresyjnych o wielu wyjÅ›ciach (MRT), ktÃ³re sÄ… w stanie przewidywaÄ‡ wyniki dla wielu powiÄ…zanych zadaÅ„ regresyjnych; te wielowyjÅ›ciowe drzewa regresyjne sÄ… oparte na funkcji podziaÅ‚u najmniejszych kwadratÃ³w zaproponowanej w ramach CART (Breiman i in. 2017). W przypadku drzewa regresyjnego o jednej odpowiedzi celem jest minimalizacja nastÄ™pujÄ…cej funkcji celu:\n\\[\n\\phi(t) = SS(t)-SS(t_L)-SS(t_R)\n\\]\ngdzie \\(SS(t)\\) jest zdefiniowana nastÄ™pujÄ…co\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))^2.\n\\]\nSegal (1992) dodaÅ‚ waÅ¼enie macierzÄ… kowariancji do bÅ‚Ä™du kwadratowego, co prowadzi algorytm drzewa do tworzenia wÄ™zÅ‚Ã³w potomnych, ktÃ³re reprezentujÄ… jednorodne klastry w odniesieniu do zestawu odpowiedzi wyjÅ›ciowych:\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))'V^{-1}(t)(y_i-\\bar{y}(t)),\n\\]\ngdzie \\(V(t)\\) oznacza macierz kowariancji w wÄ™Åºle \\(t\\).\n\n\n2.2.2.3 Adaptacja drzew decyzyjnych do realizacji obu zadaÅ„\nJak wspomniano wczeÅ›niej, jednÄ… z kluczowych motywacji do podejmowania prÃ³b rozwiÄ…zywania problemÃ³w rozpoznawania wzorcÃ³w z wieloma wyjÅ›ciami przy uÅ¼yciu metod adaptacji algorytmÃ³w jest oczekiwanie, Å¼e pojedynczy model wytrenowany na zestawie powiÄ…zanych zadaÅ„ wykaÅ¼e poprawÄ™ wydajnoÅ›ci predykcyjnej w porÃ³wnaniu do zestawu indywidualnych modeli, z ktÃ³rych kaÅ¼dy zostaÅ‚ wytrenowany na pojedynczym zadaniu. Rodzi to pytanie: co jeÅ›li problem wielowynikowy zawiera zarÃ³wno zadania klasyfikacji, jak i regresji? JeÅ›li zadania sÄ… niepowiÄ…zane, rozwiÄ…zanie takiego wspÃ³lnego problemu klasyfikacyjno-regresyjnego nie musi byÄ‡ trudniejsze niÅ¼ szkolenie zestawu klasyfikatorÃ³w i regresorÃ³w dla poszczegÃ³lnych zadaÅ„; jeÅ›li jednak zadania sÄ… powiÄ…zane, oczekujemy, Å¼e metoda adaptacji algorytmu zapewni lepsze wyniki pod wzglÄ™dem wydajnoÅ›ci predykcyjnej.\nGlocker i in. (2012) zaproponowaÅ‚ algorytm indukcji drzewa, ktÃ³ry jednoczeÅ›nie rozwiÄ…zuje jedno zadanie klasyfikacji i jedno zadanie regresji. Podobnie jak MT-DT i MRT, wspÃ³lne drzewo klasyfikacyjno-regresyjne (JCRT) rozwiÄ…zuje wiele jednoczesnych zadaÅ„ predykcji poprzez modyfikacjÄ™ funkcji podziaÅ‚u wÄ™zÅ‚a w kroku indukcyjnym i oznaczenie wÄ™zÅ‚Ã³w koÅ„cowych odpowiednimi wartoÅ›ciami dla kaÅ¼dego zadania. Ze wzglÄ™du na charakter wspÃ³lnych problemÃ³w klasyfikacyjno-regresyjnych, zmodyfikowana funkcja podziaÅ‚u jest wymagana do jednoczesnego uwzglÄ™dnienia bÅ‚Ä™du zarÃ³wno czÄ™Å›ci klasyfikacyjnej, jak i regresyjnej. Funkcja podziaÅ‚u zaproponowana przez Glocker i in. (2012) wykorzystuje funkcjÄ™ entropii skÅ‚adajÄ…cÄ… siÄ™ z trzech czÄ™Å›ci:\n\npo pierwsze, entropia Shannona jest obliczana dla czÄ™Å›ci klasyfikacji;\npo drugie, waÅ¼ona entropia rÃ³Å¼nicowa jest obliczana dla czÄ™Å›ci regresji;\npo trzecie, ze wzglÄ™du na fakt, Å¼e entropia Shannona i entropia rÃ³Å¼nicowa istniejÄ… w rÃ³Å¼nych zakresach, stosuje siÄ™ krok normalizacji w celu poÅ‚Ä…czenia dwÃ³ch entropii. Entropia Shannona jest obliczana tak, jak opisano wczeÅ›niej:\n\n\\[\nH_c(t) = \\sum_{c\\in C}p(c|x)\\log p(c|x).\n\\]\nMiara entropii rÃ³Å¼niczkowej stosowana przez Glocker i in. (2012) dla regresyjnej czÄ™Å›ci problemu jest obliczana w podobny sposÃ³b, z dwiema kluczowymi rÃ³Å¼nicami:\n\nzamiast sumowania prawdopodobieÅ„stw wartoÅ›ci nominalnych, entropia jest definiowana przez rÃ³Å¼niczkÄ™ funkcji prawdopodobieÅ„stwa wyjÅ›cia o wartoÅ›ci rzeczywistej;\ndodatkowo funkcja prawdopodobieÅ„stwa jest waÅ¼ona w klasach:\n\n\\[\nH_{r|c}(t) = \\sum_{c\\in C}p(c|x)\\int_{r\\in \\mathbb{R}^n}-p(r|c,x)\\log p(r|c,x)dr.\n\\]\nNastÄ™pnie dokonywana jest normalizacja ze wzglÄ™du na oba zadania, gdzie punktem odniesienie jest entropia w korzeniu:\n\\[\nH(t) = \\frac12\\left(\\frac{H_c(t)}{H_c(t_0)}+\\frac{H_{r|c}(t)}{H_{r|c}(t_0)}\\right).\n\\]\n\n\n\n\n\n\nAdnotacja\n\n\n\nJedyne implementacje, ktÃ³re znalazÅ‚em dla obu jÄ™zykÃ³w programowania (R i Python) dotyczyÅ‚y lasÃ³w losowych. W R pakiet nazywa siÄ™ randomForestSRC, a w Pythonie morfist. PozwalajÄ… one zarÃ³wno na wykonywanie wielowyjÅ›ciowych zadaÅ„ klasyfikacyjnych i regresyjnych, jak rÃ³wnieÅ¼ zadaÅ„ mieszanych. OczywiÅ›cie wspomniane wyÅ¼ej typy zadaÅ„ moÅ¼na realizowaÄ‡ przy uÅ¼yciu sieci neuronowych w obu jÄ™zykach programowania.\n\n\n\n\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, i Pedro LarraÃ±aga. 2015. â€A Survey on Multi-Output Regressionâ€. WIREs Data Mining and Knowledge Discovery 5 (5): 216â€“33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, i Charles J. Stone. 2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, i Massimiliano Pontil. 2004. â€Regularized multiâ€“task learningâ€. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, sierpieÅ„. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, RÃ©mi Gilleron, i Fabien Torre. 2012. â€Learning multiple tasks with boosted decision treesâ€. W Proceedings of the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I, 681â€“96. ECMLPKDDâ€™12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, i Remi Gilleron. 2010. â€Boosting Multi-Task Weak Learners with Applications to Textual and Social Dataâ€. W 2010 Ninth International Conference on Machine Learning and Applications, 367â€“72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nGlocker, Ben, Olivier Pauly, Ender Konukoglu, i Antonio Criminisi. 2012. â€Joint Classification-Regression Forests for Spatially Structured Multi-object Segmentationâ€. W Computer Vision â€“ ECCV 2012, zredagowane przez Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, i Cordelia Schmid, 7575:870â€“81. Springer Berlin Heidelberg. http://link.springer.com/10.1007/978-3-642-33765-9_62.\n\n\nIzenman, Alan Julian. 1975. â€Reduced-rank regression for the multivariate linear modelâ€. Journal of multivariate analysis 5 (2): 248â€“64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, i SaÅ¡o DÅ¾eroski. 2013. â€Tree ensembles for predicting structured outputsâ€. Pattern Recognition 46 (3): 817â€“33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, i SebastiÃ¡n Ventura. 2017. â€Multi-Target Support Vector Regression via Correlation Regressor Chainsâ€. Information Sciences 415â€“416 (listopad): 53â€“69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Morgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello Mastelini, Fabio Luiz Melquiades, i Sylvio Barbon Jr. 2020. â€Improved Prediction of Soil Properties with Multi-Target Stacked Generalisation on EDXRF Spectraâ€. arXiv preprint arXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. â€Tree-Structured Methods for Longitudinal Dataâ€. Journal of the American Statistical Association 87 (418): 407â€“18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves, i Ioannis Vlahavas. 2016. â€Multi-Target Regression via Input Space Expansion: Treating Targets as Inputsâ€. Machine Learning 104 (1): 55â€“98. https://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, i SaÅ¡o DÅ¾eroski. 2006. â€Constraint Based Induction of Multi-objective Regression Treesâ€. W Knowledge Discovery in Inductive Databases, zredagowane przez Francesco Bonchi i Jean-FranÃ§ois Boulicaut, 222â€“33. Lecture Notes w Computer Science. Springer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, i Victor Sheng. 2013. â€Empirical Comparison of Multi-Label Classification Algorithmsâ€. W Proceedings of the AAAI Conference on Artificial Intelligence, 27:1645â€“46.\n\n\nTsoumakas, Grigorios, i Ioannis Katakis. 2007. â€Multi-Label Classification: An Overviewâ€. International Journal of Data Warehousing and Mining (IJDWM) 3 (3): 1â€“13.\n\n\nVazquez, Emmanuel, i Eric Walter. 2003. â€Multi-Output Suppport Vector Regressionâ€. IFAC Proceedings Volumes, 13th IFAC Symposium on System Identification (SYSID 2003), Rotterdam, The Netherlands, 27-29 August, 2003, 36 (16): 1783â€“88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, i Laisheng Wang. 2013. â€A Twin Multi-Class Classification Support Vector Machineâ€. Cognitive Computation 5 (4): 580â€“88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, i Cheolkon Jung. b.d. â€GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputsâ€. https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "3Â  PrzykÅ‚ady - metody klasyczne",
    "section": "",
    "text": "3.1 PrzykÅ‚ad 1\nNajpierw sformuÅ‚ujemy problem badawczy wymagajÄ…cy zastosowania modeli z wieloma wyjÅ›ciami.\nKod\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_regression\nKod\n# generowanie danych do zadania\nX, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)\nKod\n# Å‚Ä…czenie ich w ramki danych\nX_df = pd.DataFrame(i for i in X)\nX_df.columns = [\"X\"+ str(i) for i in range(1,11)]\ny_df = pd.DataFrame(i for i in y)\ny_df.columns = [\"y\"+str(i) for i in range(1,4)]\n\ndf = pd.concat([X_df,y_df], axis=1)\ndf.head()\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nX9\nX10\ny1\ny2\ny3\n\n\n\n\n0\n-0.547331\n0.426522\n-1.693585\n-0.740282\n0.277445\n-1.910679\n-0.320635\n1.449172\n-0.469619\n0.371273\n-92.924393\n-116.819352\n-32.117311\n\n\n1\n2.314630\n-0.936016\n-1.833034\n-0.394507\n-0.902981\n-1.089381\n1.005442\n-0.351289\n1.218223\n0.350258\n88.577211\n182.003040\n67.579488\n\n\n2\n-0.025423\n0.684189\n1.208964\n1.325672\n0.328946\n-0.354083\n-0.566556\n0.671359\n-0.560768\n0.327379\n115.652472\n63.462207\n43.519697\n\n\n3\n-0.044533\n0.603030\n-1.495716\n-0.507870\n-0.268485\n-0.140194\n-0.246658\n-0.758946\n2.567979\n1.808345\n-12.092227\n65.181041\n96.138770\n\n\n4\n2.083679\n0.318852\n-0.080982\n-1.284608\n0.281687\n0.792470\n-0.560598\n-1.368963\n0.718059\n-1.741815\n-18.573726\n-103.219393\n11.559200\nKod\ndf_X_boxplot = pd.melt(X_df)\nsns.boxplot(data = df_X_boxplot, x = \"variable\", y = \"value\")\nplt.show()\nKod\ndf_y_boxplot = pd.melt(y_df)\nsns.boxplot(data = df_y_boxplot, x = \"variable\", y = \"value\")\nplt.show()\nZarÃ³wno zmienne X, jak i y majÄ… zbliÅ¼one rozkÅ‚ady.\nKod\nsns.set_theme(style=\"white\")\ng = sns.PairGrid(y_df, diag_sharey=False)\ng.map_upper(sns.scatterplot, s=7)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.histplot)\nplt.show()\nJak widaÄ‡ z powyÅ¼szego wykresu zmienne y1,y2,y3 wykazujÄ… pewne wzajemne zaleÅ¼noÅ›ci, dlatego budowa oddzielnych modeli dla kaÅ¼dej ze zmiennych powinna dawaÄ‡ gorsze predykcje niÅ¼ modele wiÄ…Å¼Ä…ce wszystkie zmiennej w jednym modelu. MoÅ¼na teÅ¼ zauwaÅ¼yÄ‡, Å¼e rozkÅ‚ady sÄ… zbliÅ¼one do normalnego.\nKod\ncor = df.corr()\nplt.figure(figsize = (12,10))\nsns.heatmap(cor, \n            xticklabels=cor.columns.values,\n            yticklabels=cor.columns.values,\n            annot = True, fmt = '.2f')\nplt.show()\nJak widaÄ‡ z powyÅ¼szej macierzy korelacji, przynajmniej 8 spoÅ›rÃ³d 10 zmiennych X koreluje istotnie ze zmiennymi y.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>PrzykÅ‚ady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#przykÅ‚ad-1",
    "href": "examples.html#przykÅ‚ad-1",
    "title": "3Â  PrzykÅ‚ady - metody klasyczne",
    "section": "",
    "text": "PrzykÅ‚ad 3.1 Dane do zadania wygenerujemy za pomocÄ… funkcji make_regression() z pakietu sklern.dataset1. Wygenerujemy 700 obserwacji z 10 predyktorami i 3 zmiennymi zaleÅ¼nymi.\n1Â Nie znam R-owego odpowiednika",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>PrzykÅ‚ady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#rozwiÄ…zanie",
    "href": "examples.html#rozwiÄ…zanie",
    "title": "3Â  PrzykÅ‚ady - metody klasyczne",
    "section": "3.2 RozwiÄ…zanie",
    "text": "3.2 RozwiÄ…zanie\nNa potrzeby porÃ³wnania rÃ³Å¼nych rozwiÄ…zaÅ„ zbudujemy nastÄ™pujÄ…ce konfiguracje modeli:\n\ntrzy lasy losowe dla kaÅ¼dej zmiennej y niezaleÅ¼nie;\nmodel lasu losowego z wykorzystaniem reguÅ‚y Regression Chains jako transformacji modelu;\nlas losowy dla trzech zmiennych wynikowych jednoczeÅ›nie (adaptacja modelu)\n\nRozwiÄ…zania te porÃ³wnamy pod wzglÄ™dem dopasowania.\n\n3.2.1 NiezaleÅ¼ne lasy losowe\nFunkcja MultiOutputRegressor naÅ‚oÅ¼ona na model lasu losowego takie rozwiÄ…zanie tworzy.\n\n\nKod\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)\n\n\n\n\nKod\nrf_meta = RandomForestRegressor(random_state=4)\nrf_indep = MultiOutputRegressor(rf_meta)\nprint(rf_indep)\n\n\nMultiOutputRegressor(estimator=RandomForestRegressor(random_state=4))\n\n\n\n\nKod\nrf_indep.fit(X_train, y_train)\nr2_indep= rf_indep.score(X_test, y_test)\npred_indep = rf_indep.predict(X_test)\nrmse_indep = mean_squared_error(y_test, pred_indep, squared = False)\n\nprint(f'R2 on test samples: {r2_indep:.2f}')\nprint(f'RMSE on test samples: {rmse_indep:.1f}')\n\n\nR2 on test samples: 0.82\nRMSE on test samples: 71.0\n\n\n\n\n3.2.2 Regressor Chains RF\n\n\nKod\nrf_chains = RegressorChain(rf_meta)\nprint(rf_chains)\n\n\nRegressorChain(base_estimator=RandomForestRegressor(random_state=4))\n\n\n\n\nKod\nrf_chains.fit(X_train, y_train)\nr2_chains= rf_chains.score(X_test, y_test)\npred_chains = rf_chains.predict(X_test)\nrmse_chains = mean_squared_error(y_test, pred_chains, squared = False)\n\nprint(f'R2 on test samples: {r2_chains:.2f}')\nprint(f'RMSE on test samples: {rmse_chains:.1f}')\n\n\nR2 on test samples: 0.80\nRMSE on test samples: 72.3\n\n\n\n\n3.2.3 Adaptacja modelu RF\n\n\nKod\nprint(rf_meta)\n\n\nRandomForestRegressor(random_state=4)\n\n\n\n\nKod\nrf_meta.fit(X_train, y_train)\nr2_meta= rf_meta.score(X_test, y_test)\npred_meta = rf_meta.predict(X_test)\nrmse_meta = mean_squared_error(y_test, pred_meta, squared = False)\n\nprint(f'R2 on test samples: {r2_meta:.2f}')\nprint(f'RMSE on test samples: {rmse_meta:.1f}')\n\n\nR2 on test samples: 0.77\nRMSE on test samples: 77.9\n\n\n\n\n3.2.4 Podsumowanie\nBiorÄ…c pod uwagÄ™ miary dopasowania najlepiej poradziÅ‚ sobie z tym zadaniem model skÅ‚adajÄ…cy siÄ™ z trzech niezaleÅ¼nych modeli RF, potem model RF w wersji Regressor Chains, a najgorzej (o dziwo) radzi sobie z predykcjÄ… model korzystajÄ…cy adaptacji drzew decyzyjnych do wersji wielowyjÅ›ciowej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>PrzykÅ‚ady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#przykÅ‚ad-2",
    "href": "examples.html#przykÅ‚ad-2",
    "title": "3Â  PrzykÅ‚ady - metody klasyczne",
    "section": "3.3 PrzykÅ‚ad 2",
    "text": "3.3 PrzykÅ‚ad 2\nTym razem sformuÅ‚ujemy problem z wieloma wyjÅ›ciami ale klasyfikacyjny i rÃ³wnieÅ¼ dopasujemy trzy wersje modelu lasu losowego:\n\nlasy losowe dla kaÅ¼dej zmiennej y niezaleÅ¼nie;\nmodel lasu losowego z wykorzystaniem reguÅ‚y Classifier Chains jako transformacji modelu\nlas losowy dla wszystkich zmiennych wynikowych jednoczeÅ›nie (adaptacja modelu).\n\nAnalizowany problem bÄ™dzie prawdziwy i bÄ™dzie dotyczyÅ‚ klasyfikacji enzymÃ³w na podstawie cech charakterystycznych substratÃ³w. Pierwszych 31 zmiennych stanowi zmienne objaÅ›niajÄ…ce, a 6 pozostaÅ‚ych zmienne kodujÄ…ce przynaleÅ¼noÅ›Ä‡ do danej grupy enzymÃ³w.\n\n\nKod\ndt = pd.read_csv(\"data/original.csv\", index_col = \"id\")\ndt.head()\n\n\n\n\n\n\n\n\n\nBertzCT\nChi1\nChi1n\nChi1v\nChi2n\nChi2v\nChi3v\nChi4n\nEState_VSA1\nEState_VSA2\n...\nSlogP_VSA3\nVSA_EState9\nfr_COO\nfr_COO2\nEC1\nEC2\nEC3\nEC4\nEC5\nEC6\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC00009\n49.783540\n2.000000\n0.782574\n2.347723\n0.513277\n1.539831\n0.000000\n0.000000\n7.822697\n0.000000\n...\n4.565048\n16.923611\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00013\n147.355172\n3.707107\n1.530297\n4.590890\n1.062804\n3.678309\n1.914534\n0.138556\n15.645394\n0.000000\n...\n13.440728\n20.899028\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00014\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.150546\n...\n0.000000\n0.000000\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00017\n172.720106\n4.947265\n2.081214\n2.081214\n1.157830\n1.157830\n0.489278\n0.180980\n12.514062\n12.451936\n...\n9.589074\n35.105740\n1\n1\n0\n1\n1\n0\n0\n0\n\n\nC00022\n72.039100\n2.642734\n1.381855\n1.381855\n0.861339\n0.861339\n0.301176\n0.000000\n11.752550\n0.000000\n...\n9.589074\n25.333333\n1\n1\n1\n1\n1\n1\n0\n1\n\n\n\n\n5 rows Ã— 37 columns\n\n\n\n\n\nKod\ndt.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1039 entries, C00009 to C22220\nData columns (total 37 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   BertzCT            1039 non-null   float64\n 1   Chi1               1039 non-null   float64\n 2   Chi1n              1039 non-null   float64\n 3   Chi1v              1039 non-null   float64\n 4   Chi2n              1039 non-null   float64\n 5   Chi2v              1039 non-null   float64\n 6   Chi3v              1039 non-null   float64\n 7   Chi4n              1039 non-null   float64\n 8   EState_VSA1        1039 non-null   float64\n 9   EState_VSA2        1039 non-null   float64\n 10  ExactMolWt         1039 non-null   float64\n 11  FpDensityMorgan1   1039 non-null   float64\n 12  FpDensityMorgan2   1039 non-null   float64\n 13  FpDensityMorgan3   1039 non-null   float64\n 14  HallKierAlpha      1039 non-null   float64\n 15  HeavyAtomMolWt     1039 non-null   float64\n 16  Kappa3             1039 non-null   float64\n 17  MaxAbsEStateIndex  1039 non-null   float64\n 18  MinEStateIndex     1039 non-null   float64\n 19  NumHeteroatoms     1039 non-null   int64  \n 20  PEOE_VSA10         1039 non-null   float64\n 21  PEOE_VSA14         1039 non-null   float64\n 22  PEOE_VSA6          1039 non-null   float64\n 23  PEOE_VSA7          1039 non-null   float64\n 24  PEOE_VSA8          1039 non-null   float64\n 25  SMR_VSA10          1039 non-null   float64\n 26  SMR_VSA5           1039 non-null   float64\n 27  SlogP_VSA3         1039 non-null   float64\n 28  VSA_EState9        1039 non-null   float64\n 29  fr_COO             1039 non-null   int64  \n 30  fr_COO2            1039 non-null   int64  \n 31  EC1                1039 non-null   int64  \n 32  EC2                1039 non-null   int64  \n 33  EC3                1039 non-null   int64  \n 34  EC4                1039 non-null   int64  \n 35  EC5                1039 non-null   int64  \n 36  EC6                1039 non-null   int64  \ndtypes: float64(28), int64(9)\nmemory usage: 308.5+ KB\n\n\nSprawdzimy na ile niezbalansowane sÄ… poszczegÃ³lne klasy wynikowe.\n\n\nKod\ndt_deps = dt.loc[:,'EC1':'EC6']\ndt_deps = dt_deps.melt()\ndt_deps = dt_deps[dt_deps.value == 1]\nsns.countplot(data = dt_deps, x = 'variable')\n\n\n\n\n\n\n\n\n\nNiestety nie wszystkie klasy wystÄ™pujÄ… jednakowo czÄ™sto i moÅ¼e pojawiÄ‡ siÄ™ zjawisko, Å¼e kombinacja enzymÃ³w bÄ™dzie wystÄ™powaÅ‚a bardzo rzadko (np. raz). Jak widaÄ‡ z poniÅ¼szej tabeli faktycznie tak siÄ™ dzieje. To nie pozwala przeprowadziÄ‡ uczenia. Dlatego musimy poÅ‚Ä…czyÄ‡ pewne klasy enzymÃ³w aby uniemoÅ¼liwiÄ‡ takÄ… sytuacjÄ™.\n\n\nKod\ndt.loc[:,'EC1':'EC6'].value_counts()\n\n\nEC1  EC2  EC3  EC4  EC5  EC6\n1    0    0    0    0    0      178\n0    1    0    0    0    0      136\n1    1    0    0    0    0      112\n0    1    1    0    0    0       90\n     0    1    0    0    0       69\n1    1    0    1    0    0       38\n          1    0    0    0       37\n     0    0    1    0    0       33\n0    0    0    1    0    0       30\n     1    0    1    0    0       27\n               0    1    0       21\n1    0    1    0    0    0       20\n     1    0    0    0    1       19\n               1    0    1       15\n0    0    0    0    1    0       15\n1    1    0    0    1    0       14\n     0    0    0    1    0       12\n0    0    1    1    0    0       12\n          0    0    0    1       11\n1    1    0    1    1    1       11\n0    1    0    0    0    1       10\n          1    1    0    0        9\n1    0    0    1    1    0        8\n     1    1    1    0    1        8\n     0    0    1    0    1        7\n     1    1    0    0    1        7\n     0    0    0    0    1        7\n0    1    1    0    1    0        7\n                    0    1        7\n1    1    1    1    1    0        6\n          0    1    1    0        6\n0    1    0    1    0    1        6\n1    1    1    0    1    0        5\n0    0    0    1    1    0        5\n     1    1    1    1    0        4\n1    1    0    0    1    1        4\n          1    1    0    0        4\n0    0    1    0    0    1        4\n                    1    0        3\n1    1    1    1    1    1        3\n     0    1    1    0    0        3\n               0    0    1        3\n0    1    0    1    1    0        2\n                         1        2\n     0    0    1    0    1        2\n1    0    0    0    1    1        1\n0    0    1    1    0    1        1\n               0    1    1        1\n1    0    1    1    0    1        1\n               0    1    0        1\n0    1    1    1    0    1        1\n     0    1    1    1    0        1\nName: count, dtype: int64\n\n\nUsuniemy zatem takie kombinacje, ktÃ³re wystÄ™pujÄ… bardzo rzadko w danych.\n\n\nKod\ncombinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()\nidx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()\nbad_combinations = combinations[idx&lt;10]\nidx = []\nfor i in range(len(dt)):\n  idx.append(True)\n  for j in range(len(bad_combinations)):\n    if all(dt.iloc[i, 31:] == bad_combinations[j]):\n      idx[i]=False\n\ndt = dt.iloc[idx,:]\n\n\n\n\nKod\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier, ClassifierChain\ny = dt.iloc[:,31:].to_numpy()\nX = dt.iloc[:,:31].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n\n\n\n3.3.1 NiezaleÅ¼ne lasy losowe\n\n\nKod\nrf_meta = RandomForestClassifier(random_state=4)\nrf_indep = MultiOutputClassifier(rf_meta)\nprint(rf_indep)\n\nrf_indep.fit(X_train, y_train)\nacc_indep= rf_indep.score(X_test, y_test)\npred_indep = rf_indep.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_indep:.3f}')\n\n\nMultiOutputClassifier(estimator=RandomForestClassifier(random_state=4))\nAccuracy on test samples: 0.172\n\n\n\n\n3.3.2 Classifier Chains\n\n\nKod\nrf_chains = ClassifierChain(rf_meta)\nprint(rf_chains)\n\nrf_chains.fit(X_train, y_train)\nacc_chains= rf_chains.score(X_test, y_test)\npred_chains = rf_chains.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_chains:.3f}')\n\n\nClassifierChain(base_estimator=RandomForestClassifier(random_state=4))\nAccuracy on test samples: 0.187\n\n\n\n\n3.3.3 Adaptacja modelu RF\n\n\nKod\nrf_meta.fit(X_train, y_train)\nacc_meta= rf_meta.score(X_test, y_test)\npred_meta = rf_meta.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_meta:.3f}')\n\n\nAccuracy on test samples: 0.190\n\n\n\n\n3.3.4 Podsumowanie\nRÃ³wnieÅ¼ i tym razem nie widaÄ‡ wyraÅºnych rÃ³Å¼nic pomiÄ™dzy jakoÅ›ciÄ… dopasowania modeli. Model adaptowanego lasu losowego poradziÅ‚ sobie z zadanie najlepiej (19% poprawnoÅ›ci trafieÅ„), Classifier Chains daÅ‚ 18,7%, a niezaleÅ¼ne lasy losowe 17,2%.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>PrzykÅ‚ady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples2.html",
    "href": "examples2.html",
    "title": "4Â  PrzykÅ‚ady NN",
    "section": "",
    "text": "4.1 PrzykÅ‚ad 1\nRozwiÄ…zanie przykÅ‚adu z poprzedniego rozdziaÅ‚u moÅ¼na dokonaÄ‡ z duÅ¼o lepszÄ… precyzjÄ… wykorzystujÄ…c sieci neuronowe. W tym przykÅ‚adzie po raz kolejny wygenerujemy dane do uczenia1, a nastÄ™pnie wytrenujemy sieÄ‡ (dosyÄ‡ pÅ‚ytkÄ…) MLP.\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, ReLU, LeakyReLU, ELU\nfrom keras.callbacks import EarlyStopping\nimport keras\nKod\n# generowanie danych do zadania\nX, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)\n\nn_neurons = [10,20,50]\nModel bÄ™dzie bardzo prosty, skÅ‚adajÄ…cy siÄ™ z tylko jednej warstwy ukrytej.\nKod\ndef get_model(n_inputs, n_outputs, n_neurons):\n  model = Sequential()\n  model.add(Dense(int(n_neurons), input_dim=n_inputs, activation='relu'))\n  model.add(Dense(n_outputs, activation='linear'))\n  model.compile(loss='mse', optimizer='adam')\n  return model\nW tym przykÅ‚adzie chciaÅ‚em rÃ³wnieÅ¼ pokazaÄ‡ jak wykonywaÄ‡ trenowanie sieci z uÅ¼yciem sprawdzianu krzyÅ¼owego, ktÃ³ry pomoÅ¼e nam ustaliÄ‡ optymalnÄ… liczÄ™ neuronÃ³w w warstwie ukrytej.\nKod\n# ocena dopasowania modelu z wykorzystaniem CV\ndef evaluate_model(X, y, n_neurons):\n    results = list()\n    n_inputs, n_outputs = X.shape[1], y.shape[1]\n    # definicja CV\n    cv = RepeatedKFold(n_splits=5, random_state=1)\n    # pÄ™tla po foldach\n    for train_ix, test_ix in cv.split(X):\n        # przygotowanie danych\n        X_tr, X_te = X[train_ix], X[test_ix]\n        y_tr, y_te = y[train_ix], y[test_ix]\n        # okreÅ›lenie modelu\n        model = get_model(n_inputs, n_outputs, n_neurons)\n        # dopasowanie modelu\n        model.fit(X_tr, y_tr, verbose=0, epochs=100)\n        # ocena dopasowania na foldzie testowym\n        mae = model.evaluate(X_te, y_te, verbose=0)\n        results.append(mae)\n    return results\nKod\nresults = []\nfor i in n_neurons:\n  # dopasuj i oceÅ„ model na zbiorze uczÄ…cym\n  results.append(np.mean(evaluate_model(X_train, y_train, i)))\nKod\nresults = np.load(\"./data/mlp_eval.npz\")\nresults = results['arr_0'].tolist()\nfor i in range(len(n_neurons)):\n  print(f\"Dla {n_neurons[i]} neuronÃ³w MAE: {results[i]:.0f}\")\n\n\nDla 10 neuronÃ³w MAE: 16409\nDla 20 neuronÃ³w MAE: 9806\nDla 50 neuronÃ³w MAE: 3779\nNajlepszy rezultat osiÄ…gamy dla 50 neuronÃ³w i taki parametr dobierzemy w ostatecznym modelu.\nKod\nmy_callbacks = [\n    EarlyStopping(patience=2)\n]\n\nmodel = get_model(X_train.shape[1], y_train.shape[1], 50)\nhistory = model.fit(X_train, y_train, \n                    verbose=0, epochs=1000, \n                    validation_split=0.2, callbacks=my_callbacks)\n\n\nMetal device set to: Apple M1\nProces uczenia przebiegaÅ‚ prawidÅ‚owo i osiÄ…gniÄ™to niski poziom funkcji straty.\nKod\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\nWyniki dopasowania znacznie przekraczajÄ… wyniki uzyskane metodami z poprzedniego rozdziaÅ‚u.\nKod\ny_pred = model.predict(X_test, verbose=0)\nrmse_mlp = mean_squared_error(y_test, y_pred, squared=False)\nr2_mlp = r2_score(y_test, y_pred)\nprint(f\"R2 on test sample: {r2_mlp:.2f}\")\nprint(f\"RMSE on test sample: {rmse_mlp:.1f}\")\n\n\nR2 on test sample: 1.00\nRMSE on test sample: 5.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>PrzykÅ‚ady NN</span>"
    ]
  },
  {
    "objectID": "examples2.html#przykÅ‚ad-1",
    "href": "examples2.html#przykÅ‚ad-1",
    "title": "4Â  PrzykÅ‚ady NN",
    "section": "",
    "text": "1Â te same co z przykÅ‚adu regresyjnego z poprzedniego wykÅ‚adu",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>PrzykÅ‚ady NN</span>"
    ]
  },
  {
    "objectID": "examples2.html#przykÅ‚ad-2",
    "href": "examples2.html#przykÅ‚ad-2",
    "title": "4Â  PrzykÅ‚ady NN",
    "section": "4.2 PrzykÅ‚ad 2",
    "text": "4.2 PrzykÅ‚ad 2\nW tym przykÅ‚adzie jeszcze raz rozpatrzymy zadanie klasyfikacyjne z wieloma wyjÅ›ciami z poprzedniego rozdziaÅ‚u. Przeprowadzimy czynnoÅ›ci preprocessingu podobne jak w poprzednim rozdziale, dodajÄ…c jeszcze standaryzacjÄ™, ktÃ³ra dla sieci neuronowych jest bardzo waÅ¼na.\n\n\nKod\nfrom sklearn.preprocessing import StandardScaler\ndt = pd.read_csv(\"./data/original.csv\", index_col = \"id\")\ncombinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()\nidx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()\nbad_combinations = combinations[idx&lt;10]\nidx = []\nfor i in range(len(dt)):\n  idx.append(True)\n  for j in range(len(bad_combinations)):\n    if all(dt.iloc[i, 31:] == bad_combinations[j]):\n      idx[i]=False\n\ndt = dt.iloc[idx,:]\n\ny = dt.iloc[:,31:].to_numpy()\nX = dt.iloc[:,:31].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n\nscaler = StandardScaler().fit(X_train)\nX_test = scaler.transform(X_test)\nX_train = scaler.transform(X_train)\n\n\nNastÄ™pnie przygotujemy model sieci neuronowej, ktÃ³ra pozwoli na wÅ‚aÅ›ciwe klasyfikacje obiektÃ³w.\n\n\nKod\nmodel = Sequential()\nmodel.add(Dense(15, input_dim=X_train.shape[1]))\nmodel.add(ReLU())\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y_train.shape[1], activation='sigmoid'))\n\nopt = keras.optimizers.Nadam(0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=opt)\nmodel.summary()\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 15)                480       \n                                                                 \n re_lu (ReLU)                (None, 15)                0         \n                                                                 \n dropout (Dropout)           (None, 15)                0         \n                                                                 \n dense_3 (Dense)             (None, 6)                 96        \n                                                                 \n=================================================================\nTotal params: 576\nTrainable params: 576\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nKod\nmy_callbacks = [\n    EarlyStopping(patience=10)\n]\nhistory = model.fit(X_train, y_train, epochs=1000,  validation_split=0.4, verbose=1,\ncallbacks=my_callbacks)\n\n\nEpoch 1/1000\n 1/12 [=&gt;............................] - ETA: 20s - loss: 0.7379\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/12 [===========&gt;..................] - ETA: 0s - loss: 0.7970 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/12 [========================&gt;.....] - ETA: 0s - loss: 0.7768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - ETA: 0s - loss: 0.7780\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 2s 60ms/step - loss: 0.7780 - val_loss: 0.7309\nEpoch 2/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.7156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 5/12 [===========&gt;..................] - ETA: 0s - loss: 0.7571\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b10/12 [========================&gt;.....] - ETA: 0s - loss: 0.7412\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 16ms/step - loss: 0.7343 - val_loss: 0.6912\nEpoch 3/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.7013\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.6760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.6764 - val_loss: 0.6594\nEpoch 4/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.6821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6551\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.6556\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.6498 - val_loss: 0.6347\nEpoch 5/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.6335\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6372\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.6300\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.6276 - val_loss: 0.6154\nEpoch 6/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5903\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6029\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.6064\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.6021 - val_loss: 0.6002\nEpoch 7/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5759\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.6007\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5887 - val_loss: 0.5887\nEpoch 8/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.6046\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5850\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5869\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5850 - val_loss: 0.5795\nEpoch 9/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5990\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5808\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5718 - val_loss: 0.5715\nEpoch 10/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5631\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5456\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5521\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5510 - val_loss: 0.5647\nEpoch 11/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5435\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5482 - val_loss: 0.5592\nEpoch 12/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5536\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5441\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5418\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5432 - val_loss: 0.5544\nEpoch 13/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5369\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5437\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5454 - val_loss: 0.5503\nEpoch 14/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5451\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5402\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5386\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5391 - val_loss: 0.5466\nEpoch 15/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5341\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5283\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5292 - val_loss: 0.5436\nEpoch 16/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5668\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5375\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.5345 - val_loss: 0.5405\nEpoch 17/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5348\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5317 - val_loss: 0.5377\nEpoch 18/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5282\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5473\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5349\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5292 - val_loss: 0.5352\nEpoch 19/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5267\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5197\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.5194 - val_loss: 0.5329\nEpoch 20/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5193\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5226 - val_loss: 0.5314\nEpoch 21/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5733\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5302\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5197 - val_loss: 0.5296\nEpoch 22/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5432\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5118\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5110\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5103 - val_loss: 0.5279\nEpoch 23/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5075\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5162\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5114\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5139 - val_loss: 0.5267\nEpoch 24/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5231\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5005\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5069\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5112 - val_loss: 0.5257\nEpoch 25/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5170\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5045\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5087\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5084 - val_loss: 0.5244\nEpoch 26/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4575\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5066\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5043\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5017 - val_loss: 0.5232\nEpoch 27/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5069\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5038\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5072\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5097 - val_loss: 0.5222\nEpoch 28/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5287\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5075\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5077 - val_loss: 0.5213\nEpoch 29/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4963\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4988 - val_loss: 0.5204\nEpoch 30/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4713\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4968\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5088\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5038 - val_loss: 0.5198\nEpoch 31/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4479\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5086\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5009\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.5047 - val_loss: 0.5193\nEpoch 32/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5056\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4993 - val_loss: 0.5186\nEpoch 33/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4913\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4906 - val_loss: 0.5178\nEpoch 34/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.5023\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.4993 - val_loss: 0.5168\nEpoch 35/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4994\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4936\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4942 - val_loss: 0.5162\nEpoch 36/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4814\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4924\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4925 - val_loss: 0.5156\nEpoch 37/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4778\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4706\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4898 - val_loss: 0.5149\nEpoch 38/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4793\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4896\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4883 - val_loss: 0.5145\nEpoch 39/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5018\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4979\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4940\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4933 - val_loss: 0.5139\nEpoch 40/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.5008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4916\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4927 - val_loss: 0.5134\nEpoch 41/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4682\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4812\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4830\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4817 - val_loss: 0.5126\nEpoch 42/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5112\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4823\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4880 - val_loss: 0.5121\nEpoch 43/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4846\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4785\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4861 - val_loss: 0.5116\nEpoch 44/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4405\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4808\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4816 - val_loss: 0.5111\nEpoch 45/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4553\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4718\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4840\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4880 - val_loss: 0.5107\nEpoch 46/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.3993\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4767 - val_loss: 0.5103\nEpoch 47/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4710\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4794\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4834 - val_loss: 0.5103\nEpoch 48/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4980\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4770\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4789\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4824 - val_loss: 0.5099\nEpoch 49/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4817\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4811\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4760 - val_loss: 0.5095\nEpoch 50/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4907\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4855\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4803\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4794 - val_loss: 0.5090\nEpoch 51/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4992\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4792 - val_loss: 0.5087\nEpoch 52/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4982\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4655\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4731\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4749 - val_loss: 0.5083\nEpoch 53/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4884\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4812 - val_loss: 0.5079\nEpoch 54/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4747 - val_loss: 0.5076\nEpoch 55/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5205\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4866\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4751\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4736 - val_loss: 0.5073\nEpoch 56/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4894\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4754 - val_loss: 0.5069\nEpoch 57/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4827\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4708 - val_loss: 0.5068\nEpoch 58/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4760\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4642\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4736\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4765 - val_loss: 0.5067\nEpoch 59/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4315\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4727\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4777\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.4784 - val_loss: 0.5067\nEpoch 60/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4478\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4711 - val_loss: 0.5070\nEpoch 61/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4228\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4630\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4675 - val_loss: 0.5066\nEpoch 62/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4196\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4544\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.4690 - val_loss: 0.5061\nEpoch 63/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4761\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4728\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4722 - val_loss: 0.5062\nEpoch 64/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4270\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4617\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4745 - val_loss: 0.5064\nEpoch 65/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4821\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4772\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4676\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4697 - val_loss: 0.5062\nEpoch 66/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5521\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4854\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4681 - val_loss: 0.5061\nEpoch 67/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4781\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4699\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4706 - val_loss: 0.5062\nEpoch 68/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4986\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4635\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4670 - val_loss: 0.5067\nEpoch 69/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4416\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4515\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4658\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4683 - val_loss: 0.5066\nEpoch 70/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4828\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4611\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4702\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4697 - val_loss: 0.5064\nEpoch 71/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4562\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4687\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4665 - val_loss: 0.5062\nEpoch 72/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4557\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4668\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4699 - val_loss: 0.5061\nEpoch 73/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.3911\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4695\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4660\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4655 - val_loss: 0.5060\nEpoch 74/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4734\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4714 - val_loss: 0.5061\nEpoch 75/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5306\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4768\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4737\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4688 - val_loss: 0.5065\nEpoch 76/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4074\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4492\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4646 - val_loss: 0.5063\nEpoch 77/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4487\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4746\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4638\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4649 - val_loss: 0.5062\nEpoch 78/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4848\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4775\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4633 - val_loss: 0.5060\nEpoch 79/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4610\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4643\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4653 - val_loss: 0.5060\nEpoch 80/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4529\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4586\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4627\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4602 - val_loss: 0.5057\nEpoch 81/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4433\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4645\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4621 - val_loss: 0.5059\nEpoch 82/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4350\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4663\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4644 - val_loss: 0.5061\nEpoch 83/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4153\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4530\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4621\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4608 - val_loss: 0.5063\nEpoch 84/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.3970\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4599\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4600 - val_loss: 0.5065\nEpoch 85/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4605\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4644\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4587\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4619 - val_loss: 0.5066\nEpoch 86/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5031\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4773\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4626\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4633 - val_loss: 0.5066\nEpoch 87/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4935\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4572\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4592\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 15ms/step - loss: 0.4593 - val_loss: 0.5064\nEpoch 88/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.5105\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4767\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4596 - val_loss: 0.5065\nEpoch 89/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4389\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4735\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4657\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4618 - val_loss: 0.5065\nEpoch 90/1000\n 1/12 [=&gt;............................] - ETA: 0s - loss: 0.4063\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 6/12 [==============&gt;...............] - ETA: 0s - loss: 0.4548\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b11/12 [==========================&gt;...] - ETA: 0s - loss: 0.4611\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12/12 [==============================] - 0s 14ms/step - loss: 0.4626 - val_loss: 0.5066\n\n\n\n\nKod\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nKod\nfrom sklearn.metrics import accuracy_score\ny_pred = model.predict(X_test, verbose=0)\ny_class = y_pred.round()\nacc = accuracy_score(y_test, y_class)\nprint(f\"ACC on test sample: {acc:.3f}\")\n\n\nACC on test sample: 0.183\n\n\n\n4.2.1 Podsumowanie\nModel prostej sieci neuronowej nie poprawiÅ‚ znaczÄ…co jakoÅ›ci dopasowania w stosunku do modelu lasu losowego adaptowanego do zadania z wieloma wyjÅ›ciami.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>PrzykÅ‚ady NN</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Borchani, Hanen, Gherardo Varando, Concha Bielza, and Pedro LarraÃ±aga.\n2015. â€œA Survey on Multi-Output Regression.â€ WIREs Data\nMining and Knowledge Discovery 5 (5): 216â€“33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, and Charles J. Stone.\n2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, and Massimiliano Pontil. 2004. â€œRegularized\nMultiâ€“Task Learning.â€ Proceedings of the Tenth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\nAugust. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, RÃ©mi Gilleron, and Fabien\nTorre. 2012. â€œLearning Multiple Tasks with Boosted Decision\nTrees.â€ In Proceedings of the 2012th European Conference on\nMachine Learning and Knowledge Discovery in Databases - Volume Part\ni, 681â€“96. ECMLPKDDâ€™12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, and Remi\nGilleron. 2010. â€œBoosting Multi-Task Weak Learners with\nApplications to Textual and Social Data.â€ In 2010 Ninth\nInternational Conference on Machine Learning and Applications,\n367â€“72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nGlocker, Ben, Olivier Pauly, Ender Konukoglu, and Antonio Criminisi.\n2012. â€œJoint Classification-Regression Forests for Spatially\nStructured Multi-Object Segmentation.â€ In Computer Vision â€“\nECCV 2012, edited by Andrew Fitzgibbon, Svetlana Lazebnik, Pietro\nPerona, Yoichi Sato, and Cordelia Schmid, 7575:870â€“81. Springer Berlin\nHeidelberg. http://link.springer.com/10.1007/978-3-642-33765-9_62.\n\n\nIzenman, Alan Julian. 1975. â€œReduced-Rank Regression for the\nMultivariate Linear Model.â€ Journal of Multivariate\nAnalysis 5 (2): 248â€“64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, and SaÅ¡o DÅ¾eroski. 2013.\nâ€œTree Ensembles for Predicting Structured Outputs.â€\nPattern Recognition 46 (3): 817â€“33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, and SebastiÃ¡n Ventura.\n2017. â€œMulti-Target Support Vector Regression via Correlation\nRegressor Chains.â€ Information Sciences 415â€“416\n(November): 53â€“69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning.\nMorgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello\nMastelini, Fabio Luiz Melquiades, and Sylvio Barbon Jr. 2020.\nâ€œImproved Prediction of Soil Properties with Multi-Target Stacked\nGeneralisation on EDXRF Spectra.â€ arXiv Preprint\narXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. â€œTree-Structured Methods for\nLongitudinal Data.â€ Journal of the American Statistical\nAssociation 87 (418): 407â€“18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves,\nand Ioannis Vlahavas. 2016. â€œMulti-Target Regression\nvia Input Space Expansion: Treating Targets as\nInputs.â€ Machine Learning 104 (1): 55â€“98.\nhttps://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, and SaÅ¡o DÅ¾eroski. 2006. â€œConstraint Based Induction\nof Multi-Objective Regression Trees.â€ In Knowledge Discovery\nin Inductive Databases, edited by Francesco Bonchi and\nJean-FranÃ§ois Boulicaut, 222â€“33. Lecture Notes in Computer Science.\nSpringer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, and Victor Sheng. 2013. â€œEmpirical Comparison of\nMulti-Label Classification Algorithms.â€ In Proceedings of the\nAAAI Conference on Artificial\nIntelligence, 27:1645â€“46.\n\n\nTsoumakas, Grigorios, and Ioannis Katakis. 2007. â€œMulti-Label\nClassification: An Overview.â€ International\nJournal of Data Warehousing and Mining (IJDWM) 3 (3): 1â€“13.\n\n\nVazquez, Emmanuel, and Eric Walter. 2003. â€œMulti-Output Suppport\nVector Regression.â€ IFAC Proceedings Volumes, 13th IFAC\nsymposium on system identification (SYSID 2003), rotterdam, the\nnetherlands, 27-29 august, 2003, 36 (16): 1783â€“88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, and Laisheng Wang. 2013. â€œA Twin Multi-Class\nClassification Support Vector Machine.â€ Cognitive\nComputation 5 (4): 580â€“88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, and Cheolkon Jung. n.d. â€œGBDT-MO: Gradient\nBoosted Decision Trees for Multiple Outputs.â€ https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "Literatura"
    ]
  }
]