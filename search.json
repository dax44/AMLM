[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaawansowane metody uczenia maszynowego",
    "section": "",
    "text": "Wstęp\nKsiążka ta jest napisana na potrzeby prowadzenia zajęć na kierunku Inżynieria i analiza danych z przedmiotu Zaawansowane metody uczenia maszynowego. Jest swego rodzaju autorskim podejściem do tematu, przedstawiającym wybrane metody uczenia maszynowego, które rzadziej występują w opracowaniach na temat uczenia maszynowego.\nUczenie maszynowe stanowi obszar intensywnego rozwoju, który obejmuje szereg technik umożliwiających bardziej skomplikowane i wydajne modele predykcyjne. Wśród tych metod warto wyróżnić głębokie sieci neuronowe, zwłaszcza konwolucyjne sieci neuronowe (CNN) i rekurencyjne sieci neuronowe (RNN). CNN są wykorzystywane w zadaniach przetwarzania obrazów, gdzie potrafią efektywnie ekstrahować hierarchiczne cechy z danych wejściowych, natomiast RNN są efektywne w analizie sekwencji danych, takich jak język naturalny. Ponadto, metody uczenia maszynowego obejmują techniki transferu wiedzy, uczenie ze wzmocnieniem, generatywne modele, takie jak generatywne sieci przeciwdziedzinowe (GAN), czy też autokodery. Te nowoczesne podejścia umożliwiają modelom uczącym się wykonywanie bardziej złożonych zadań, a także adaptację do różnorodnych danych wejściowych, co sprawia, że są one stosowane w obszarach takich jak rozpoznawanie obrazów, przetwarzanie języka naturalnego, czy nawet w autonomicznych systemach decyzyjnych.\nPonadto, zaawansowane metody uczenia maszynowego obejmują także techniki regularyzacji, optymalizacji i inżynierię cech. Regularyzacja ma na celu zapobieganie przeuczeniu poprzez kontrolowanie złożoności modelu, natomiast optymalizacja skupia się na dostosowywaniu wag modelu w celu minimalizacji funkcji straty. Inżynieria cech polega na ręcznym lub automatycznym dostosowywaniu danych wejściowych w celu uzyskania lepszych wyników modelu. Dzięki tym zaawansowanym metodom, uczenie maszynowe staje się coraz bardziej potężnym narzędziem w analizie danych i podejmowaniu skomplikowanych decyzji w różnych dziedzinach.\nModele predykcyjne dla wielu wyjść, czyli tzw. multi-target regression and classification, stanowią kolejny istotny obszar w dziedzinie uczenia maszynowego. W przypadku multi-target regression, celem jest przewidywanie wielu wartości wyjściowych dla danego zestawu wejściowego, co często spotyka się w złożonych problemach predykcyjnych, takich jak prognozowanie wielu parametrów jednocześnie. Z kolei w przypadku multi-target classification, model ma za zadanie przypisanie jednego lub więcej klas dla każdego przykładu wejściowego. Te modele są powszechnie stosowane w różnych dziedzinach, takich jak bioinformatyka, finanse czy przemysł, gdzie jednoczesne przewidywanie wielu zmiennych jest kluczowe dla skutecznego rozwiązania problemu. Wdrożenie takich zaawansowanych modeli predykcyjnych wymaga starannej obróbki danych, odpowiedniego dostosowania architektury modelu oraz precyzyjnej oceny wyników, co sprawia, że są one istotnym narzędziem w obszarze analizy danych i podejmowania decyzji.\nModele językowe stanowią jeszcze jeden kluczowy obszar w dziedzinie uczenia maszynowego, skoncentrowany na zrozumieniu i generowaniu ludzkiego języka naturalnego. Głębokie sieci neuronowe, zwłaszcza rekurencyjne sieci neuronowe (RNN) i transformery, zostały skutecznie wykorzystane do tworzenia modeli językowych o zdolnościach przetwarzania i generowania tekstu na poziomie zbliżonym do ludzkiego. Te modele zdolne są do zrozumienia kontekstu, analizy gramatyki, a także generowania spójnych i sensownych odpowiedzi. Wykorzystywane są w różnorodnych zastosowaniach, takich jak tłumaczenie maszynowe, generowanie tekstu, czy analiza nastroju w tekście. Ponadto, pre-trenowane modele językowe, takie jak BERT czy GPT (Generative Pre-trained Transformer), zdobywają popularność, umożliwiając dostosowanie ich do różnych zadań poprzez fine-tuning. W miarę postępu badań i rozwoju w tej dziedzinie, modele językowe stają się coraz bardziej zaawansowane, co przyczynia się do doskonalenia komunikacji między maszynami a ludźmi oraz do rozwijania nowych możliwości w dziedzinie przetwarzania języka naturalnego.\nWspomniane powyżej metody i modele będą stanowić treść wykładów z wspomnianego na wstępie przedmiotu.",
    "crumbs": [
      "Wstęp"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Wprowadzenie",
    "section": "",
    "text": "Witam w świecie zaawansowanych metod uczenia maszynowego 🤖, prezentowanej w niniejszej publikacji. Książka ta skupia się na trzech głównych obszarach, zaczynając od wielowymiarowych problemów predykcyjnych, przechodząc przez kompleksowe modele głębokich sieci neuronowych, a kończąc na zaawansowanych modelach językowych. Koncepcyjnie rozpoczniemy od omówienia multiple target regression and classification, gdzie przedstawimy skomplikowane zadania predykcyjne wymagające jednoczesnej prognozy wielu zmiennych. Przeanalizujemy praktyczne zastosowania tych modeli w obszarach, takich jak nauki społeczne, biologia i finanse.\nNastępnie poświęcimy uwagę głębokim sieciom neuronowym, głównemu filarowi nowoczesnej sztucznej inteligencji. Omówimy ewolucję od konwolucyjnych sieci neuronowych (CNN) do rekurencyjnych sieci neuronowych (RNN), zwracając uwagę na ich zdolność do efektywnego przetwarzania obrazów, sekwencji danych i rozwiązania bardziej złożonych problemów. W ramach tego obszaru, przyjrzymy się również technikom transferu wiedzy, uczenia ze wzmocnieniem oraz generatywnym modelom, takim jak generatywne sieci przeciwdziedzinowe (GAN), które poszerzają granice możliwości maszynowego uczenia się.\n\n\n\nTrzeci kluczowy obszar, który będzie przedmiotem analizy, to modele językowe. Rozważania rozpoczniemy od głębokich sieci neuronowych, a następnie skoncentrujemy się na transformatorach, które rewolucjonizują przetwarzanie języka naturalnego 👅. Przedstawimy praktyczne zastosowania tych modeli, zwłaszcza w tłumaczeniu maszynowym, generowaniu tekstu i analizie sentymentu. Ponadto, omówimy pre-trenowane modele językowe, takie jak BERT czy GPT, jako kluczowe narzędzia adaptacyjne, zdolne do fine-tuningu w zależności od konkretnego zadania.\nKażdy podejmowany temat będzie wzbogacony o implementację analizowanych metod w realnych scenariuszach. Omówimy kroki od obróbki danych, przez dostosowywanie architektury modelu, aż po ocenę wyników. W tym kontekście poruszymy także aspekty etyczne i wyzwania związane z zastosowaniem zaawansowanych modeli uczenia maszynowego.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html",
    "href": "multi_target_models.html",
    "title": "2  Modele z wieloma wyjściami",
    "section": "",
    "text": "2.1 Typy modeli z wieloma zmiennymi wynikowymi\nWśród nadzorowanych modeli uczenia maszynowego z wieloma zmiennymi wynikowymi można wymienić zarówno te dedykowane do klasyfikacji, jak i regresji. Modele te są znane jako modele z wieloma wyjściami (klasyfikacyjne) lub modele z wieloma wyjściami (regresyjne), w zależności od rodzaju problemu, który rozwiązują.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "href": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "title": "2  Modele z wieloma wyjściami",
    "section": "",
    "text": "Modele z wieloma wyjściami (klasyfikacyjne)\nW przypadku klasyfikacji, gdy mamy wiele kategorii (klas) jako zmienną wynikową, modele te są nazywane modelami z wieloma wyjściami. Przykłady obejmują algorytmy takie jak regresja logistyczna, metoda k najbliższych sąsiadów (k-NN) czy algorytmy drzew decyzyjnych, które zostały dostosowane do obsługi wielu klas.\nPrzykładowe zadanie: Załóżmy, że mamy zbiór danych dotyczący różnych rodzajów owoców (np. jabłek, pomarańczy, bananów) i chcemy stworzyć model, który jednocześnie przewiduje gatunek owocu oraz kolor owocu. Mamy więc dwie zmienne wynikowe: gatunek (klasyfikacja wieloklasowa) i kolor (klasyfikacja wieloklasowa).\nModele z wieloma wyjściami (regresyjne).\nW przypadku regresji, gdzie zmienną wynikową jest wektor wartości numerycznych, modele te są nazywane modelami z wieloma wyjściami. Przykłady obejmują algorytmy regresji liniowej lub nieliniowej, algorytmy oparte na drzewach decyzyjnych, czy też bardziej zaawansowane modele, takie jak sieci neuronowe.\nPrzykładowe zadanie: Zakładamy, że mamy zbiór danych zawierający informacje o pracownikach, takie jak doświadczenie zawodowe, poziom wykształcenia, liczba godzin pracy tygodniowo itp. Chcemy stworzyć model, który jednocześnie przewiduje zarobki pracowników oraz ich poziom satysfakcji zawodowej.\nModele wielozadaniowe.\nModele wielozadaniowe to rodzaj nadzorowanego uczenia maszynowego, w którym model jest trenowany jednocześnie do rozwiązania kilku zadań. Te zadania mogą obejmować zarówno klasyfikację, jak i regresję. Dzięki wspólnemu trenowaniu modelu na wielu zadaniach, można uzyskać korzyści w postaci wspólnego wykorzystywania wiedzy między zadaniami.\nPrzykładowe zadanie: Załóżmy, że mamy zbiór danych dotyczący zakupów klientów w sklepie internetowym. Dla każdego klienta mamy informacje o różnych aspektach zakupów, takich jak czas dostawy, łatwość obsługi strony, jakość produktów itp. Chcemy stworzyć model, który jednocześnie przewiduje dwie zmienne wynikowe: jakość obsługi klienta (skala jakościowa, np. “Niska”, “Średnia”, “Wysoka”) oraz całkowity wydatek klienta (zmienna ilościowa, np. kwota zakupów).\nModele hierarchiczne.\nW niektórych przypadkach, szczególnie gdy mamy hierarchię zmiennych wynikowych, modele te mogą być budowane w sposób hierarchiczny. Przykładowo, w problemie klasyfikacji obrazów z hierarchią kategorii (na przykład rozpoznawanie gatunków zwierząt), model może być zaprojektowany do rozpoznawania zarówno ogólnych, jak i bardziej szczegółowych kategorii.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#różnie-podejścia-do-modelowania-z-wieloma-wyjściami",
    "href": "multi_target_models.html#różnie-podejścia-do-modelowania-z-wieloma-wyjściami",
    "title": "2  Modele z wieloma wyjściami",
    "section": "2.2 Różnie podejścia do modelowania z wieloma wyjściami",
    "text": "2.2 Różnie podejścia do modelowania z wieloma wyjściami\nIstnieją dwa ogólne podejścia do rozwiązywania problemów wieloetykietowych: transformacja problemu i adaptacja algorytmu. Transformacja problemu polega na manipulowaniu zbiorem danych w taki sposób, że problem wieloetykietowy staje się jednym lub kilkoma problemami jednoetykietowymi (Tawiah i Sheng 2013). Adaptacja algorytmu polega na tym, że sam algorytm jest w stanie poradzić sobie bezpośrednio z problemem wieloetykietowym. Okazuje się, że wiele, choć nie wszystkie, metody adaptacji algorytmów metod adaptacji algorytmów w rzeczywistości wykorzystuje transformację problemu (Tsoumakas i Katakis 2007).\n\n2.2.1 Transformacja problemu\nTechniki te przewidują stworzenie indywidualnego modelu dla każdego celu, a następnie połączenie oddzielnych modeli w celu uzyskania ogólnej prognozy. Metody transformacji problemów okazały się lepsze od metod adaptacji algorytmów pod względem dokładności (Spyromitros-Xioufis i in. 2016). Co więcej, podstawowa zasada sprawia, że metody transformacji problemu są niezależne od algorytmu. W konsekwencji, można je łatwo dostosować do danego problemu poprzez zastosowanie odpowiednich bazowych metod uczących. Punkt ten ma również szczególne znaczenie dla modeli typu ensemble, które łączą oszacowania z wielu potencjalnie różnych algorytmów w ostateczną prognozę. Niedawno Spyromitros-Xioufis i in. (2016) zaproponowali rozszerzenie znanych metod transformacji klasyfikacji wieloetykietowej, aby poradzić sobie z problemem regresji wielowynikowej i modelować zależności między celami. W szczególności wprowadzili oni dwa nowe podejścia do regresji wielocelowej, składanie regresorów wielocelowych i łańcuchy regresorów, inspirowane popularnymi i skutecznymi podejściami do klasyfikacji wieloznaczeniowej.\nPodstawową koncepcją w metodach transformacji problemów jest wykorzystanie poprzednich modeli do nowego przewidywania poprzez rozszerzoną przestrzeń cech (Borchani i in. 2015). Stacked generalization to podejście do meta-uczenia, które wykorzystuje dane wyjściowe wcześniej wyuczonych modeli do uczenia się nowego modelu. W związku z tym początkowe dane wyjściowe modelu są traktowane jako nowe cechy i są układane w stos do początkowego wektora cech przed ponownym uczeniem. W oryginalnym sformułowaniu przewidziano tylko dwuetapową procedurę, tj. początkowe modele wyuczone z początkowego wektora cech odpowiadają odpowiednio modelom i danym poziomu 0, a powiększony wektor cech i ponownie wyuczony model są określane odpowiednio jako dane poziomu 1 i generalizator. Jednakże, rozsądnie rzecz biorąc, ten proces układania pojedynczego celu (ang. Single-target Stacking - STS) może być również przeprowadzany w wielu iteracjach. Aby wdrożyć tę zasadę dla problemów z wieloma celami, w których kodowane są również możliwe korelacje między zmiennymi docelowymi, wprowadzono koncepcję układania wielu celów (ang. Multi-target Stacking - MTS) (Borchani i in. 2015). Analogicznie do STS, szkolenie modelu MTS można uznać za procedurę dwuetapową. W pierwszym etapie uczone są niezależne modele dla każdej zmiennej docelowej. Następnie uczone są meta-modele dla każdej zmiennej docelowej z rozszerzonymi wektorami cech, które zawierają początkowe wektory cech, a także oszacowania poziomu 0 pozostałych zmiennych docelowych. Podobne pomysły były również stosowane w kontekście modeli zespołowych, tj. uczenia się kilku modeli poziomu 0 dla każdej zmiennej docelowej, które są łączone w procedurze uogólniania poziomu 1 dla wielu zmiennych docelowych (Santana i in. 2020).\n\n2.2.1.1 Single-target stacking\nMetoda ta jest stosowana przede wszystkim z zadaniach regresyjnych z wieloma wyjściami. Rozważmy zbiór danych \\(D = \\left\\{\\left(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(N)}, \\mathbf{y}^{(N)}\\right)\\right\\}\\), składający się z \\(N\\) obserwacji, które są realizacjami zmiennych losowych \\(X_1,\\ldots,X_m, Y_1,\\ldots,Y_d\\). Zatem każde wejście do modelu jest charakteryzowane przez \\(m\\) zmiennych \\(\\mathbf{x}{(l)}=\\left(x_1^{(l)},\\ldots, x_j^{(l)}, \\ldots, x_m^{(l)} \\right)\\) oraz \\(d\\) odpowiadających im wyjść \\(\\mathbf{y}{(l)}=\\left(y_1^{(l)},\\ldots, y_i^{(l)}, \\ldots, y_d^{(l)} \\right)\\), gdzie \\(l\\in\\{1,\\ldots,N\\}, j\\in\\{1,\\ldots,m\\}, i\\in\\{1,\\ldots,d\\}\\). Naszym celem w zadaniu regresyjnym (MTR - Multi-target Regression) jest nauczenie takiego modelu \\(h\\), który przekształca \\(\\mathbf{x}\\) w \\(\\mathbf{y}\\).\nW podejściu STS w pierwszym kroku budowanych jest \\(d\\) niezależnych modeli przewidujących pojedyncze wyjście. Po tej czynności meta-model jest trenowany na zbiorze \\(D_i'\\), który jest wzbogaconym zbiorem \\(D_i\\) o predykcje zmiennej \\(Y_i\\), czyli\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)} \\right)\\). W zależności czy rozpatrujemy algorytm STS niekumulatywny, czy kumulatywny, drugi krok iteracji wygląda nieco inaczej:\n\nniekumulatywny\n\\[\n\\bar{D}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i'^{(l)} \\right)\\)\nkumulatywny\n\\[\n\\bar{\\bar{D}}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)},\\hat{y}_i'^{(l)} \\right)\\).\n\n\n\n\nSingle-target stacking\n\n\n\n\n2.2.1.2 Multi-target stacking\nW przeciwieństwie do STS, MTS został zaprojektowany do dzielenia się wiedzą w skorelowanych zmiennych docelowych w ramach procedury łączenia w stosy. Podobnie, najpierw uczone są modele pojedynczego celu. Następnie tworzony jest zestaw meta-modeli, które zawierają model dla każdej zmiennej docelowej \\(Y_i,\\) \\(i \\in \\{1, \\ldots, d\\}\\). W ten sposób uwzględniane są szacunki dotyczące pozostałych zmiennych docelowych z pierwszego etapu, tj. model jest uczony z przekształconego zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_1^{(l)},\\ldots,\\hat{y}_d^{(l)} \\right)\\). W metodzie MTS istnieją również dwa sposoby składania kolejnych iteracji. Przebiegają one w podobny sposób jak w przypadku STS.\n\n\n\nMulti-target stacking\n\n\nIstnieje jeszcze trzecia metoda powszechnie stosowana do predykcji wielowyniowej zwana Regressor Chains lub Classifier Chains w zależności od celu zadania. Idę działania tej metody przedstawię na przykładzie modelu regresyjnego.\n\n\n2.2.1.3 Regressor Chains\nRC opierają się na idei dopasowywania modeli pojedynczego celu wzdłuż wybranej permutacji, tj. łańcucha. Najpierw losowana jest permutacja w odniesieniu do zmiennych docelowych. Proces ten można przeprowadzić w sposób losowy (Spyromitros-Xioufis i in. 2016) lub uporządkowany (Melki i in. 2017). Wybrana permutacja jest wykorzystywana do zbudowania oddzielnego modelu regresji dla zmiennych docelowych zgodnie z kolejnością permutacji. Aby wykorzystać tę strukturę do MTR, rzeczywiste wartości zmiennych docelowych są dostarczane do kolejnych modeli podczas uczenia się wzdłuż łańcucha. Na podstawie pełnego łańcucha lub wybranego zestawu \\(C = (Y_1,\\ldots,Y_d)\\), pierwszy model jest ograniczony do ustalenia predykcji dla \\(Y_1\\). Następnie, kolejno dla \\(Y_i\\) uczone są modele na podstawie zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, y_1^{(l)},\\ldots, y_{i-1}^{(l)} \\right)\\). Ten algorytm ma również dwie odmiany (niekumulatywną i kumulatywną) w zależności od kształtu kolejnych iteracji.\n\n\n\nRegressor chains\n\n\nPonieważ, jak można się spodziewać wyniki modelowania w znaczny sposób zależą od wylosowanej permutacji, to w metodzie zaproponowanej przez Melki i in. (2017) aby uniknąć tego efektu buduje się \\(k\\) modeli dla różnych permutacji i łączy się wyniki w podobny sposób jak w lasach losowych.\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nSłowo komentarza jeśli chodzi o dostępność tych metod w językach programowania. Niestety wspomniane metody w R nie są zaimplementowane w sposób, który pozwalałby na bezpieczne używanie przygotowanych rozwiązań. Istnieje kilka wzmianek (na dzień dzisiejszy, czyli początek 2024 roku) na ten temat. Twórcy dwóch głównych frameworków do uczenia maszynowego, czyli mlr3 oraz tidymodels przygotowują implementacje tych metod. Dodatkowo istnieje rozwiązanie w wersji eksperymentalnej mtr-toolkit, które pozwala na wykonanie modelowania z wieloma wyjściami, którym można się posiłkować. Na potrzeby klasyfikacji istnieje również pakiet mldr i ultim, które pozwalają na uczenie modeli klasyfikacyjnymi z wieloma wyjściami.\nNiestety w przypadku Python-a nie jest dużo lepiej. Wprawdzie w pakiecie scikit-learn istnieją implementacje pozwalające na predykcje wielowyjściowe w obu typach zadań poprzez MultiOutputRegressor i MultiOutputClassifier, ale dokonują one predykcji naiwnej poprzez złożenie w listę wyników pojedynczych modeli dla każdej zmiennej. Nieco lepiej sprawa wygląda w przypadku metod łańcuchowych, ponieważ zarówno dla klasyfikacji, jak i regresji są metody to realizujące (ClassifierChain i RegressorChain).\n\n\n\n\n\n2.2.2 Adaptacja algorytmu\nProstota podejścia transformacji problemu sprawia, że jest ono odpowiednie dla problemów, w których jego wady mają niewielki lub żaden wpływ - jednak dla złożonych problemów podejście adaptacji algorytmu może okazać się bardziej efektywne. Dodatkowo, dowody empiryczne sugerują, że uczenie się powiązanych zadań jednocześnie, a nie niezależnie, może poprawić wyniki predykcyjne (Evgeniou i Pontil 2004). Z drugiej strony, jeśli zadania są bardzo odmienne, wydajność predykcyjna może ucierpieć, gdy zadania są uczone razem, a nie niezależnie (Faddoul i in. 2010). W związku z tym możemy wyciągnąć następujące wnioski:\n\njeśli zadania, których nasz predyktor ma się nauczyć, są powiązane, powinniśmy dążyć do znalezienia odpowiedniej metody adaptacji algorytmu;\njeśli zadania, których chcemy się nauczyć, nie są powiązane, powinniśmy zamiast tego dążyć do znalezienia odpowiedniej metody transformacji problemu.\n\nWreszcie, powinniśmy wziąć pod uwagę rozmiar problemu i zdać sobie sprawę, że gdy zadania są niepowiązane, istnieje potencjalny kompromis między efektywnością czasową a wydajnością predykcyjną przy wyborze metody transformacji problemu lub metody adaptacji algorytmu. W przypadku niepowiązanych ze sobą zadań, metody transformacji problemu mogą zwiększać skuteczność predykcyjną, ale zmniejszać wydajność czasową w przypadku dużych problemów i odwrotnie.\nNiestety tej metody nie da się zastosować do każdego typu modelu. Rodzina modeli, których adaptacja jest wykonana cały czas rośnie. Adaptacja modelu polega na przekształceniu go do postaci, w której da się wykonać predykcję dla wielu wyjść. Wśród modeli, których wersje native multi-target istnieją należy wymienić:\n\nregresja wieloraka (Izenman 1975)\nkNN\ndrzewo decyzyjne (Struyf i Džeroski 2006)\nlas losowy (Kocev i in. 2013)\nbagging (Kocev i in. 2013)\ngradient boosting (Zhang i Jung, b.d.; Faddoul i in. 2012)\nSVM (Xu, Guo, i Wang 2013; Vazquez i Walter 2003)\nno i oczywiście sieci neuronowe.\n\nNie sposób przedstawić w jaki sposób wprowadzone zostały zmiany we wszystkich algorytmach. Skupię się jednak na pokazaniu adaptacji drzew decyzyjnych do predykcji wielu wyjść jednocześnie, ponieważ jest to meta-model modeli takich jak lasy losowe, bagging czy boosting.\n\n2.2.2.1 Adaptacja klasyfikacyjnego drzewa decyzyjnego\nFaddoul i in. (2012) zaproponowali zmodyfikowaną wersję algorytmu drzewa decyzyjnego C4.5 (Quinlan 1993), która bezpośrednio obsługuje problemy klasyfikacji wielowyjściowej. Zmodyfikowana wersja (nazwana MT-DT) różni się od standardowej implementacji C4.5 w dwóch krytycznych aspektach: kryteriach podziału węzłów i procesie decyzyjnym. Faddoul i in. (2012) proponują trzy różne podejścia do łączenia wielu miar przyrostu informacji w jedną miarę: wspólny przyrost informacji, suma nieważona i maksymalny przyrost informacji. Wspólny przyrost informacyjny jest definiowany przy użyciu konkatenacji wszystkich poszczególnych zadań, tj. względnej różnicy w entropii mierzonej we wszystkich zadaniach decyzyjnych. Autorzy pokazują, że nieważona suma (Równanie 2.1) indywidualnych przyrostów informacyjnych wszystkich zadań jest równoważna wspólnemu przyrostowi informacyjnemu.\n\\[\nIG_U=\\sum_YIG_Y\n\\tag{2.1}\\]\nMaksymalny przyrost informacyjny, zgodnie z propozycją autorów jest definiowany po prostu jako maksymalny przyrost informacyjny wszystkich zadań:\n\\[\nIG_M=\\max_YIG_Y\n\\tag{2.2}\\]\nBadania eksperymentalne pokazały, że maksymalny przyrost informacyjny wykorzystany do budowania reguł podziału, charakteryzuje się wyższym poziomem dopasowania modeli, niż przy zastosowaniu \\(IG_U\\) i \\(IG_J\\).\nW przypadku klasyfikacji z jedną etykietą, algorytm indukcji drzewa decyzyjnego (taki jak C4.5) rekurencyjnie dzieli węzły, dodając (zazwyczaj dwa) elementy potomne, aż możliwe jest utworzenie liścia takiego, że znaczna większość (lub nawet wszystkie) jego przykładowych instancji należy do tej samej klasy. W przypadku wielu wyjść, indukcja drzewa niekoniecznie jest tak prosta. Rozważmy problem klasyfikacji wielowyjściowej z dwoma wyjściami binarnymi \\(\\nu_1\\) i \\(\\nu_2\\); możliwe jest, że po \\(t\\) podziałach, węzeł zawiera tylko wartości pozytywne dla \\(\\nu_1\\), ale mieszankę wartości pozytywnych i negatywnych dla \\(\\nu_2\\) - stąd, podczas konstruowania drzew decyzyjnych dla wielu jednoczesnych zadań, należy pamiętać, że proces decyzyjny dla pewnego zadania może wymagać krótszej ścieżki decyzyjnej niż inne zadania w ramach tego samego problemu wielowyjściowego. MT-DT radzi sobie z tym, sprawdzając w każdym węźle, czy możliwe jest utworzenie węzła terminalnego dla któregokolwiek z zadań - w powyższym przykładzie spowodowałoby to utworzenie drzewa, w którym wewnętrzny węzeł \\(t_1\\) jest oznaczony jako węzeł zatrzymania dla \\(\\nu_1\\), oznaczony klasą pozytywną. Ponieważ celem jest prognozowanie dla obu wyjść binarnych, \\(t_1\\) nie jest węzłem liścia - zamiast tego rekurencyjne dzielenie jest kontynuowane od \\(t_1\\), aż do znalezienia węzła \\(t_2\\) takiego, że \\(t_2\\) jest wystarczająco czysty w odniesieniu do \\(\\nu_2\\), aby można było utworzyć regułę klasyfikacji dla drugiego zadania binarnego. W tym momencie węzły decyzyjne (węzły wewnętrzne lub liście) zostały znalezione dla wszystkich wyników (\\(\\nu_1\\) i \\(\\nu_2\\)), a algorytm indukcji drzewa rekurencyjnego może zostać zakończony.\nNic dziwnego, że klasyfikacja przy użyciu już zbudowanego modelu MT-DT przebiega według tej samej formuły, co jego indukcja - podczas przechodzenia przez drzewo każdy węzeł jest sprawdzany w celu ustalenia, czy można podjąć decyzję dla któregokolwiek z aktualnie nierozstrzygniętych zadań. W przykładzie \\(\\nu_1\\), \\(\\nu_2\\), klasyfikacja zostanie dokonana dla \\(\\nu_1\\) w węźle \\(t_1\\), ponieważ jest on oznaczony jako węzeł zatrzymania dla \\(\\nu_1\\); następnie przejście jest kontynuowane do momentu napotkania \\(t_2\\) i klasyfikacja może zostać dokonana dla \\(\\nu_2\\). W tym momencie wszystkie wyjścia zostały sklasyfikowane, a przechodzenie może się zakończyć, zwracając dwie wartości w \\(t_1\\) i \\(t_2\\) jako klasyfikacje odpowiednio dla \\(\\nu_1\\) i \\(\\nu_2\\).\n\n\n2.2.2.2 Adaptacja regresyjnego drzewa decyzyjnego\nSegal (1992) zaproponował rozwiązanie dla drzew regresyjnych o wielu wyjściach (MRT), które są w stanie przewidywać wyniki dla wielu powiązanych zadań regresyjnych; te wielowyjściowe drzewa regresyjne są oparte na funkcji podziału najmniejszych kwadratów zaproponowanej w ramach CART (Breiman i in. 2017). W przypadku drzewa regresyjnego o jednej odpowiedzi celem jest minimalizacja następującej funkcji celu:\n\\[\n\\phi(t) = SS(t)-SS(t_L)-SS(t_R)\n\\]\ngdzie \\(SS(t)\\) jest zdefiniowana następująco\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))^2.\n\\]\nSegal (1992) dodał ważenie macierzą kowariancji do błędu kwadratowego, co prowadzi algorytm drzewa do tworzenia węzłów potomnych, które reprezentują jednorodne klastry w odniesieniu do zestawu odpowiedzi wyjściowych:\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))'V^{-1}(t)(y_i-\\bar{y}(t)),\n\\]\ngdzie \\(V(t)\\) oznacza macierz kowariancji w węźle \\(t\\).\n\n\n2.2.2.3 Adaptacja drzew decyzyjnych do realizacji obu zadań\nJak wspomniano wcześniej, jedną z kluczowych motywacji do podejmowania prób rozwiązywania problemów rozpoznawania wzorców z wieloma wyjściami przy użyciu metod adaptacji algorytmów jest oczekiwanie, że pojedynczy model wytrenowany na zestawie powiązanych zadań wykaże poprawę wydajności predykcyjnej w porównaniu do zestawu indywidualnych modeli, z których każdy został wytrenowany na pojedynczym zadaniu. Rodzi to pytanie: co jeśli problem wielowynikowy zawiera zarówno zadania klasyfikacji, jak i regresji? Jeśli zadania są niepowiązane, rozwiązanie takiego wspólnego problemu klasyfikacyjno-regresyjnego nie musi być trudniejsze niż szkolenie zestawu klasyfikatorów i regresorów dla poszczególnych zadań; jeśli jednak zadania są powiązane, oczekujemy, że metoda adaptacji algorytmu zapewni lepsze wyniki pod względem wydajności predykcyjnej.\nGlocker i in. (2012) zaproponował algorytm indukcji drzewa, który jednocześnie rozwiązuje jedno zadanie klasyfikacji i jedno zadanie regresji. Podobnie jak MT-DT i MRT, wspólne drzewo klasyfikacyjno-regresyjne (JCRT) rozwiązuje wiele jednoczesnych zadań predykcji poprzez modyfikację funkcji podziału węzła w kroku indukcyjnym i oznaczenie węzłów końcowych odpowiednimi wartościami dla każdego zadania. Ze względu na charakter wspólnych problemów klasyfikacyjno-regresyjnych, zmodyfikowana funkcja podziału jest wymagana do jednoczesnego uwzględnienia błędu zarówno części klasyfikacyjnej, jak i regresyjnej. Funkcja podziału zaproponowana przez Glocker i in. (2012) wykorzystuje funkcję entropii składającą się z trzech części:\n\npo pierwsze, entropia Shannona jest obliczana dla części klasyfikacji;\npo drugie, ważona entropia różnicowa jest obliczana dla części regresji;\npo trzecie, ze względu na fakt, że entropia Shannona i entropia różnicowa istnieją w różnych zakresach, stosuje się krok normalizacji w celu połączenia dwóch entropii. Entropia Shannona jest obliczana tak, jak opisano wcześniej:\n\n\\[\nH_c(t) = \\sum_{c\\in C}p(c|x)\\log p(c|x).\n\\]\nMiara entropii różniczkowej stosowana przez Glocker i in. (2012) dla regresyjnej części problemu jest obliczana w podobny sposób, z dwiema kluczowymi różnicami:\n\nzamiast sumowania prawdopodobieństw wartości nominalnych, entropia jest definiowana przez różniczkę funkcji prawdopodobieństwa wyjścia o wartości rzeczywistej;\ndodatkowo funkcja prawdopodobieństwa jest ważona w klasach:\n\n\\[\nH_{r|c}(t) = \\sum_{c\\in C}p(c|x)\\int_{r\\in \\mathbb{R}^n}-p(r|c,x)\\log p(r|c,x)dr.\n\\]\nNastępnie dokonywana jest normalizacja ze względu na oba zadania, gdzie punktem odniesienie jest entropia w korzeniu:\n\\[\nH(t) = \\frac12\\left(\\frac{H_c(t)}{H_c(t_0)}+\\frac{H_{r|c}(t)}{H_{r|c}(t_0)}\\right).\n\\]\n\n\n\n\n\n\nAdnotacja\n\n\n\nJedyne implementacje, które znalazłem dla obu języków programowania (R i Python) dotyczyły lasów losowych. W R pakiet nazywa się randomForestSRC, a w Pythonie morfist. Pozwalają one zarówno na wykonywanie wielowyjściowych zadań klasyfikacyjnych i regresyjnych, jak również zadań mieszanych. Oczywiście wspomniane wyżej typy zadań można realizować przy użyciu sieci neuronowych w obu językach programowania.\n\n\n\n\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, i Pedro Larrañaga. 2015. „A Survey on Multi-Output Regression”. WIREs Data Mining and Knowledge Discovery 5 (5): 216–33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, i Charles J. Stone. 2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, i Massimiliano Pontil. 2004. „Regularized multi–task learning”. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, sierpień. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Rémi Gilleron, i Fabien Torre. 2012. „Learning multiple tasks with boosted decision trees”. W Proceedings of the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I, 681–96. ECMLPKDD’12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, i Remi Gilleron. 2010. „Boosting Multi-Task Weak Learners with Applications to Textual and Social Data”. W 2010 Ninth International Conference on Machine Learning and Applications, 367–72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nGlocker, Ben, Olivier Pauly, Ender Konukoglu, i Antonio Criminisi. 2012. „Joint Classification-Regression Forests for Spatially Structured Multi-object Segmentation”. W Computer Vision – ECCV 2012, zredagowane przez Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, i Cordelia Schmid, 7575:870–81. Springer Berlin Heidelberg. http://link.springer.com/10.1007/978-3-642-33765-9_62.\n\n\nIzenman, Alan Julian. 1975. „Reduced-rank regression for the multivariate linear model”. Journal of multivariate analysis 5 (2): 248–64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, i Sašo Džeroski. 2013. „Tree ensembles for predicting structured outputs”. Pattern Recognition 46 (3): 817–33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, i Sebastián Ventura. 2017. „Multi-Target Support Vector Regression via Correlation Regressor Chains”. Information Sciences 415–416 (listopad): 53–69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Morgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello Mastelini, Fabio Luiz Melquiades, i Sylvio Barbon Jr. 2020. „Improved Prediction of Soil Properties with Multi-Target Stacked Generalisation on EDXRF Spectra”. arXiv preprint arXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. „Tree-Structured Methods for Longitudinal Data”. Journal of the American Statistical Association 87 (418): 407–18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves, i Ioannis Vlahavas. 2016. „Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs”. Machine Learning 104 (1): 55–98. https://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, i Sašo Džeroski. 2006. „Constraint Based Induction of Multi-objective Regression Trees”. W Knowledge Discovery in Inductive Databases, zredagowane przez Francesco Bonchi i Jean-François Boulicaut, 222–33. Lecture Notes w Computer Science. Springer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, i Victor Sheng. 2013. „Empirical Comparison of Multi-Label Classification Algorithms”. W Proceedings of the AAAI Conference on Artificial Intelligence, 27:1645–46.\n\n\nTsoumakas, Grigorios, i Ioannis Katakis. 2007. „Multi-Label Classification: An Overview”. International Journal of Data Warehousing and Mining (IJDWM) 3 (3): 1–13.\n\n\nVazquez, Emmanuel, i Eric Walter. 2003. „Multi-Output Suppport Vector Regression”. IFAC Proceedings Volumes, 13th IFAC Symposium on System Identification (SYSID 2003), Rotterdam, The Netherlands, 27-29 August, 2003, 36 (16): 1783–88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, i Laisheng Wang. 2013. „A Twin Multi-Class Classification Support Vector Machine”. Cognitive Computation 5 (4): 580–88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, i Cheolkon Jung. b.d. „GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputs”. https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Modele z wieloma wyjściami</span>"
    ]
  },
  {
    "objectID": "examples.html",
    "href": "examples.html",
    "title": "3  Przykłady - metody klasyczne",
    "section": "",
    "text": "3.1 Przykład 1\nNajpierw sformułujemy problem badawczy wymagający zastosowania modeli z wieloma wyjściami.\nKod\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.datasets import make_regression\nKod\n# generowanie danych do zadania\nX, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)\nKod\n# łączenie ich w ramki danych\nX_df = pd.DataFrame(i for i in X)\nX_df.columns = [\"X\"+ str(i) for i in range(1,11)]\ny_df = pd.DataFrame(i for i in y)\ny_df.columns = [\"y\"+str(i) for i in range(1,4)]\n\ndf = pd.concat([X_df,y_df], axis=1)\ndf.head()\n\n\n\n\n\n\n\n\n\nX1\nX2\nX3\nX4\nX5\nX6\nX7\nX8\nX9\nX10\ny1\ny2\ny3\n\n\n\n\n0\n-0.547331\n0.426522\n-1.693585\n-0.740282\n0.277445\n-1.910679\n-0.320635\n1.449172\n-0.469619\n0.371273\n-92.924393\n-116.819352\n-32.117311\n\n\n1\n2.314630\n-0.936016\n-1.833034\n-0.394507\n-0.902981\n-1.089381\n1.005442\n-0.351289\n1.218223\n0.350258\n88.577211\n182.003040\n67.579488\n\n\n2\n-0.025423\n0.684189\n1.208964\n1.325672\n0.328946\n-0.354083\n-0.566556\n0.671359\n-0.560768\n0.327379\n115.652472\n63.462207\n43.519697\n\n\n3\n-0.044533\n0.603030\n-1.495716\n-0.507870\n-0.268485\n-0.140194\n-0.246658\n-0.758946\n2.567979\n1.808345\n-12.092227\n65.181041\n96.138770\n\n\n4\n2.083679\n0.318852\n-0.080982\n-1.284608\n0.281687\n0.792470\n-0.560598\n-1.368963\n0.718059\n-1.741815\n-18.573726\n-103.219393\n11.559200\nKod\ndf_X_boxplot = pd.melt(X_df)\nsns.boxplot(data = df_X_boxplot, x = \"variable\", y = \"value\")\nplt.show()\nKod\ndf_y_boxplot = pd.melt(y_df)\nsns.boxplot(data = df_y_boxplot, x = \"variable\", y = \"value\")\nplt.show()\nZarówno zmienne X, jak i y mają zbliżone rozkłady.\nKod\nsns.set_theme(style=\"white\")\ng = sns.PairGrid(y_df, diag_sharey=False)\ng.map_upper(sns.scatterplot, s=7)\ng.map_lower(sns.kdeplot)\ng.map_diag(sns.histplot)\nplt.show()\nJak widać z powyższego wykresu zmienne y1,y2,y3 wykazują pewne wzajemne zależności, dlatego budowa oddzielnych modeli dla każdej ze zmiennych powinna dawać gorsze predykcje niż modele wiążące wszystkie zmiennej w jednym modelu. Można też zauważyć, że rozkłady są zbliżone do normalnego.\nKod\ncor = df.corr()\nplt.figure(figsize = (12,10))\nsns.heatmap(cor, \n            xticklabels=cor.columns.values,\n            yticklabels=cor.columns.values,\n            annot = True, fmt = '.2f')\nplt.show()\nJak widać z powyższej macierzy korelacji, przynajmniej 8 spośród 10 zmiennych X koreluje istotnie ze zmiennymi y.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przykłady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#przykład-1",
    "href": "examples.html#przykład-1",
    "title": "3  Przykłady - metody klasyczne",
    "section": "",
    "text": "Przykład 3.1 Dane do zadania wygenerujemy za pomocą funkcji make_regression() z pakietu sklern.dataset1. Wygenerujemy 700 obserwacji z 10 predyktorami i 3 zmiennymi zależnymi.\n1 Nie znam R-owego odpowiednika",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przykłady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#rozwiązanie",
    "href": "examples.html#rozwiązanie",
    "title": "3  Przykłady - metody klasyczne",
    "section": "3.2 Rozwiązanie",
    "text": "3.2 Rozwiązanie\nNa potrzeby porównania różnych rozwiązań zbudujemy następujące konfiguracje modeli:\n\ntrzy lasy losowe dla każdej zmiennej y niezależnie;\nmodel lasu losowego z wykorzystaniem reguły Regression Chains jako transformacji modelu;\nlas losowy dla trzech zmiennych wynikowych jednocześnie (adaptacja modelu)\n\nRozwiązania te porównamy pod względem dopasowania.\n\n3.2.1 Niezależne lasy losowe\nFunkcja MultiOutputRegressor nałożona na model lasu losowego takie rozwiązanie tworzy.\n\n\nKod\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.multioutput import RegressorChain\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)\n\n\n\n\nKod\nrf_meta = RandomForestRegressor(random_state=4)\nrf_indep = MultiOutputRegressor(rf_meta)\nprint(rf_indep)\n\n\nMultiOutputRegressor(estimator=RandomForestRegressor(random_state=4))\n\n\n\n\nKod\nrf_indep.fit(X_train, y_train)\nr2_indep= rf_indep.score(X_test, y_test)\npred_indep = rf_indep.predict(X_test)\nrmse_indep = mean_squared_error(y_test, pred_indep, squared = False)\n\nprint(f'R2 on test samples: {r2_indep:.2f}')\nprint(f'RMSE on test samples: {rmse_indep:.1f}')\n\n\nR2 on test samples: 0.82\nRMSE on test samples: 71.1\n\n\n\n\n3.2.2 Regressor Chains RF\n\n\nKod\nrf_chains = RegressorChain(rf_meta)\nprint(rf_chains)\n\n\nRegressorChain(base_estimator=RandomForestRegressor(random_state=4))\n\n\n\n\nKod\nrf_chains.fit(X_train, y_train)\nr2_chains= rf_chains.score(X_test, y_test)\npred_chains = rf_chains.predict(X_test)\nrmse_chains = mean_squared_error(y_test, pred_chains, squared = False)\n\nprint(f'R2 on test samples: {r2_chains:.2f}')\nprint(f'RMSE on test samples: {rmse_chains:.1f}')\n\n\nR2 on test samples: 0.80\nRMSE on test samples: 72.5\n\n\n\n\n3.2.3 Adaptacja modelu RF\n\n\nKod\nprint(rf_meta)\n\n\nRandomForestRegressor(random_state=4)\n\n\n\n\nKod\nrf_meta.fit(X_train, y_train)\nr2_meta= rf_meta.score(X_test, y_test)\npred_meta = rf_meta.predict(X_test)\nrmse_meta = mean_squared_error(y_test, pred_meta, squared = False)\n\nprint(f'R2 on test samples: {r2_meta:.2f}')\nprint(f'RMSE on test samples: {rmse_meta:.1f}')\n\n\nR2 on test samples: 0.77\nRMSE on test samples: 78.0\n\n\n\n\n3.2.4 Podsumowanie\nBiorąc pod uwagę miary dopasowania najlepiej poradził sobie z tym zadaniem model składający się z trzech niezależnych modeli RF, potem model RF w wersji Regressor Chains, a najgorzej (o dziwo) radzi sobie z predykcją model korzystający adaptacji drzew decyzyjnych do wersji wielowyjściowej.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przykłady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples.html#przykład-2",
    "href": "examples.html#przykład-2",
    "title": "3  Przykłady - metody klasyczne",
    "section": "3.3 Przykład 2",
    "text": "3.3 Przykład 2\nTym razem sformułujemy problem z wieloma wyjściami ale klasyfikacyjny i również dopasujemy trzy wersje modelu lasu losowego:\n\nlasy losowe dla każdej zmiennej y niezależnie;\nmodel lasu losowego z wykorzystaniem reguły Classifier Chains jako transformacji modelu\nlas losowy dla wszystkich zmiennych wynikowych jednocześnie (adaptacja modelu).\n\nAnalizowany problem będzie prawdziwy i będzie dotyczył klasyfikacji enzymów na podstawie cech charakterystycznych substratów. Pierwszych 31 zmiennych stanowi zmienne objaśniające, a 6 pozostałych zmienne kodujące przynależność do danej grupy enzymów.\n\n\nKod\ndt = pd.read_csv(\"data/original.csv\", index_col = \"id\")\ndt.head()\n\n\n\n\n\n\n\n\n\nBertzCT\nChi1\nChi1n\nChi1v\nChi2n\nChi2v\nChi3v\nChi4n\nEState_VSA1\nEState_VSA2\n...\nSlogP_VSA3\nVSA_EState9\nfr_COO\nfr_COO2\nEC1\nEC2\nEC3\nEC4\nEC5\nEC6\n\n\nid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nC00009\n49.783540\n2.000000\n0.782574\n2.347723\n0.513277\n1.539831\n0.000000\n0.000000\n7.822697\n0.000000\n...\n4.565048\n16.923611\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00013\n147.355172\n3.707107\n1.530297\n4.590890\n1.062804\n3.678309\n1.914534\n0.138556\n15.645394\n0.000000\n...\n13.440728\n20.899028\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00014\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n6.150546\n...\n0.000000\n0.000000\n0\n0\n1\n1\n1\n1\n0\n1\n\n\nC00017\n172.720106\n4.947265\n2.081214\n2.081214\n1.157830\n1.157830\n0.489278\n0.180980\n12.514062\n12.451936\n...\n9.589074\n35.105740\n1\n1\n0\n1\n1\n0\n0\n0\n\n\nC00022\n72.039100\n2.642734\n1.381855\n1.381855\n0.861339\n0.861339\n0.301176\n0.000000\n11.752550\n0.000000\n...\n9.589074\n25.333333\n1\n1\n1\n1\n1\n1\n0\n1\n\n\n\n\n5 rows × 37 columns\n\n\n\n\n\nKod\ndt.info()\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 1039 entries, C00009 to C22220\nData columns (total 37 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   BertzCT            1039 non-null   float64\n 1   Chi1               1039 non-null   float64\n 2   Chi1n              1039 non-null   float64\n 3   Chi1v              1039 non-null   float64\n 4   Chi2n              1039 non-null   float64\n 5   Chi2v              1039 non-null   float64\n 6   Chi3v              1039 non-null   float64\n 7   Chi4n              1039 non-null   float64\n 8   EState_VSA1        1039 non-null   float64\n 9   EState_VSA2        1039 non-null   float64\n 10  ExactMolWt         1039 non-null   float64\n 11  FpDensityMorgan1   1039 non-null   float64\n 12  FpDensityMorgan2   1039 non-null   float64\n 13  FpDensityMorgan3   1039 non-null   float64\n 14  HallKierAlpha      1039 non-null   float64\n 15  HeavyAtomMolWt     1039 non-null   float64\n 16  Kappa3             1039 non-null   float64\n 17  MaxAbsEStateIndex  1039 non-null   float64\n 18  MinEStateIndex     1039 non-null   float64\n 19  NumHeteroatoms     1039 non-null   int64  \n 20  PEOE_VSA10         1039 non-null   float64\n 21  PEOE_VSA14         1039 non-null   float64\n 22  PEOE_VSA6          1039 non-null   float64\n 23  PEOE_VSA7          1039 non-null   float64\n 24  PEOE_VSA8          1039 non-null   float64\n 25  SMR_VSA10          1039 non-null   float64\n 26  SMR_VSA5           1039 non-null   float64\n 27  SlogP_VSA3         1039 non-null   float64\n 28  VSA_EState9        1039 non-null   float64\n 29  fr_COO             1039 non-null   int64  \n 30  fr_COO2            1039 non-null   int64  \n 31  EC1                1039 non-null   int64  \n 32  EC2                1039 non-null   int64  \n 33  EC3                1039 non-null   int64  \n 34  EC4                1039 non-null   int64  \n 35  EC5                1039 non-null   int64  \n 36  EC6                1039 non-null   int64  \ndtypes: float64(28), int64(9)\nmemory usage: 308.5+ KB\n\n\nSprawdzimy na ile niezbalansowane są poszczególne klasy wynikowe.\n\n\nKod\ndt_deps = dt.loc[:,'EC1':'EC6']\ndt_deps = dt_deps.melt()\ndt_deps = dt_deps[dt_deps.value == 1]\nsns.countplot(data = dt_deps, x = 'variable')\n\n\n\n\n\n\n\n\n\nNiestety nie wszystkie klasy występują jednakowo często i może pojawić się zjawisko, że kombinacja enzymów będzie występowała bardzo rzadko (np. raz). Jak widać z poniższej tabeli faktycznie tak się dzieje. To nie pozwala przeprowadzić uczenia. Dlatego musimy połączyć pewne klasy enzymów aby uniemożliwić taką sytuację.\n\n\nKod\ndt.loc[:,'EC1':'EC6'].value_counts()\n\n\nEC1  EC2  EC3  EC4  EC5  EC6\n1    0    0    0    0    0      178\n0    1    0    0    0    0      136\n1    1    0    0    0    0      112\n0    1    1    0    0    0       90\n     0    1    0    0    0       69\n1    1    0    1    0    0       38\n          1    0    0    0       37\n     0    0    1    0    0       33\n0    0    0    1    0    0       30\n     1    0    1    0    0       27\n               0    1    0       21\n1    0    1    0    0    0       20\n     1    0    0    0    1       19\n               1    0    1       15\n0    0    0    0    1    0       15\n1    1    0    0    1    0       14\n     0    0    0    1    0       12\n0    0    1    1    0    0       12\n          0    0    0    1       11\n1    1    0    1    1    1       11\n0    1    0    0    0    1       10\n          1    1    0    0        9\n1    0    0    1    1    0        8\n     1    1    1    0    1        8\n     0    0    1    0    1        7\n     1    1    0    0    1        7\n     0    0    0    0    1        7\n0    1    1    0    1    0        7\n                    0    1        7\n1    1    1    1    1    0        6\n          0    1    1    0        6\n0    1    0    1    0    1        6\n1    1    1    0    1    0        5\n0    0    0    1    1    0        5\n     1    1    1    1    0        4\n1    1    0    0    1    1        4\n          1    1    0    0        4\n0    0    1    0    0    1        4\n                    1    0        3\n1    1    1    1    1    1        3\n     0    1    1    0    0        3\n               0    0    1        3\n0    1    0    1    1    0        2\n                         1        2\n     0    0    1    0    1        2\n1    0    0    0    1    1        1\n0    0    1    1    0    1        1\n               0    1    1        1\n1    0    1    1    0    1        1\n               0    1    0        1\n0    1    1    1    0    1        1\n     0    1    1    1    0        1\nName: count, dtype: int64\n\n\nUsuniemy zatem takie kombinacje, które występują bardzo rzadko w danych.\n\n\nKod\ncombinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()\nidx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()\nbad_combinations = combinations[idx&lt;10]\nidx = []\nfor i in range(len(dt)):\n  idx.append(True)\n  for j in range(len(bad_combinations)):\n    if all(dt.iloc[i, 31:] == bad_combinations[j]):\n      idx[i]=False\n\ndt = dt.iloc[idx,:]\n\n\n\n\nKod\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier, ClassifierChain\ny = dt.iloc[:,31:].to_numpy()\nX = dt.iloc[:,:31].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n\n\n\n3.3.1 Niezależne lasy losowe\n\n\nKod\nrf_meta = RandomForestClassifier(random_state=4)\nrf_indep = MultiOutputClassifier(rf_meta)\nprint(rf_indep)\n\nrf_indep.fit(X_train, y_train)\nacc_indep= rf_indep.score(X_test, y_test)\npred_indep = rf_indep.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_indep:.3f}')\n\n\nMultiOutputClassifier(estimator=RandomForestClassifier(random_state=4))\nAccuracy on test samples: 0.179\n\n\n\n\n3.3.2 Classifier Chains\n\n\nKod\nrf_chains = ClassifierChain(rf_meta)\nprint(rf_chains)\n\nrf_chains.fit(X_train, y_train)\nacc_chains= rf_chains.score(X_test, y_test)\npred_chains = rf_chains.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_chains:.3f}')\n\n\nClassifierChain(base_estimator=RandomForestClassifier(random_state=4))\nAccuracy on test samples: 0.190\n\n\n\n\n3.3.3 Adaptacja modelu RF\n\n\nKod\nrf_meta.fit(X_train, y_train)\nacc_meta= rf_meta.score(X_test, y_test)\npred_meta = rf_meta.predict(X_test)\n\nprint(f'Accuracy on test samples: {acc_meta:.3f}')\n\n\nAccuracy on test samples: 0.201\n\n\n\n\n3.3.4 Podsumowanie\nRównież i tym razem nie widać wyraźnych różnic pomiędzy jakością dopasowania modeli. Model adaptowanego lasu losowego poradził sobie z zadanie najlepiej (19% poprawności trafień), Classifier Chains dał 18,7%, a niezależne lasy losowe 17,2%.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Przykłady - metody klasyczne</span>"
    ]
  },
  {
    "objectID": "examples2.html",
    "href": "examples2.html",
    "title": "4  Przykłady NN",
    "section": "",
    "text": "4.1 Przykład 1\nRozwiązanie przykładu z poprzedniego rozdziału można dokonać z dużo lepszą precyzją wykorzystując sieci neuronowe. W tym przykładzie po raz kolejny wygenerujemy dane do uczenia1, a następnie wytrenujemy sieć (dosyć płytką) MLP.\nKod\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization, ReLU, LeakyReLU, ELU\nfrom keras.callbacks import EarlyStopping\nimport keras\nKod\n# generowanie danych do zadania\nX, y = make_regression(n_samples=700, n_features=10, n_informative = 8, n_targets=3, random_state=4)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 4)\n\nn_neurons = [10,20,50]\nModel będzie bardzo prosty, składający się z tylko jednej warstwy ukrytej.\nKod\ndef get_model(n_inputs, n_outputs, n_neurons):\n  model = Sequential()\n  model.add(Dense(int(n_neurons), input_dim=n_inputs, activation='relu'))\n  model.add(Dense(n_outputs, activation='linear'))\n  model.compile(loss='mse', optimizer='adam')\n  return model\nW tym przykładzie chciałem również pokazać jak wykonywać trenowanie sieci z użyciem sprawdzianu krzyżowego, który pomoże nam ustalić optymalną liczę neuronów w warstwie ukrytej.\nKod\n# ocena dopasowania modelu z wykorzystaniem CV\ndef evaluate_model(X, y, n_neurons):\n    results = list()\n    n_inputs, n_outputs = X.shape[1], y.shape[1]\n    # definicja CV\n    cv = RepeatedKFold(n_splits=5, random_state=1)\n    # pętla po foldach\n    for train_ix, test_ix in cv.split(X):\n        # przygotowanie danych\n        X_tr, X_te = X[train_ix], X[test_ix]\n        y_tr, y_te = y[train_ix], y[test_ix]\n        # określenie modelu\n        model = get_model(n_inputs, n_outputs, n_neurons)\n        # dopasowanie modelu\n        model.fit(X_tr, y_tr, verbose=0, epochs=100)\n        # ocena dopasowania na foldzie testowym\n        mae = model.evaluate(X_te, y_te, verbose=0)\n        results.append(mae)\n    return results\nKod\nresults = []\nfor i in n_neurons:\n  # dopasuj i oceń model na zbiorze uczącym\n  results.append(np.mean(evaluate_model(X_train, y_train, i)))\nKod\nresults = np.load(\"./data/mlp_eval.npz\")\nresults = results['arr_0'].tolist()\nfor i in range(len(n_neurons)):\n  print(f\"Dla {n_neurons[i]} neuronów MAE: {results[i]:.0f}\")\n\n\nDla 10 neuronów MAE: 16409\nDla 20 neuronów MAE: 9806\nDla 50 neuronów MAE: 3779\nNajlepszy rezultat osiągamy dla 50 neuronów i taki parametr dobierzemy w ostatecznym modelu.\nKod\nkeras.utils.set_random_seed(44)\nmy_callbacks = [\n    EarlyStopping(patience=2)\n]\n\nmodel = get_model(X_train.shape[1], y_train.shape[1], 50)\nhistory = model.fit(X_train, y_train, \n                    verbose=0, epochs=1000, \n                    validation_split=0.2, callbacks=my_callbacks)\nProces uczenia przebiegał prawidłowo i osiągnięto niski poziom funkcji straty.\nKod\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()\nWyniki dopasowania znacznie przekraczają wyniki uzyskane metodami z poprzedniego rozdziału.\nKod\ny_pred = model.predict(X_test, verbose=0)\nrmse_mlp = mean_squared_error(y_test, y_pred, squared=False)\nr2_mlp = r2_score(y_test, y_pred)\nprint(f\"R2 on test sample: {r2_mlp:.2f}\")\nprint(f\"RMSE on test sample: {rmse_mlp:.1f}\")\n\n\nR2 on test sample: 1.00\nRMSE on test sample: 0.0",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Przykłady NN</span>"
    ]
  },
  {
    "objectID": "examples2.html#przykład-1",
    "href": "examples2.html#przykład-1",
    "title": "4  Przykłady NN",
    "section": "",
    "text": "1 te same co z przykładu regresyjnego z poprzedniego wykładu",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Przykłady NN</span>"
    ]
  },
  {
    "objectID": "examples2.html#przykład-2",
    "href": "examples2.html#przykład-2",
    "title": "4  Przykłady NN",
    "section": "4.2 Przykład 2",
    "text": "4.2 Przykład 2\nW tym przykładzie jeszcze raz rozpatrzymy zadanie klasyfikacyjne z wieloma wyjściami z poprzedniego rozdziału. Przeprowadzimy czynności preprocessingu podobne jak w poprzednim rozdziale, dodając jeszcze standaryzację, która dla sieci neuronowych jest bardzo ważna.\n\n\nKod\nfrom sklearn.preprocessing import StandardScaler\ndt = pd.read_csv(\"./data/original.csv\", index_col = \"id\")\ncombinations = dt.loc[:,'EC1':'EC6'].value_counts().index.to_numpy()\nidx = dt.loc[:,'EC1':'EC6'].value_counts().to_numpy()\nbad_combinations = combinations[idx&lt;10]\nidx = []\nfor i in range(len(dt)):\n  idx.append(True)\n  for j in range(len(bad_combinations)):\n    if all(dt.iloc[i, 31:] == bad_combinations[j]):\n      idx[i]=False\n\ndt = dt.iloc[idx,:]\n\ny = dt.iloc[:,31:].to_numpy()\nX = dt.iloc[:,:31].to_numpy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 44)\n\nscaler = StandardScaler().fit(X_train)\nX_test = scaler.transform(X_test)\nX_train = scaler.transform(X_train)\n\n\nNastępnie przygotujemy model sieci neuronowej, która pozwoli na właściwe klasyfikacje obiektów.\n\n\nKod\nmodel = Sequential()\nmodel.add(Dense(15, input_dim=X_train.shape[1]))\nmodel.add(ReLU())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(y_train.shape[1], activation='sigmoid'))\n\nopt = keras.optimizers.Nadam(0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=opt)\nmodel.summary()\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n dense_2 (Dense)             (None, 15)                480       \n                                                                 \n re_lu (ReLU)                (None, 15)                0         \n                                                                 \n dropout (Dropout)           (None, 15)                0         \n                                                                 \n dense_3 (Dense)             (None, 6)                 96        \n                                                                 \n=================================================================\nTotal params: 576 (2.25 KB)\nTrainable params: 576 (2.25 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\n\n\n\n\nKod\nkeras.utils.set_random_seed(44)\nmy_callbacks = [\n    EarlyStopping(patience=10)\n]\nhistory = model.fit(X_train, y_train, epochs=1000,  validation_split=0.4, verbose=0,\ncallbacks=my_callbacks)\n\n\n\n\nKod\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nKod\nfrom sklearn.metrics import accuracy_score\ny_pred = model.predict(X_test, verbose=0)\ny_class = y_pred.round()\nacc = accuracy_score(y_test, y_class)\nprint(f\"ACC on test sample: {acc:.3f}\")\n\n\nACC on test sample: 0.198\n\n\n\n4.2.1 Podsumowanie\nModel prostej sieci neuronowej nie poprawił znacząco jakości dopasowania w stosunku do modelu lasu losowego adaptowanego do zadania z wieloma wyjściami.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Przykłady NN</span>"
    ]
  },
  {
    "objectID": "rnn.html",
    "href": "rnn.html",
    "title": "\n5  DNN dla danych sekwencyjnych\n",
    "section": "",
    "text": "5.1 Rodzaje szeregów czasowych\nSzereg czasowy może być dowolnym zestawem danych uzyskanym poprzez pomiary w regularnych odstępach czasu, jak np. dzienna cena akcji, godzinowe zużycie energii elektrycznej w mieście lub tygodniowa sprzedaż w sklepie. Szeregi czasowe są obecne wszędzie, niezależnie od tego, czy patrzymy na zjawiska naturalne (takie jak aktywność sejsmiczna, ewolucja populacji ryb w rzece czy pogoda w danym miejscu), czy na wzorce aktywności ludzkiej (takie jak odwiedzający stronę internetową, PKB kraju czy transakcje kartami kredytowymi). W przeciwieństwie do typów danych, z którymi mieliśmy do tej pory do czynienia, praca z szeregami czasowymi wymaga zrozumienia dynamiki systemu - jego cykli okresowych, trendów w czasie, regularnego reżimu i nagłych skoków.\nZdecydowanie najbardziej powszechnym zadaniem związanym z szeregami czasowymi jest prognozowanie: przewidywanie, co stanie się w następnej serii; prognozowanie zużycia energii elektrycznej z kilkugodzinnym wyprzedzeniem, aby można było przewidzieć popyt; prognozowanie przychodów z kilkumiesięcznym wyprzedzeniem, aby można było zaplanować budżet; prognozowanie pogody z kilkudniowym wyprzedzeniem, aby można było zaplanować wyjazd. Prognozowanie jest tym, na czym skupia się ten rozdział. Ale w rzeczywistości istnieje wiele innych rzeczy, które można zrobić z szeregami czasowymi:\nPodczas pracy z szeregami czasowymi, spotkamy się z szeroką gamą technik reprezentacji danych specyficznych dla danej dziedziny. Na przykład, transformata Fouriera, która polega na wyrażeniu serii wartości w kategoriach superpozycji fal o różnych częstotliwościach. Transformata Fouriera może być bardzo cenna podczas wstępnego przetwarzania wszelkich danych, które charakteryzują się przede wszystkim cyklami i oscylacjami (jak dźwięk, drgania ramy wieżowca czy fale mózgowe). W kontekście głębokiego uczenia, analiza Fouriera (lub powiązana analiza częstotliwości Mel) i inne reprezentacje specyficzne dla danej domeny mogą być przydatne jako forma inżynierii cech, sposób na przygotowanie danych przed trenowaniem modelu na nich, aby ułatwić pracę modelu.\nMożemy wyróżnić trzy rodzaje zadań z wykorzystaniem szeregów czasowych:\nOprócz wspomnianego podziału rodzajów zadań z wykorzystaniem szeregów czasowych, możemy je również podzielić na typy zadań ze względu na postać wejść i wyjść z modelu sieci neuronowej:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#rodzaje-szeregów-czasowych",
    "href": "rnn.html#rodzaje-szeregów-czasowych",
    "title": "\n5  DNN dla danych sekwencyjnych\n",
    "section": "",
    "text": "klasyfikacja - przypisanie jednej lub więcej etykiet kategorycznych do szeregu czasowego. Na przykład, biorąc pod uwagę szereg czasowy aktywności osoby odwiedzającej stronę internetową, należy sklasyfikować, czy osoba ta jest botem czy człowiekiem.\nwykrywanie zdarzeń - identyfikacja wystąpienia określonego, oczekiwanego zdarzenia w ciągłym strumieniu danych. Szczególnie użytecznym zastosowaniem jest “wykrywanie słów kluczy”, gdzie model monitoruje strumień audio i wykrywa wypowiedzi takie jak “OK, Google” lub “Hej, Alexa”.\nwykrywanie anomalii - wykrywanie wszelkich nietypowych zdarzeń w ciągłym strumieniu danych. Nietypowa aktywność w sieci firmowej, które może być rozpoznane jako atak. Nietypowe odczyty na linii produkcyjnej, po to, aby człowiek się temu przyjrzał. Wykrywanie anomalii odbywa się zazwyczaj poprzez uczenie bez nadzoru, ponieważ często nie wiadomo, jakiego rodzaju anomalii się szuka, więc nie można trenować na konkretnych przykładach anomalii.\n\n\n\n\njednowymiarowe szeregi czasowe (ang. univariate time series) - problem sekwencyjny, w którym na podstawie historycznych wartości szeregu czasowego przewidujemy przyszłe wartości. Przykładowo na podstawie temperatur z kilku poprzednich dni, chcemy przewidywać temperaturę jutrzejszą.\nwielowymiarowe szeregi czasowe (ang. multivariate time series) - problem sekwencyjny, w którym na podstawie historycznych wartości kilku zmiennych (w tym historycznych danych na temat zmiennej wyjściowej) przewidujemy przyszłe wartości pewnej zmiennej wyjściowej. Przykładowo jeśli chcemy przewidywać zanieczyszczenie powietrza1 na podstawie wcześniejszych odczytów zanieczyszczenia oraz wcześniejszych informacji o sile i kierunku wiatru, temperaturze, punkcie rosy itp.\nwieloetapowe szeregi czasowe (ang. multistage time series) - problem sekwencyjny, w którym na postawie historycznych wartości szeregu czasowego przewidujemy kilka (więcej niż jedną) wartość przyszłą tego szeregu czasowego. Przykładowo jeśli na podstawie historycznych wielkości opadów chcemy przewidzieć wielkość opadów w najbliższych trzech dniach.\n\n1 wyrażone w jednostkach pm2.5\n\njeden do jednego - gdy istnieje jedno wejście i jedno wyjście. Typowym przykładem problemu sekwencji jeden do jednego jest przypadek, gdy mamy obraz i chcemy przewidzieć jedną etykietę dla tego obrazu.\nwiele do jednego - problemach sekwencyjnych wiele do jednego, mamy sekwencję danych jako wejście i musimy przewidzieć pojedyncze wyjście. Klasyfikacja tekstu jest doskonałym przykładem problemów sekwencji many-to-one, gdzie mamy sekwencję wejściową słów i chcemy przewidzieć pojedynczy tag wyjściowy.\njeden do wielu - w problemach sekwencji jeden do wielu, mamy pojedyncze wejście i sekwencję wyjść. Typowym przykładem jest obraz i odpowiadający mu opis sceny.\nwiele do wielu - problemy sekwencji wiele do wielu obejmują sekwencję wejść i sekwencję wyjść. Na przykład, ceny akcji z 7 dni jako dane wejściowe i ceny akcji z kolejnych 7 dni jako dane wyjściowe. Chatboty są również przykładem problemów sekwencji wiele do wielu, gdzie sekwencja tekstowa jest wejściem, a inna sekwencja tekstowa jest wyjściem.\n\n\n\nRóżne typy zadań sekwencyjnych realizowanych przez sieci neuronowe",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#zastosowanie-sieci-rekurencyjnych-w-szeregach-czasowych",
    "href": "rnn.html#zastosowanie-sieci-rekurencyjnych-w-szeregach-czasowych",
    "title": "\n5  DNN dla danych sekwencyjnych\n",
    "section": "\n5.2 Zastosowanie sieci rekurencyjnych w szeregach czasowych",
    "text": "5.2 Zastosowanie sieci rekurencyjnych w szeregach czasowych\nW tym podrozdziale omówimy trzy zaawansowane techniki poprawy wydajności i siły generalizacji rekurencyjnych sieci neuronowych. Zademonstrujemy wszystkie trzy koncepcje na problemie prognozowania pogody, gdzie mamy dostęp do szeregu obserwacji pochodzących z czujników zainstalowanych na dachu budynku, takich jak temperatura, ciśnienie powietrza i wilgotność, które użyjemy do przewidywania, jaka będzie temperatura 24 godziny po ostatniej obserwacji w bazie danych. Jest to dość trudny problem, który ilustruje wiele typowych trudności napotykanych podczas pracy z szeregami czasowymi.\nOmówimy następujące techniki:\n\n\nRecurrent dropout - to specyficzny, wbudowany sposób użycia dropoutu do walki z nadmiernym dopasowaniem w warstwach rekurencyjnych.\nSkładanie warstw rekurencyjnych - zwiększa to moc reprezentacyjną sieci (kosztem większego obciążenia obliczeniowego).\nDwukierunkowe warstwy rekurencyjne - prezentują one tę samą informację sieci rekurencyjnej na różne sposoby, zwiększając dokładność i łagodząc problemy związane z zapominaniem.\n\n\nPrzykład 5.1 W analizowanym zestawie danych 14 różnych wielkości (takich jak temperatura powietrza, ciśnienie atmosferyczne, wilgotność, kierunek wiatru i tak dalej) było rejestrowanych co 10 minut, przez kilka lat. Oryginalne dane sięgają 2003 roku, ale ten przykład jest ograniczony do danych z lat 2009-2016.\nNa początek pobierzemy dane z serwera https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip i rozpakujemy.\n\nKodurl &lt;-\n \"https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\"\ndownload.file(url, destfile = basename(url))\nzip::unzip(zipfile = \"jena_climate_2009_2016.csv.zip\",\n          files = \"jena_climate_2009_2016.csv\")\n\n\n\nKodfull_df &lt;- readr::read_csv(\"jena_climate_2009_2016.csv\")\nfull_df\n\n# A tibble: 420,451 × 15\n   `Date Time`         `p (mbar)` `T (degC)` `Tpot (K)` `Tdew (degC)` `rh (%)`\n   &lt;chr&gt;                    &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;\n 1 01.01.2009 00:10:00       997.      -8.02       265.         -8.9      93.3\n 2 01.01.2009 00:20:00       997.      -8.41       265.         -9.28     93.4\n 3 01.01.2009 00:30:00       997.      -8.51       265.         -9.31     93.9\n 4 01.01.2009 00:40:00       997.      -8.31       265.         -9.07     94.2\n 5 01.01.2009 00:50:00       997.      -8.27       265.         -9.04     94.1\n 6 01.01.2009 01:00:00       996.      -8.05       265.         -8.78     94.4\n 7 01.01.2009 01:10:00       996.      -7.62       266.         -8.3      94.8\n 8 01.01.2009 01:20:00       996.      -7.62       266.         -8.36     94.4\n 9 01.01.2009 01:30:00       996.      -7.91       266.         -8.73     93.8\n10 01.01.2009 01:40:00       997.      -8.43       265.         -9.34     93.1\n# ℹ 420,441 more rows\n# ℹ 9 more variables: `VPmax (mbar)` &lt;dbl&gt;, `VPact (mbar)` &lt;dbl&gt;,\n#   `VPdef (mbar)` &lt;dbl&gt;, `sh (g/kg)` &lt;dbl&gt;, `H2OC (mmol/mol)` &lt;dbl&gt;,\n#   `rho (g/m**3)` &lt;dbl&gt;, `wv (m/s)` &lt;dbl&gt;, `max. wv (m/s)` &lt;dbl&gt;,\n#   `wd (deg)` &lt;dbl&gt;\n\n\nChoć nie jest to konieczne w tym zadaniu, to przekształcimy datę (będącą pierwsza kolumną w zestawie danych) z typu character do typu DateTime.\n\nKodfull_df$`Date Time` %&lt;&gt;%\n as.POSIXct(tz = \"Etc/GMT+1\", format = \"%d.%m.%Y %H:%M:%S\")\n\n\nZwróć uwagę, że przekazujemy tz = \"Etc/GMT+1\" zamiast tz = \"Europe/Berlin\", ponieważ znaczniki czasu w zbiorze danych nie dostosowują się do czasu letniego środkowoeuropejskiego (znanego również jako czas letni), ale zawsze są według czasu środkowoeuropejskiego.\n\n\n\n\n\n\nWskazówka\n\n\n\nPowyżej po raz pierwszy użyliśmy potoku przypisania x %&lt;&gt;% fn(), który jest skrótem od x &lt;- x %&gt;% fn(). Jest to przydatne, ponieważ pozwala pisać bardziej czytelny kod i uniknąć wielokrotnego powtarzania tej samej nazwy zmiennej. Przypisanie jest udostępniane przez wywołanie library(keras).\n\n\n\nKodplot(`T (degC)` ~ `Date Time`, data = full_df, pch = 20, cex = .3)\n\n\n\n\n\n\nRys. 5.1: Przebieg szeregu czasowego temperatury\n\n\n\n\nRys. 5.1 przedstawia wykres temperatury (w stopniach Celsjusza) w czasie. Na tym wykresie wyraźnie widać roczną okresowość temperatury - dane obejmują 8 lat.\n\nKodplot(`T (degC)` ~ `Date Time`, data = full_df[1:1440, ], type = 'l')\n\n\n\n\n\n\nRys. 5.2: Wycinek przebiegu szeregu czasowego temperatury\n\n\n\n\nRys. 5.2 przedstawia węższy wykres danych temperatury z pierwszych 10 dni. Ponieważ dane są rejestrowane co 10 minut, otrzymujemy 24 × 6 = 144 punkty danych dziennie. Zauważ też, że ten 10-dniowy okres musi pochodzić z dość zimnego zimowego miesiąca. Gdybyś próbował przewidzieć średnią temperaturę na następny miesiąc biorąc pod uwagę kilka miesięcy danych z przeszłości, problem byłby łatwy, ze względu na okresowość danych w skali roku. Ale patrząc na dane w skali dni, temperatura wygląda o wiele bardziej chaotycznie. Czy ten szereg czasowy jest przewidywalny w skali dziennej? Przekonajmy się.\nWe wszystkich naszych eksperymentach wykorzystamy pierwsze 50% danych do szkolenia, kolejne 25% do walidacji, a ostatnie 25% do testowania. Podczas pracy z danymi szeregów czasowych ważne jest, aby używać danych walidacyjnych i testowych, które są nowsze niż dane treningowe, ponieważ próbujemy przewidzieć przyszłość na podstawie przeszłości, a nie odwrotnie, a podziały walidacyjne / testowe powinny to odzwierciedlać.\n\nKodnum_train_samples &lt;- round(nrow(full_df) * .5)\nnum_val_samples &lt;- round(nrow(full_df) * 0.25)\nnum_test_samples &lt;- nrow(full_df) - num_train_samples - num_val_samples\n\ntrain_df &lt;- full_df[seq(num_train_samples), ]\n\nval_df &lt;- full_df[seq(from = nrow(train_df) + 1,\n                      length.out = num_val_samples), ]\n\ntest_df &lt;- full_df[seq(to = nrow(full_df),\n                       length.out = num_test_samples), ]\n\ncat(\"num_train_samples:\", nrow(train_df), \"\\n\")\n\nnum_train_samples: 210226 \n\nKodcat(\"num_val_samples:\", nrow(val_df), \"\\n\")\n\nnum_val_samples: 105113 \n\nKodcat(\"num_test_samples:\", nrow(test_df), \"\\n\")\n\nnum_test_samples: 105112 \n\n\nDokładne sformułowanie problemu będzie następujące: biorąc pod uwagę dane obejmujące poprzednie pięć dni i próbkowane raz na godzinę, czy możemy przewidzieć temperaturę w ciągu 24 godzin?\nPo pierwsze, wstępnie przetworzymy dane do formatu, który może przyjąć sieć neuronowa. Dane są już numeryczne, więc nie trzeba ich wektoryzować. Jednak każdy szereg czasowy w danych ma inną skalę (np. ciśnienie atmosferyczne, mierzone w mbar, wynosi około 1000, podczas gdy H2OC, mierzone w milimolach na mol, wynosi około 3). Znormalizujemy każdą serię czasową (kolumnę) niezależnie, tak aby wszystkie przyjmowały małe wartości w podobnej skali. Użyjemy pierwszych 210 226 kroków czasowych jako danych treningowych, więc obliczymy średnią i odchylenie standardowe tylko dla tej części danych.\n\nKodinput_data_colnames &lt;- names(full_df) %&gt;%\n  setdiff(c(\"Date Time\"))\n\nnormalization_values &lt;-\n  zip_lists(mean = lapply(train_df[input_data_colnames], mean),\n            sd = lapply(train_df[input_data_colnames], sd))\n\nstr(normalization_values)\n\nList of 14\n $ p (mbar)       :List of 2\n  ..$ mean: num 989\n  ..$ sd  : num 8.51\n $ T (degC)       :List of 2\n  ..$ mean: num 8.83\n  ..$ sd  : num 8.77\n $ Tpot (K)       :List of 2\n  ..$ mean: num 283\n  ..$ sd  : num 8.87\n $ Tdew (degC)    :List of 2\n  ..$ mean: num 4.31\n  ..$ sd  : num 7.08\n $ rh (%)         :List of 2\n  ..$ mean: num 75.9\n  ..$ sd  : num 16.6\n $ VPmax (mbar)   :List of 2\n  ..$ mean: num 13.1\n  ..$ sd  : num 7.6\n $ VPact (mbar)   :List of 2\n  ..$ mean: num 9.19\n  ..$ sd  : num 4.15\n $ VPdef (mbar)   :List of 2\n  ..$ mean: num 3.95\n  ..$ sd  : num 4.77\n $ sh (g/kg)      :List of 2\n  ..$ mean: num 5.81\n  ..$ sd  : num 2.63\n $ H2OC (mmol/mol):List of 2\n  ..$ mean: num 9.3\n  ..$ sd  : num 4.2\n $ rho (g/m**3)   :List of 2\n  ..$ mean: num 1218\n  ..$ sd  : num 42\n $ wv (m/s)       :List of 2\n  ..$ mean: num 2.15\n  ..$ sd  : num 1.53\n $ max. wv (m/s)  :List of 2\n  ..$ mean: num 3.56\n  ..$ sd  : num 2.32\n $ wd (deg)       :List of 2\n  ..$ mean: num 176\n  ..$ sd  : num 85.9\n\nKodnormalize_input_data &lt;- function(df) {\n  normalize &lt;- function(x, center, scale)\n    (x - center) / scale\n\n  for(col_nm in input_data_colnames) {\n    col_nv &lt;- normalization_values[[col_nm]]\n    df[[col_nm]] %&lt;&gt;% normalize(., col_nv$mean, col_nv$sd)\n  }\n\n  df\n}\n\n\nNastępnie stwórzmy obiekt TF Dataset, który zawiera partie danych z ostatnich pięciu dni wraz z temperaturą docelową 24 godziny w przyszłości. Ponieważ próbki w zestawie danych są wysoce nadmiarowe (próbka N i próbka N + 1 będą miały większość wspólnych kroków czasowych), marnotrawstwem byłoby jawne przydzielanie pamięci dla każdej próbki. Zamiast tego będziemy generować próbki w locie, zachowując w pamięci tylko oryginalne tablice danych i nic więcej.\nMoglibyśmy z łatwością napisać funkcję w R, aby to zrobić, ale istnieje wbudowane narzędzie w keras, które właśnie to robi - (timeseries_dataset_from_array()) - więc możemy zaoszczędzić sobie trochę pracy, korzystając z niego. Ogólnie rzecz biorąc, można go używać do wszelkiego rodzaju zadań związanych z prognozowaniem szeregów czasowych.\n\n\n\n\n\n\nAdnotacja\n\n\n\nAby zrozumieć działanie funkcji timeseries_dataset_from_array(), przyjrzyjmy się prostemu przykładowi. Ogólna idea polega na dostarczeniu tablicy danych szeregów czasowych (argument dane), a funkcja timeseries_dataset_from_array() daje okna wyodrębnione z oryginalnych szeregów czasowych (nazwiemy je “sekwencjami”).\nNa przykład, jeśli użyjesz data = [0 1 2 3 4 5 6] i sequence_length = 3, wówczas timeseries_dataset_from_array() wygeneruje następujące próbki: [0 1 2], [1 2 3] , [2 3 4], [3 4 5], [4 5 6].\nDo funkcji timeseries_dataset_ from_array() można również przekazać argument targets (tablicę). Pierwszy wpis w tablicy targets powinien odpowiadać pożądanemu celowi dla pierwszej sekwencji, która zostanie wygenerowana z tablicy data. Tak więc, jeśli wykonujemy prognozowanie szeregów czasowych, targets powinny być tą samą tablicą co data, przesuniętą o pewną wartość.\nNa przykład, z data = [0 1 2 3 4 5 6 ...] i sequence_length = 3, można utworzyć zestaw danych do przewidywania następnego kroku w serii, przekazując targets = [3 4 5 6 ...]. Przykładowo:\n\nKodint_sequence &lt;- seq(10)\ndummy_dataset &lt;- timeseries_dataset_from_array(\n  data = head(int_sequence, -3),\n  targets = tail(int_sequence, -3),\n  sequence_length = 3,\n  batch_size = 2\n)\n\nlibrary(tfdatasets)\ndummy_dataset_iterator &lt;- as_array_iterator(dummy_dataset)\n\nrepeat {\n  batch &lt;- iter_next(dummy_dataset_iterator)\n  if (is.null(batch))\n    break\n  c(inputs, targets) %&lt;-% batch\n  for (r in 1:nrow(inputs))\n    cat(sprintf(\"input: [ %s ]  target: %s\\n\",\n                paste(inputs[r, ], collapse = \" \"), targets[r]))\n  cat(strrep(\"-\", 27), \"\\n\")\n}\n\ninput: [ 1 2 3 ]  target: 4\ninput: [ 2 3 4 ]  target: 5\n--------------------------- \ninput: [ 3 4 5 ]  target: 6\ninput: [ 4 5 6 ]  target: 7\n--------------------------- \ninput: [ 5 6 7 ]  target: 8\n--------------------------- \n\n\n\n\nUżyjemy funkcji timeseries_dataset_from_array(), aby utworzyć trzy zestawy danych: jeden do szkolenia, jeden do walidacji i jeden do testowania. Użyjemy następujących wartości parametrów:\n\n\nsampling_rate = 6 - obserwacje będą próbkowane z częstotliwością jednego punktu danych na godzinę (zachowamy tylko jeden punkt danych z 6).\n\nsequence_length = 120 - obserwacje będą sięgać pięciu dni wstecz (120 godzin).\n\ndelay = sampling_rate * (sequence_length + 24 - 1) - celem dla sekwencji będzie temperatura 24 godziny po zakończeniu sekwencji.\n\n\nKodsampling_rate &lt;- 6\nsequence_length &lt;- 120\ndelay &lt;- sampling_rate * (sequence_length + 24 - 1)\nbatch_size &lt;- 256\n\ndf_to_inputs_and_targets &lt;- function(df) {\n  inputs &lt;- df[input_data_colnames] %&gt;%\n    normalize_input_data() %&gt;%\n    as.matrix()\n\n  targets &lt;- as.array(df$`T (degC)`)\n\n  list(\n    head(inputs, -delay),\n    tail(targets, -delay)\n  )\n}\n\nmake_dataset &lt;- function(df) {\n  c(inputs, targets) %&lt;-% df_to_inputs_and_targets(df)\n  timeseries_dataset_from_array(\n    inputs, targets,\n    sampling_rate = sampling_rate,\n    sequence_length = sequence_length,\n    shuffle = TRUE,\n    batch_size = batch_size\n  )\n}\n\ntrain_dataset &lt;- make_dataset(train_df)\nval_dataset &lt;- make_dataset(val_df)\ntest_dataset &lt;- make_dataset(test_df)\n\n\nKażdy zestaw danych daje partie jako parę (samples, targets), gdzie samples to partia 256 próbek, z których każda zawiera 120 kolejnych godzin danych wejściowych, a targets to odpowiednia tablica 256 temperatur docelowych. Należy pamiętać, że próbki są losowo tasowane, więc dwie kolejne sekwencje w partii (takie jak sample[1, ] i sample[2, ]) niekoniecznie są czasowo blisko siebie.\n\nKodc(samples, targets) %&lt;-% iter_next(as_iterator(train_dataset))\ncat(\"samples shape: \", format(samples$shape), \"\\n\",\n    \"targets shape: \", format(targets$shape), \"\\n\", sep = \"\")\n\nsamples shape: (256, 120, 14)\ntargets shape: (256)\n\n\n\n\n\n\n\n\n\nWażne\n\n\n\nZanim zaczniemy wykorzystywać modele uczenia głębokiego typu black-box w rozwiązywaniu problemu przewidywania temperatury, wypróbujmy proste, zdroworozsądkowe podejście. Posłuży ono jako sprawdzian poprawności i ustanowi punkt odniesienia, który będziemy musieli pokonać, aby zademonstrować przydatność bardziej zaawansowanych modeli uczenia maszynowego. Takie zdroworozsądkowe podstawy mogą być przydatne, gdy zabieramy się do nowego problemu, dla którego nie ma (jeszcze) znanego rozwiązania. Klasycznym przykładem są niezrównoważone zadania klasyfikacyjne, w których niektóre klasy występują znacznie częściej niż inne. Jeśli zbiór danych zawiera 90% przypadków klasy A i 10% przypadków klasy B, wówczas zdroworozsądkowym podejściem do zadania klasyfikacji jest zawsze przewidywanie “A”, gdy prezentowana jest nowa próbka. Taki klasyfikator jest ogólnie dokładny w 90%, a zatem każde podejście oparte na uczeniu powinno pokonać ten 90% wynik, aby wykazać przydatność. Czasami takie elementarne wartości bazowe mogą okazać się zaskakująco trudne do pokonania.\n\n\n\n5.2.1 Model bazowy\nW tym przypadku można bezpiecznie założyć, że szereg czasowy temperatury jest ciągły (temperatury jutro będą prawdopodobnie zbliżone do temperatur dzisiaj), a także okresowy z okresem dziennym. Dlatego zdroworozsądkowym podejściem jest zawsze przewidywanie, że temperatura za 24 godziny będzie równa temperaturze w obecnej chwili. Oceńmy to podejście, korzystając z metryki średniego błędu bezwzględnego (MAE).\nZamiast oceniać wszystko w R stosując funkcje for, as_ array_iterator() i iter_next(), możemy to równie łatwo zrobić za pomocą transformacji TF Dataset. Najpierw wywołujemy dataset_unbatch(), aby każdy element zbioru danych stał się pojedynczym przypadkiem (sample, target). Następnie używamy funkcji dataset_map(), aby obliczyć błąd bezwzględny dla każdej pary (sample, target), a następnie dataset_reduce(), aby zgromadzić całkowity błąd i całkowitą liczbę widzianych próbek.\nPrzypomnijmy, że funkcje przekazywane do dataset_map() i dataset_reduce() będą wywoływane z tensorami symoblicznymi. Wycinanie tensora z liczbą ujemną, taką jak samples[-1, ], wybiera ostatni wyraz wzdłuż tej osi, tak jakbyśmy napisali samples[nrow(samples), ].\n\nKodevaluate_naive_method &lt;- function(dataset) {\n\n  unnormalize_temperature &lt;- function(x) {\n    nv &lt;- normalization_values$`T (degC)`\n    (x * nv$sd) + nv$mean\n  }\n\n  temp_col_idx &lt;- match(\"T (degC)\", input_data_colnames)\n\n  reduction &lt;- dataset %&gt;%\n    dataset_unbatch() %&gt;%\n    dataset_map(function(samples, target) {\n      last_temp_in_input &lt;- samples[-1, temp_col_idx]\n      pred &lt;- unnormalize_temperature(last_temp_in_input)\n      abs(pred - target)\n    }) %&gt;%\n    dataset_reduce(\n      initial_state = list(total_samples_seen = 0L,\n                           total_abs_error = 0),\n      reduce_func = function(state, element) {\n        state$total_samples_seen %&lt;&gt;% `+`(1L)\n        state$total_abs_error %&lt;&gt;% `+`(element)\n        state\n      }\n    ) %&gt;%\n    lapply(as.numeric)\n\n  mae &lt;- with(reduction,\n              total_abs_error / total_samples_seen)\n  mae\n}\n\nsprintf(\"Validation MAE: %.2f\", evaluate_naive_method(val_dataset))\n\n[1] \"Validation MAE: 2.43\"\n\nKodsprintf(\"Test MAE: %.2f\", evaluate_naive_method(test_dataset))\n\n[1] \"Test MAE: 2.62\"\n\n\nZdroworozsądkowy punkt odniesienia osiąga MAE na zbiorze walidacyjnym na poziomie 2,43 stopnia Celsjusza i MAE na testowym na poziomie 2,62 stopnia Celsjusza. Jeśli więc zawsze zakładamy, że temperatura w ciągu 24 godzin w przyszłości będzie taka sama jak obecnie, będziesz się mylił średnio o około dwa i pół stopnia. Prognoza nie jest zła ale z pewnością da się ją poprawić.\nW ten sam sposób, w jaki warto ustalić zdroworozsądkowy model bazowy przed użyciem uczenia maszynowego, warto wypróbować również proste modele uczenia maszynowego (takie jak małe, gęsto połączone sieci) przed przejściem do bardziej skomplikownych i kosztownych obliczeniowo modeli, takich jak RNN. Jest to najlepszy sposób na upewnienie się, że jakakolwiek dalsza złożoność problemu jest uzasadniona i przynosi realne korzyści.\n\n5.2.2 Prosty model sieci gęstej\nPoniższy kod pokazuje w pełni połączony model, który rozpoczyna się od spłaszczenia danych, a następnie przepuszcza je przez dwie warstwy layer_dense(). Zwróćmy uwagę na brak funkcji aktywacji w ostatniej warstwie layer_dense(), co jest typowe dla problemu regresji. Używamy błędu średniokwadratowego (MSE) jako funkcji straty, a nie MAE, ponieważ w przeciwieństwie do MAE, jest on różniczkowalny wokół zera, co jest użyteczną właściwością dla spadku gradientu. MAE będziemy również monitorować, dodając go jako metrykę w funkcji compile().\n\nKodncol_input_data &lt;- length(input_data_colnames)\n\ninputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_flatten() %&gt;%\n  layer_dense(16, activation=\"relu\") %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;%\n  compile(optimizer = \"rmsprop\",\n          loss = \"mse\",\n          metrics = \"mae\")\n\nsave_model_tf(model, \"models/jena_dense.keras\")\nsaveRDS(history, \"models/jena_dense_history.rds\")\n\nhistory &lt;- model %&gt;%\n  fit(train_dataset,\n      epochs = 10,\n      validation_data = val_dataset)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_dense.keras\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 2s - loss: 58.4128 - mae: 6.2125 - 2s/epoch - 6ms/step\n\n\n[1] \"Test MAE: 6.21\"\n\nKodhistory &lt;- readRDS(\"models/jena_dense_history.rds\")\nplot(history, metrics = \"mae\")\n\n\n\n\n\n\n\nNiektóre wartości funkcji straty na zbiorze walidacyjnym są zbliżone do modelu bazowego ale nie można powiedzieć, że model ten jest lepszy od bazowego. To pokazuje zaletę posiadania modelu odniesienia, bo okazuje się, że nie jest łatwo go “pokonać”. Zdrowy rozsądek zawiera wiele cennych informacji, do których model uczenia maszynowego nie ma dostępu. Jest to dość istotne ograniczenie uczenia maszynowego w ogóle: o ile algorytm uczenia nie jest zakodowany na sztywno, by szukać konkretnego rodzaju prostego modelu, może on czasem nie znaleźć prostego rozwiązania problemu. Właśnie dlatego wykorzystanie dobrej inżynierii cech i odpowiednich założeń dotyczących architektury ma zasadnicze znaczenie. Powinniśmy dokładnie powiedzieć modelowi, czego powinien szukać.\n\n5.2.3 Model oparty o konwolucje 1D\nMówiąc o wykorzystaniu odpowiednich założeń co do architektury, być może model konwolucyjny będzie działać poprawnie. Sieć konwolucyjna 1D mogłaby ponownie wykorzystywać te same reprezentacje w różnych dniach, podobnie jak przestrzenna sieć konwolucyjna może ponownie wykorzystywać te same reprezentacje w różnych lokalizacjach na obrazie. Znamy już warstwy layer_conv_2d() i layer_separable_conv_2d(), które widzą dane wejściowe przez małe okna (filtry), które przesuwają się po siatkach 2D. Istnieją również wersje 1D, a nawet 3D tych warstw: layer_conv_1d(), layer_separable_ conv_1d() i layer_conv_3d(). Warstwa layer_conv_1d() opiera się na oknach 1D, które przesuwają się po sekwencjach wejściowych, natomiast warstwa layer_conv_3d() opiera się na sześciennych oknach, które przesuwają się po woluminach wejściowych.\nW ten sposób można budować sieci konwolucyjne 1D, ściśle analogiczne do sieci 2D. Świetnie nadają się do wszelkich danych sekwencyjnych, które są zgodne z założeniem niezmienności translacji (co oznacza, że jeśli przesuniesz okno po sekwencji, zawartość okna powinna mieć te same właściwości niezależnie od położenia okna).\nWypróbujmy sieć 1D na naszym problemie prognozowania temperatury. Wybierzemy początkową długość okna wynoszącą 24, tak abyśmy patrzyli na 24 godziny danych na raz (jeden cykl). Gdy zmniejszymy próbkowanie sekwencji (poprzez warstwy layer_max_pooling_1d()), odpowiednio zmniejszymy rozmiar okna:\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_conv_1d(8, 24, activation = \"relu\") %&gt;%\n  layer_max_pooling_1d(2) %&gt;%\n  layer_conv_1d(8, 12, activation = \"relu\") %&gt;%\n  layer_max_pooling_1d(2) %&gt;%\n  layer_conv_1d(8, 6, activation = \"relu\") %&gt;%\n  layer_global_average_pooling_1d() %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;% fit(\n  train_dataset,\n  epochs = 10,\n  validation_data = val_dataset\n)\n\nsave_model_tf(model, filepath = \"models/jena_conv1D.keras\")\nsaveRDS(history, file = \"models/jena_conv1D_history.rds\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_conv1D.keras\")\nhistory &lt;- readRDS(\"models/jena_conv1D_history.rds\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 2s - loss: 18.9587 - mae: 3.4415 - 2s/epoch - 6ms/step\n\n\n[1] \"Test MAE: 3.44\"\n\nKodplot(history)\n\n\n\n\n\n\n\nJak się okazuje, model ten wypada jeszcze gorzej niż model gęsto połączony, osiągając jedynie testowy MAE na poziomie 3,4 stopnia, daleko od zdroworozsądkowej wartości bazowej. Co poszło nie tak? Dwie rzeczy:\n\nPo pierwsze, dane pogodowe nie do końca spełniają założenie o niezmienności translacji. Chociaż dane te charakteryzują się cyklami dobowymi, dane z poranka mają inne właściwości niż dane z wieczora lub ze środka nocy. Dane pogodowe są translacyjnie niezmienne tylko w bardzo określonej skali czasowej.\nPo drugie, kolejność w naszych danych ma duże znaczenie. Niedawna przeszłość jest znacznie bardziej pouczająca dla przewidywania temperatury następnego dnia niż dane sprzed pięciu dni. Sieć konwekcyjna 1D nie jest w stanie wykorzystać tego faktu. W szczególności nasze warstwy łączenia maksymalnego i średniego globalnego w dużej mierze niszczą informacje o kolejności.\n\n5.2.4 Model LSTM\nAni w pełni połączone podejście, ani podejście konwolucyjne nie poradziły sobie dobrze z zadanie, ale nie oznacza to, że uczenie maszynowe nie ma zastosowania do tego problemu. Sieć gęsto połączona najpierw spłaszczyła szereg czasowy, co usunęło pojęcie czasu z danych wejściowych. Podejście konwolucyjne traktowało każdy segment danych w ten sam sposób, nawet stosując łączenie, które niszczyło informacje o kolejności. Zamiast tego spójrzmy na dane jako na to, czym są - sekwencją, w której liczy się przyczynowość i kolejność. W tym celu użyjemy sieci LSTM.\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_lstm(16) %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;% fit(\n  train_dataset,\n  epochs = 10,\n  validation_data = val_dataset\n)\n\nsave_model_tf(model, filepath = \"models/jena_lstm.keras\")\nsaveRDS(history, file = \"models/jena_lstm_history.rds\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_lstm.keras\")\nhistory &lt;- readRDS(\"models/jena_lstm_history.rds\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 3s - loss: 11.2176 - mae: 2.6383 - 3s/epoch - 8ms/step\n\n\n[1] \"Test MAE: 2.64\"\n\nKodplot(history)\n\n\n\n\n\n\n\nDalej nie udało się pokonać modelu bazowego, ale jesteśmy już bardzo blisko. Można się też zastanawiąc dlaczego model LSTM wypadł znacznie lepiej niż model gęsto połączony lub Conv1D? I jak możemy dalej udoskonalać ten model? Aby odpowiedzieć na to pytanie, przyjrzyjmy się bliżej rekurencyjnym sieciom neuronowym.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#rnn",
    "href": "rnn.html#rnn",
    "title": "\n5  DNN dla danych sekwencyjnych\n",
    "section": "\n6.1 RNN",
    "text": "6.1 RNN\nRekurencyjne sieci neuronowe (ang. Recurrent Neural Network) są bardzo często używanym typem sztucznych sieci neuronowych w rozwiązywaniu zadań, w których wartości pewnych cech są obserwowane w następstwie czasowym. RNN są specjalnym typem sieci, które pozwalają na przechowywanie informacji “na później” w celu wykorzystania ich przewidywaniu przyszłych wartości. W dalszej części tego rozdziału zostaną one szczegółowo omówione. Rozdział ten jednak zaczniemy od przybliżenie z jakimi typami szeregów czasowych możemy mieć do czynienia i w jaki sposób możemy używać do nich sieci rekurencyjnych.\n\nKodmodel &lt;- keras_model_sequential() %&gt;%\n  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;%\n  layer_simple_rnn(units = 32) # retunr only last state\n\nsummary(model)\n\n\nlub\n\nKodmodel &lt;- keras_model_sequential() %&gt;%\n  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;%\n  layer_simple_rnn(units = 32, return_sequences = TRUE) # returns the full state sequence\n\nsummary(model)\n\n\nCzasami przydatne jest ułożenie kilku warstw rekurencyjnych jedna po drugiej w celu zwiększenia mocy reprezentacyjnej sieci. W takiej konfiguracji musisz pamiętać wszystkie warstwy pośrednie, aby zwrócić pełne sekwencje:\n\nKodmodel &lt;- keras_model_sequential() %&gt;%\n  layer_embedding(input_dim = 10000, output_dim = 32) %&gt;%\n  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;%\n  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;%\n  layer_simple_rnn(units = 32, return_sequences = TRUE) %&gt;%\n  layer_simple_rnn(units = 32)\n\nsummary(model)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#lstm-i-gru",
    "href": "rnn.html#lstm-i-gru",
    "title": "\n5  DNN dla danych sekwencyjnych\n",
    "section": "\n6.2 LSTM i GRU",
    "text": "6.2 LSTM i GRU\nProste RNN to nie jedyne warstwy rekurencyjne dostępne w keras. Są jeszcze dwie inne: layer_lstm i layer_gru. W praktyce zawsze będziemy używać jednej z nich, ponieważ layer_simple_rnn jest zbyt prosta, aby była naprawdę użyteczna. Jednym z głównych problemów z layer_simple_rnn jest to, że chociaż teoretycznie powinna ona być w stanie zachować w czasie \\(t\\) informacje o wejściach widzianych wiele kroków czasowych wcześniej, w praktyce takie długoterminowe zależności są niemożliwe do nauczenia. Wynika to z problemu znikającego gradientu, efektu podobnego do tego, który obserwuje się w sieciach nierekursywnych (feedforward networks), które mają wiele warstw: w miarę dodawania warstw do sieci, sieć w końcu staje się nie do wytrenowania. Teoretyczne przyczyny tego efektu były badane przez Bengio, Simard, i Frasconi (1994) we wczesnych latach 90-tych. Warstwy LSTM i GRU zostały zaprojektowane w celu rozwiązania tego problemu.\nWeźmy pod uwagę warstwę LSTM. Leżący u podstaw algorytmu Long Short-Term Memory (LSTM) kod został opracowany przez Hochreiter i Schmidhuber (1997) był on zwieńczeniem ich badań nad problemem znikającego gradientu.\nTa warstwa jest wariantem layer_simple_rnn, wzbogaconym o sposób na przenoszenie informacji przez wiele kroków czasowych. Wyobraź sobie taśmę transportową biegnącą równolegle do sekwencji, którą przetwarzasz. Informacja z sekwencji może wskoczyć na taśmę w dowolnym punkcie, zostać przetransportowana do późniejszego kroku czasowego i wyskoczyć z niej, nienaruszona, kiedy jej potrzebujesz. To jest zasadniczo to, co robi LSTM: zapisuje informacje na później, zapobiegając w ten sposób stopniowemu znikaniu starszych sygnałów podczas przetwarzania.\n\n\n\n\n\nRys. 6.2: Schemat sieci LSTM\n\n\nTypowa jednostka LSTM składa się z komórki (ang. cell), bramki wejściowej (ang. input gate), bramki wyjściowej (ang. output gate) i bramki zapomnienia (ang. forget gate). Komórka zapamiętuje wartości w dowolnych odstępach czasu, a trzy bramki regulują przepływ informacji do i z komórki. Bramki zapominania decydują o tym, jakie informacje z poprzedniego stanu należy odrzucić, przypisując poprzedniemu stanowi, w porównaniu z bieżącym wejściem, wartość z przedziału od 0 do 1. Wartość 1 oznacza zachowanie informacji, a wartość 0 - jej odrzucenie. Bramki wejściowe decydują, które kawałki nowej informacji zapisać w bieżącym stanie, używając tego samego systemu co bramki zapomnienia. Bramki wyjściowe kontrolują, które fragmenty informacji z bieżącego stanu należy wypisać, przypisując im wartość od 0 do 1, biorąc pod uwagę stan poprzedni i bieżący. Selektywne wyprowadzanie odpowiednich informacji z bieżącego stanu pozwala sieci LSTM zachować użyteczne, długoterminowe zależności, pozwalające na dokonywanie przewidywań, zarówno w bieżących, jak i przyszłych krokach czasowych.\nWróćmy do modelu opartego na LSTM, którego używaliśmy w przykładzie przewidywania temperatury. Jeśli spojrzymy na krzywe uczenia, oczywiste jest, że model szybko ulega przeuczeniu (funkcje straty zaczynają się znacznie różnić po kilku epokach), mimo że jest dość prosty. Znamy już klasyczną technikę przeciwdziałania temu zjawisku. Dropout, który losowo zeruje jednostki wejściowe warstwy, aby przerwać przypadkowe korelacje w danych treningowych, na które narażona jest warstwa. Jednak to, jak prawidłowo stosować dropout w sieciach rekurencyjnych, nie jest trywialnym pytaniem.\nSprawdzono, że zastosowanie dropout przed warstwą rekurencyjną raczej utrudnia uczenie się niż pomaga w regularyzacji. W 2016 roku Yarin Gal, w ramach swojej pracy doktorskiej na temat głębokiego uczenia bayesowskiego, określił właściwy sposób stosowania dropoutu w sieci rekurencyjnej: ta sama maska dropoutu (ten sam wzór porzuconych jednostek) powinna być stosowana w każdym kroku czasowym, zamiast stosowania maski dropoutu, która zmienia się losowo z kroku na krok czasowy. Co więcej, aby uregulować reprezentacje utworzone przez rekurencyjne bramki warstw, takich jak layer_gru() i layer_lstm(), do wewnętrznych rekurencyjnych aktywacji warstwy należy zastosować czasowo stałą maskę dropout (rekurencyjną maskę porzucania). Używanie tej samej maski porzucania w każdym kroku czasowym pozwala sieci na prawidłową propagację błędu uczenia się w czasie; czasowo losowa maska porzucania zakłóciłaby ten sygnał błędu i byłaby szkodliwa dla procesu uczenia się.\nYarin Gal przeprowadził swoje badania przy użyciu keras i pomógł zaimplementować ten mechanizm bezpośrednio w warstwach rekurencyjnych keras. Każda warstwa rekurencyjna w keras ma dwa argumenty związane z dropout: dropout, zmienna określająca współczynnik porzucenia dla jednostek wejściowych warstwy, oraz recurrent_dropout, określająca współczynnik porzucenia dla jednostek rekurencyjnych. Dodajmy rekurencyjne porzucanie do funkcji layer_lstm() naszego przykładu LSTM i zobaczmy, jak wpływa to na overfitting.\nDzięki dropoutowi nie będziemy musieli tak bardzo polegać na rozmiarze sieci do regularyzacji, więc użyjemy warstwy LSTM z dwukrotnie większą liczbą jednostek, co powinno, miejmy nadzieję, być bardziej wyraziste (bez dropoutu ta sieć od razu zaczęłaby się przeuczać2). Ponieważ sieci regularyzowane z dropoutem zawsze potrzebują znacznie więcej czasu, aby osiągnąć zbieżność, będziemy trenować model przez pięć razy więcej epok.\n2 możesz sam spróbować\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_lstm(32, recurrent_dropout = 0.25) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;% fit(\n  train_dataset,\n  epochs = 50,\n  validation_data = val_dataset\n)\nsave_model_tf(\"models/jena_lstm_dropout.keras\")\n\n\n\n\n\n\n\n\nZagrożenie\n\n\n\nModele rekurencyjne z bardzo małą liczbą parametrów, takie jak te w tym rozdziale, są zwykle znacznie szybsze na wielordzeniowym CPU niż na GPU, ponieważ obejmują tylko mnożenia małych macierzy, a łańcuch mnożeń nie jest dobrze zrównoleglony ze względu na obecność pętli for. Większe sieci RNN mogą jednak w znacznym stopniu skorzystać z możliwości GPU.\nPodczas korzystania z warstw LSTM i GRU na GPU z domyślnymi argumentami, warstwy będą wykorzystywać jądro cuDNN, wysoce zoptymalizowaną, niskopoziomową implementację algorytmu dostarczoną przez firmę NVIDIA. Niestety, jądra cuDNN są wątpliwym błogosławieństwem: są szybkie, ale nieelastyczne - jeśli spróbujemy zrobić coś, co nie jest obsługiwane przez domyślne jądro, doświadczymy dramatycznego spowolnienia. Przykładowo, rekurencyjny dropout nie jest obsługiwany przez jądra LSTM i GRU cuDNN, więc dodanie go do warstw zmusza algorytm do wykonywania na zwykłej implementacji TensorFlow, która jest generalnie od dwóch do pięciu razy wolniejsza na GPU (mimo że jej koszt obliczeniowy jest taki sam).\nAby przyspieszyć działanie warstwy RNN, gdy nie można użyć cuDNN, można spróbować ją rozwinąć (ang. unfold). Rozwijanie pętli for polega na usunięciu pętli i po prostu wpisaniu jej zawartości N razy. W przypadku pętli for sieci RNN, rozwijanie może pomóc TensorFlow zoptymalizować bazowy graf obliczeniowy. Jednak znacznie zwiększy to również zużycie pamięci przez sieć RNN. W związku z tym jest to opłacalne tylko w przypadku stosunkowo małych sekwencji (około 100 kroków lub mniej). Należy również pamiętać, że można to zrobić tylko wtedy, gdy liczba kroków czasowych w danych jest znana z góry przez model. Działa to w następujący sposób:\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, num_features))\nx &lt;- inputs %&gt;% layer_lstm(32, recurrent_dropout = 0.2, unroll = TRUE)\n\n\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_lstm_dropout.keras\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 273s - loss: 10.2870 - mae: 2.5415 - 273s/epoch - 675ms/step\n\n\n[1] \"Test MAE: 2.54\"\n\n\n\n\n\n\n\nRys. 6.3\n\n\nRys. 6.3 przedstawia wyniki uczenia. Usunięto przeuczenie (do co najmniej 20 epoki). Osiągamy MAE walidacji na poziomie zaledwie 2,37 stopnia (2,5% poprawa w stosunku do modelu bazowego bez uczenia) i testowy MAE na poziomie 2,54 stopnia (3% poprawa w stosunku do lini bazowej).\nPonieważ overfitting nie jest już tak wyraźnym problemem, ale wydaje się, że trafiliśmy na wąskie gardło wydajności, powinniśmy rozważyć zwiększenie pojemności i mocy obliczeniowej sieci. Przypomnijmy sobie opis uniwersalnego przepływu pracy uczenia maszynowego: generalnie dobrym pomysłem jest zwiększenie pojemności modelu, dopóki overfitting nie stanie się głównym problemem.\nZwiększenie pojemności sieci odbywa się zazwyczaj poprzez zwiększenie liczby neuronów w warstwach lub dodanie większej liczby warstw. Składanie warstw rekurencyjnych to klasyczny sposób budowania potężniejszych sieci rekurencyjnych. Aby układać warstwy rekurencyjne jedna na drugiej w Kerasie, wszystkie warstwy pośrednie powinny zwracać pełną sekwencję swoich wyjść (tensor rangi 3), a nie swoje wyjście w ostatnim kroku czasowym. Jak już się dowiedzieliśmy, odbywa się to poprzez ustawienie flagi return_ sequences = TRUE.\nW poniższym przykładzie wypróbujemy stos dwóch warstw rekurencyjnych z regularyzacją dropout. Dla odmiany użyjemy warstw Gated Recurrent Unit (GRU) zamiast LSTM. GRU jest bardzo podobny do LSTM - można o nim myśleć jako o nieco prostszej, usprawnionej wersji architektury LSTM. Została ona wprowadzona w 2014 roku przez Cho i in. (b.d.), gdy sieci rekurencyjne dopiero zaczynały na nowo zyskiwać zainteresowanie w niewielkiej wówczas społeczności badawczej.\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_gru(32, recurrent_dropout = 0.5, return_sequences = TRUE) %&gt;%\n  layer_gru(32, recurrent_dropout = 0.5) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;% fit(\n  train_dataset,\n  epochs = 50,\n  validation_data = val_dataset\n)\nsave_model_tf(\"models/jena_gru_dropout.keras\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_gru_dropout.keras\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 577s - loss: 9.6074 - mae: 2.4491 - 577s/epoch - 1s/step\n\n\n[1] \"Test MAE: 2.45\"\n\n\n\n\n\n\n\nRys. 6.4\n\n\nRys. 6.4 przedstawia wyniki uczenia. Osiągnęliśmy testowy MAE na poziomie 2,45 stopnia (poprawa o 6,5% w stosunku do linii bazowej). Widać, że dodana warstwa nieco poprawia wyniki, choć nie dramatycznie, zatem można zaobserwować malejące zyski ze zwiększania pojemności sieci.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "rnn.html#dwukierunkowe-sieci-rekurencyjne",
    "href": "rnn.html#dwukierunkowe-sieci-rekurencyjne",
    "title": "\n5  DNN dla danych sekwencyjnych\n",
    "section": "\n6.3 Dwukierunkowe sieci rekurencyjne",
    "text": "6.3 Dwukierunkowe sieci rekurencyjne\nOstatnią techniką, której przyjrzymy się w tej sekcji, jest dwukierunkowa sieć RNN. Dwukierunkowy RNN jest powszechnym wariantem RNN, który może oferować większą wydajność niż zwykły RNN w niektórych zadaniach. Jest często używany w przetwarzaniu języka naturalnego - można go nazwać szwajcarskim scyzorykiem głębokiego uczenia się do przetwarzania języka naturalnego.\nRNN są w szczególności zależne od kolejności: przetwarzają kroki czasowe swoich sekwencji wejściowych w kolejności, a tasowanie lub odwracanie kroków czasowych może całkowicie zmienić reprezentacje, które RNN wyodrębnia z sekwencji. To jest właśnie powód, dla którego dobrze radzą sobie z problemami, w których kolejność ma znaczenie, takimi jak problem prognozowania temperatury. Dwukierunkowa RNN wykorzystuje wrażliwość RNN na kolejność: wykorzystuje dwie zwykłe RNN, takie jak GRU i LSTM, z których każda przetwarza sekwencję wejściową w jednym kierunku (chronologicznie i antychronologicznie), a następnie łączy ich reprezentacje. Przetwarzając sekwencję w obie strony, dwukierunkowa sieć RNN może wychwycić wzorce, które mogą zostać przeoczone przez jednokierunkową sieć RNN.\nCzy RNN mogłyby działać wystarczająco dobrze, gdyby na przykład przetwarzały sekwencje wejściowe w porządku antychronologicznym (z nowszymi krokami czasowymi jako pierwszymi)? Spróbujmy tego i zobaczmy, co się stanie. Wszystko, co musimy zrobić, to zmodyfikować zestaw danych TF, aby sekwencje wejściowe zostały odwrócone wzdłuż wymiaru czasu. Wystarczy przekształcić zbiór danych za pomocą funkcji dataset_map() w następujący sposób:\n\nKoddataset_map(function(samples, targets) {\n  list(samples[, NA:NA:-1, ], targets)\n})\n\n\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  layer_lstm(16) %&gt;%\n  layer_dense(1)\nmodel &lt;- keras_model(inputs, outputs)\n\ncallbacks &lt;- list(callback_model_checkpoint(\"jena_lstm_reversed\",\n                                            save_best_only = TRUE))\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\ndataset_reverse_time &lt;- function(ds) {\n  dataset_map(ds, function(samples, targets)\n    list(samples[, NA:NA:-1, ], targets))\n}\nhistory &lt;- model %&gt;% fit(\n  train_dataset %&gt;% dataset_reverse_time(),\n  epochs = 10,\n  validation_data = val_dataset %&gt;% dataset_reverse_time(),\n  callbacks = callbacks\n)\nsave_model_tf(model, \"models/jena_lstm_rev.keras\")\nsaveRDS(history, \"models/jena_lstm_rev_hist.rds\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_lstm_rev.keras\")\nhistory &lt;- readRDS(\"models/jena_lstm_rev_hist.rds\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 3s - loss: 22.8799 - mae: 3.8075 - 3s/epoch - 8ms/step\n\n\n[1] \"Test MAE: 3.81\"\n\nKodplot(history)\n\n\n\n\n\n\n\nLSTM z odwróconym czasem silnie ustępuje nawet zdroworozsądkowemu poziomowi bazowemu, wskazując, że w tym przypadku przetwarzanie chronologiczne jest ważne dla powodzenia tego podejścia. Ma to sens: warstwa LSTM zazwyczaj lepiej zapamiętuje niedawną przeszłość niż odległą przeszłość, a naturalnie bardziej aktualne dane pogodowe niosą w sobie więcej informacji niż starsze dane (to właśnie sprawia, że zdroworozsądkowa linia bazowa jest dość silna). Tak więc chronologiczna wersja sieci z pewnością przewyższy wersję z odwróconym porządkiem.\nNie jest to jednak prawdą w przypadku wielu innych problemów, w tym języka naturalnego: intuicyjnie, znaczenie słowa w zrozumieniu zdania zwykle nie zależy od jego pozycji w zdaniu. W przypadku danych tekstowych, przetwarzanie w odwróconej kolejności działa równie dobrze jak przetwarzanie chronologiczne - można czytać tekst od tyłu. Chociaż kolejność słów ma znaczenie dla zrozumienia języka, to kolejność, której używamy, nie jest kluczowa. Co ważne, RNN wytrenowana na odwróconych sekwencjach nauczy się innych reprezentacji niż ta wytrenowana na oryginalnych sekwencjach, podobnie jak w prawdziwym świecie mielibyśmy inne modele mentalne, gdyby czas płynął wstecz - gdybyśmy żyli życiem, w którym umieramy pierwszego dnia i rodzimy się ostatniego. W uczeniu maszynowym reprezentacje, które są użyteczne, są zawsze warte wykorzystania, a im bardziej się różnią, tym lepiej, bo oferują nowy kąt, z którego można spojrzeć na dane, wychwytując aspekty danych, które zostały pominięte przez inne podejścia, a tym samym mogą pomóc zwiększyć wydajność zadania.\nDwukierunkowa sieć RNN wykorzystuje ten pomysł, aby poprawić wydajność sieci RNN z porządkiem chronologicznym. Analizuje sekwencję wejściową w obie strony (patrz Rys. 6.5), uzyskując potencjalnie bogatsze reprezentacje i wychwytując wzorce, które mogły zostać pominięte przez samą wersję chronologiczną.\n\n\n\n\n\nRys. 6.5: Zasada działania warstw dwukierunkowych\n\n\nAby utworzyć instancję dwukierunkowej RNN w Keras, należy użyć warstw bidirectional(), które jako pierwszy argument przyjmują instancję warstwy rekurencyjnej. bidirectional() tworzy drugą, oddzielną instancję tej warstwy rekurencyjnej i wykorzystuje jedną instancję do przetwarzania sekwencji wejściowych w porządku chronologicznym, a drugą instancję do przetwarzania sekwencji wejściowych w porządku odwróconym.\n\nKodinputs &lt;- layer_input(shape = c(sequence_length, ncol_input_data))\noutputs &lt;- inputs %&gt;%\n  bidirectional(layer_lstm(units = 16)) %&gt;%\n  layer_dense(1)\n\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"mse\",\n                  metrics = \"mae\")\n\nhistory &lt;- model %&gt;%\n  fit(train_dataset,\n      epochs = 10,\n      validation_data = val_dataset)\n\nsave_model_tf(model, \"models/jena_lstm_bi.keras\")\nsaveRDS(history, \"models/jena_lstm_bi_hist.rds\")\n\n\n\nKodmodel &lt;- load_model_tf(\"models/jena_lstm_bi.keras\")\nhistory &lt;- readRDS(\"models/jena_lstm_bi_hist.rds\")\nsprintf(\"Test MAE: %.2f\", evaluate(model, test_dataset)[\"mae\"])\n\n405/405 - 5s - loss: 10.8280 - mae: 2.5962 - 5s/epoch - 13ms/step\n\n\n[1] \"Test MAE: 2.60\"\n\nKodplot(history)\n\n\n\n\n\n\n\nJeśli porównamy wyniki do zwykłej layer_lstm(), to zauważymy tylko nieznaczną poprawę wyników. Łatwo jest zrozumieć, dlaczego - niemal cała zdolność predykcyjna musi pochodzić z chronologicznej połowy sieci, ponieważ wiadomo, że antychronologiczna połowa ma znacznie gorsze wyniki w tym zadaniu (ponownie, ponieważ niedawna przeszłość ma w tym przypadku znacznie większe znaczenie niż odległa przeszłość). Jednocześnie obecność antychronologicznej połowy podwaja pojemność sieci i powoduje, że zaczyna się ona przeuczać znacznie wcześniej.\nJednak dwukierunkowe sieci RNN doskonale nadają się do danych tekstowych lub innych rodzajów danych, w których kolejność ma znaczenie, ale gdzie kolejność, której używasz, nie ma znaczenia. W rzeczywistości aż do roku 2016 dwukierunkowe LSTM były uważane za najnowocześniejsze w wielu zadaniach przetwarzania języka naturalnego (przed pojawieniem się architektury Transformer, o której będzie nieco później).\nIstnieje wiele innych rzeczy, które można wypróbować w celu poprawy wydajności prognozowania temperatury:\n\nDostosować liczbę neuronów w każdej warstwie rekurencyjnej, a także ilość porzuconych neurnonów. Obecne wybory są w dużej mierze arbitralne, a zatem prawdopodobnie nieoptymalne.\nDostosować szybkość uczenia optymalizatora RMSprop lub wypróbować inny optymalizator.\nUżyć stosu kilku warstw gęstych layer_dense() jako regresora na wierzchu warstwy rekurencyjnej, zamiast pojedynczej.\nUlepszyć dane wejściowe do modelu - użyć dłuższych lub krótszych sekwencji lub innej częstotliwości próbkowania lub wykonać inżynierię cech.\n\n\n\n\n\n\n\nWskazówka\n\n\n\nGłębokie uczenie jest bardziej sztuką niż nauką. Możemy dostarczać wskazówek, które sugerują, co może działać lub nie działać w danym problemie, ale ostatecznie każdy zbiór danych jest wyjątkowy; będziesz musiał empirycznie ocenić różne strategie. Obecnie nie istnieje żadna teoria, która z góry powiedziałaby, co powinniśmy zrobić, aby optymalnie rozwiązać dany problem.\n\n\n\n\n\n\n\n\nOstrzeżenie\n\n\n\nNiektórzy czytelnicy z pewnością będą chcieli skorzystać z technik, które tu przedstawiliśmy i wypróbować je w problemie prognozowania przyszłych cen papierów wartościowych na giełdzie (lub kursów wymiany walut itp.). Rynki mają jednak zupełnie inną charakterystykę statystyczną niż zjawiska naturalne, takie jak wzorce pogodowe. Jeśli chodzi o rynki, przeszłe wyniki nie są dobrym predyktorem przyszłych zwrotów3. Z drugiej strony uczenie maszynowe ma zastosowanie do zbiorów danych, w których przeszłość jest dobrym predyktorem przyszłości, takich jak pogoda, zużycie energii elektrycznej lub ruch pieszych na danym odcinku drogi.\nZawsze pamiętajmy, że cały handel papierami wartościowymi jest zasadniczo arbitrażem informacyjnym: zdobywaniem przewagi poprzez wykorzystanie danych lub spostrzeżeń, których brakuje innym uczestnikom rynku. Próba wykorzystania dobrze znanych technik uczenia maszynowego i publicznie dostępnych danych w celu pokonania rynków jest w rzeczywistości ślepym zaułkiem, ponieważ nie będzie dawać żadnej przewagi informacyjnej w porównaniu do wszystkich innych.\n\n\n\n\n3 patrzenie w lusterko wsteczne nie jest najlepszą metodą prowadzenia auta 🙉 🤔\n\nBengio, Y., P. Simard, i P. Frasconi. 1994. „Learning long-term dependencies with gradient descent is difficult”. IEEE Transactions on Neural Networks 5 (2): 157–66. https://doi.org/10.1109/72.279181.\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, i Yoshua Bengio. b.d. „On the Properties of Neural Machine Translation: Encoder-Decoder Approaches”. https://doi.org/10.48550/arXiv.1409.1259.\n\n\nHochreiter, Sepp, i Jürgen Schmidhuber. 1997. „Long Short-Term Memory”. Neural Computation 9 (8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>DNN dla danych sekwencyjnych</span>"
    ]
  },
  {
    "objectID": "llm.html",
    "href": "llm.html",
    "title": "\n6  Modele językowe\n",
    "section": "",
    "text": "6.1 Rys historyczny\nW informatyce, ludzkie języki, takie jak angielski czy mandaryński, określa się mianem “naturalnych”, aby odróżnić je od języków zaprojektowanych dla maszyn, takich jak Assembly, LISP czy XML. Każdy język maszynowy został zaprojektowany - jego punkt wyjścia stanowił inżynier, który spisał zbiór formalnych zasad opisujących, jakie stwierdzenia można formułować w danym języku i co one oznaczają. Zasady powstały jako pierwsze, a ludzie zaczęli używać języka dopiero po ukończeniu zestawu reguł. W przypadku języka ludzkiego jest odwrotnie, najpierw pojawia się użycie, a zasady powstają później. Język naturalny został ukształtowany przez proces ewolucji, podobnie jak organizmy biologiczne - to czyni go “naturalnym”. Jego “zasady”, takie jak gramatyka języka polskiego, zostały sformalizowane dopiero po fakcie i często są ignorowane lub łamane przez jego użytkowników. W rezultacie, chociaż język “maszynowy”, czytelny dla maszyn jest wysoce uporządkowany i rygorystyczny, używając precyzyjnych zasad składniowych do łączenia dokładnie zdefiniowanych pojęć z ustalonego słownictwa, język naturalny jest nieuporządkowany – niejednoznaczny, chaotyczny, rozległy i ciągle podlegający zmianom.\nTworzenie algorytmów zdolnych do rozumienia języka naturalnego to poważne wyzwanie, bo w końcu język, a w szczególności tekst, stanowi podstawę większości naszych komunikatów i rozwoju kulturowego. Internet to w większości tekst. Język to sposób, w jaki przechowujemy niemal całą naszą wiedzę. Nasze myśli w dużej mierze opierają się na języku. Jednak zdolność rozumienia języka naturalnego przez długi czas była poza zasięgiem maszyn. Niektórzy ludzie naiwnie sądzili, że można po prostu spisać “zestaw zasad języka angielskiego”, podobnie jak można spisać zestaw zasad LISP1. Wczesne próby budowy systemów przetwarzania języka naturalnego (NLP) były więc podejmowane przez pryzmat “stosowanej lingwistyki”. Inżynierowie i lingwiści ręcznie tworzyli złożone zestawy zasad, aby wykonać podstawowe tłumaczenia maszynowe lub stworzyć proste chatboty, takie jak słynny program ELIZA2 z lat 60., który używał dopasowywania wzorców, aby podtrzymać bardzo podstawową konwersację. Ale język jest rzeczą niepokorną: nie poddaje się łatwo formalizacji. Po kilku dekadach wysiłków możliwości tych systemów pozostały rozczarowujące.\nRęcznie tworzone reguły utrzymywały się jako dominujące podejście aż do lat 90. XX wieku. Jednak zaczynając od końca lat 80., szybsze komputery i większa dostępność danych zaczęły czynić alternatywę bardziej wykonalną. Gdy znajdziemy się w sytuacji, w której budujemy systemy będące dużymi stosami reguł ad hoc, prawdopodobnie zaczniemy zadawać sobie pytanie: “Czy mogę użyć korpusu danych, aby zautomatyzować proces znajdowania tych reguł? Czy mogę szukać reguł w pewnej przestrzeni reguł, zamiast samemu je wymyślać?” I tak właśnie przeszło się do uczenia maszynowego. W związku z tym, pod koniec lat 80. zaczęliśmy obserwować podejścia uczenia maszynowego do przetwarzania języka naturalnego. Najwcześniejsze z nich opierały się na drzewach decyzyjnych – intencją było dosłownie zautomatyzowanie rozwoju rodzaju reguł if/then/else poprzednich systemów. Następnie podejścia statystyczne zaczęły zyskiwać na popularności, zaczynając od regresji logistycznej. Z czasem, modele parametryczne oparte na uczeniu w pełni przejęły kontrolę, a lingwistyka zaczęła być postrzegana bardziej jako przeszkoda niż użyteczne narzędzie. Frederick Jelinek, wczesny badacz rozpoznawania mowy, żartował w latach 90.: “Za każdym razem, gdy zwalniam lingwistę, wydajność systemu rozpoznawania mowy rośnie.”\nTo na czym polega współczesne przetwarzanie języka naturalnego (NLP), to wykorzystanie uczenia maszynowego i dużych zbiorów danych, aby dać komputerom zdolność nie tyle rozumienia języka, co bardziej ambitnego celu, przyswajania fragmentu języka jako danych wejściowych i zwracania czegoś użytecznego, na przykład przewidywania następujących kwestii:\nOczywiście, powinniśmy pamiętać, że modele przetwarzania tekstu, które będziemy szkolić, nie będą posiadać ludzkiego zrozumienia języka, raczej będą po prostu szukać statystycznych reguł w danych wejściowych, co okazuje się wystarczające do dobrego wykonywania wielu prostych zadań. W podobny sposób, w jaki rozpoznawanie obrazów, to rozpoznawanie wzorców stosowane do pikseli, przetwarzanie języka naturalnego (NLP) to rozpoznawanie wzorców stosowane do słów, zdań i akapitów.\nNarzędzia NLP - drzewa decyzyjne i regresja logistyczna - ewoluowały, choć powoli od lat 90. do wczesnych lat 2010. Większość badań skupiała się na inżynierii cech. Kiedy François Chollet wygrał swój pierwszy konkurs NLP na Kaggle w 2013 roku, jego model opierał się na drzewach decyzyjnych i regresji logistycznej. Jednak około 2014-2015 roku sytuacja zaczęła się wreszcie zmieniać. Wielu badaczy zaczęło badać zdolności rozumienia języka przez rekurencyjne sieci neuronowe, w szczególności LSTM.\nNa początku 2015 roku, keras udostępnił pierwszą otwartą, łatwą w użyciu implementację LSTM, tuż na początku ogromnej fali zainteresowania sieciami neuronowymi rekurencyjnymi. Następnie od 2015 do 2017 roku, sieci neuronowe rekurencyjne zdominowały rozwijającą się scenę NLP. Modele LSTM dwukierunkowe, w szczególności, ustanowiły standard w wielu ważnych zadaniach, od streszczania, przez odpowiedzi na pytania, po tłumaczenie maszynowe. W końcu około 2017–2018 roku pojawiła się nowa architektura, która zastąpiła RNN - transformer, o którym dowiemy się więcej w drugiej części tego rozdziału. Transformatory umożliwiły znaczący postęp w całej dziedzinie w krótkim czasie, a obecnie większość systemów NLP opiera się na nich.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#rys-historyczny",
    "href": "llm.html#rys-historyczny",
    "title": "\n6  Modele językowe\n",
    "section": "",
    "text": "1 LISP (skrót od LISt Processing) jest jednym z najstarszych języków programowania, nadal używanych. Został zaprojektowany w 1958 roku przez Johna McCarthy’ego w Massachusetts Institute of Technology (MIT)2 ELIZA to jeden z pierwszych programów komputerowych, który imitował rozmowę z człowiekiem. Został stworzony w połowie lat 60. XX wieku przez Josepha Weizenbauma w Massachusetts Institute of Technology (MIT)\n\n\n“Jaki jest temat tego tekstu?” (klasyfikacja tekstu);\n“Czy ten tekst zawiera treści obraźliwe?” (filtrowanie treści);\n“Czy ten tekst brzmi pozytywnie czy negatywnie?” (analiza sentymentu);\n“Jaki powinno być następne słowo w tym niekompletnym zdaniu?” (modelowanie języka);\n“Jak powiedziałbyś to po niemiecku?” (tłumaczenie);\n“Jak można by streścić ten artykuł w jednym akapicie?” (streszczanie);\nI tak dalej.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#standaryzacja-tekstu",
    "href": "llm.html#standaryzacja-tekstu",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.1 Standaryzacja tekstu",
    "text": "7.1 Standaryzacja tekstu\nStandaryzacja tekstu jest podstawową formą inżynierii cech, która ma na celu usunięcie różnic w kodowaniu, z którymi nie chcemy, aby nasz model miał do czynienia. Nie jest to wyłączna dziedzina uczenia maszynowego - musielibyśmy zrobić to samo, gdybyśmy budowali wyszukiwarkę. Jednym z najprostszych i najbardziej rozpowszechnionych schematów standaryzacji jest “konwersja na małe litery i usunięcie znaków interpunkcyjnych”.\nInnym częstym przekształceniem jest konwersja znaków specjalnych do standardowej formy, np. zastąpienie “é” przez “e”, “æ” przez “ae” itd. Np. token “méxico” stałby się wtedy “mexico”.\nOstatnim, znacznie bardziej zaawansowanym wzorcem standaryzacji, który jest rzadziej używany w kontekście uczenia maszynowego, jest stemming: przekształcanie odmian słów (takich jak różne formy koniugacyjne czasownika) w jedną wspólną reprezentację, jak przekształcanie “złapany” i “łapiąc” w “[łapać]” lub “koty” w “[kot]”. Dzięki stemmingowi, “rozpoczynając” i “rozpoczęty” stałyby się czymś w rodzaju “[rozpoczynać]”.\nDzięki tym technikom standaryzacji, nasz model będzie wymagał mniej danych treningowych i będzie lepiej generalizował - nie będzie potrzebował wielu przykładów zarówno “Zachodu słońca”, jak i “zachodów słońca”, aby nauczyć się, że oznaczają one to samo, i będzie w stanie nadać sens słowu “Meksyk”, nawet jeśli widział tylko “meksyk” w swoim zestawie treningowym. Oczywiście standaryzacja może również wymazać pewną ilość informacji, więc zawsze należy pamiętać o kontekście: na przykład, jeśli piszesz model, który wyodrębnia pytania z artykułów z wywiadami, powinien on zdecydowanie traktować “?” jako oddzielny token zamiast go upuszczać, ponieważ jest to przydatny sygnał dla tego konkretnego zadania.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#tokenizacja",
    "href": "llm.html#tokenizacja",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.2 Tokenizacja",
    "text": "7.2 Tokenizacja\nKiedy tekst jest już znormalizowany, musimy podzielić go na jednostki do wektoryzacji (tokeny) - krok zwany tokenizacją. Można to zrobić na trzy różne sposoby:\n\nTokenizacja na poziomie słowa - gdzie tokeny są oddzielonymi spacjami (lub interpunkcją) podciągami. Wariantem tego jest dalsze dzielenie słów na podsłowia, gdy ma to zastosowanie, na przykład traktowanie “zaczyna” jako “zaczyna+jąc” lub “wezwany” jako “wezwani”.\nN-gram tokenizacji - gdzie tokeny są grupami N kolejnych słów. Na przykład “the cat” lub “he was” byłyby tokenami 2-gramowymi (zwanymi również bigramami). Generalnie N-gramy słów to grupy N (lub mniej) kolejnych słów, które można wyodrębnić ze zdania. Ta sama koncepcja może być również zastosowana do znaków zamiast słów.\nTokenizacja na poziomie znaków - gdzie każdy znak jest swoim własnym tokenem. W praktyce, ten schemat jest rzadko używany i naprawdę widzisz go tylko w specjalistycznych kontekstach, takich jak generowanie tekstu lub rozpoznawanie mowy.\n\nOgólnie rzecz biorąc, zawsze będziemy używać tokenizacji na poziomie słowa lub N-gramu. Istnieją dwa rodzaje modeli przetwarzania tekstu: te, które dbają o kolejność słów, zwane modelami sekwencyjnymi, oraz te, które traktują słowa wejściowe jako zestaw, odrzucając ich oryginalną kolejność, zwane modelami bag-of-words. Jeśli budujesz model sekwencyjny, używasz tokenizacji na poziomie słów, a jeśli budujesz model worka słów, używasz tokenizacji N-gramów. N-gramy są sposobem na sztuczne wprowadzenie do modelu niewielkiej ilości informacji o lokalnym porządku słów. W tym rozdziale dowiesz się więcej o każdym typie modelu i o tym, kiedy należy ich używać.\nPrzykładowo zdanie “the cat sat on the mat” można zamienić na bigramy w następujący sposób:\n\nKodc(\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\")\n\n [1] \"the\"     \"the cat\" \"cat\"     \"cat sat\" \"sat\"     \"sat on\"  \"on\"     \n [8] \"on the\"  \"the mat\" \"mat\"    \n\n\nnatomiast w 3-gramy:\n\nKodc(\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\",\n \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\",\n \"sat on the\", \"the mat\", \"mat\", \"on the mat\")\n\n [1] \"the\"         \"the cat\"     \"cat\"         \"cat sat\"     \"the cat sat\"\n [6] \"sat\"         \"sat on\"      \"on\"          \"cat sat on\"  \"on the\"     \n[11] \"sat on the\"  \"the mat\"     \"mat\"         \"on the mat\"",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#indeksowanie-słownika",
    "href": "llm.html#indeksowanie-słownika",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.3 Indeksowanie słownika",
    "text": "7.3 Indeksowanie słownika\nGdy nasz tekst jest podzielony na tokeny, musimy zakodować każdy token w reprezentacji numerycznej. Wszystkie procesy wektoryzacji tekstu polegają na zastosowaniu pewnego schematu tokenizacji, a następnie skojarzeniu wektorów liczbowych z wygenerowanymi tokenami. Wektory te, spakowane w tensory sekwencji, są wprowadzane do głębokich sieci neuronowych. Istnieje wiele sposobów na powiązanie wektora z tokenem. W tej sekcji przedstawimy dwa główne: kodowanie tokenów metodą one-hot oraz osadzanie tokenów (ang. embeddings - zwykle używane wyłącznie dla słów). Pozostała część tego rozdziału wyjaśnia te techniki i pokazuje jak ich użyć, aby przejść od surowego tekstu do tensora, który można wysłać do sieci.\nTechnicznie rzecz ujmując, należy zauważyć, że na tym etapie często ogranicza się słownictwo tylko do 20000 lub 30000 najczęściej występujących słów znalezionych w danych treningowych. Każdy zbiór danych tekstowych ma tendencję do zawierania ogromnej liczby unikalnych terminów, z których większość pojawia się tylko raz lub dwa. Indeksowanie tych rzadkich terminów skutkowałoby nadmiernie dużą przestrzenią cech, gdzie większość cech miałaby prawie żadną zawartość informacyjną.\nWażny szczegół, który nie powinien umknąć naszej uwadze: gdy szukamy nowego tokenu w naszym indeksie słownictwa, może się okazać, że go tam nie ma. Nasze dane treningowe mogły nie zawierać żadnego wystąpienia słowa “cherimoya”3 (lub być może wykluczyliśmy je z indeksu, ponieważ było zbyt rzadkie), więc wykonanie polecenia token_index = match(\"cherimoya\", vocabulary) może zwrócić NA. Aby sobie z tym poradzić, należy użyć indeksu “poza słownictwem” (skrótowo OOV index) - rodzaju schowka na wszystkie tokeny, które nie znalazły się w indeksie. Zwykle jest to indeks 1: tak naprawdę wykonujesz token_index = match(\"cherimoya\", vocabulary, nomatch = 1). Dekodując sekwencję liczb całkowitych z powrotem na słowa, zastąpisz 1 czymś w rodzaju “[UNK]” (co nazwałbyś “tokenem OOV”).\n3 przypominający kształtem serce owoc, jest słodki jak budyń, dlatego Anglicy nazywają go custard apple - czyli jabłko o smaku kremu budyniowego“Dlaczego używamy 1, a nie 0?” można zapytać. Ponieważ 0 jest już zajęte. Istnieją dwa specjalne tokeny, których będziemy często używać: token OOV (indeks 1) i token maski (indeks 0). Chociaż token OOV oznacza “to jest słowo, którego nie rozpoznaliśmy”, to token maski mówi nam “zignoruj mnie, nie jestem słowem”. Używa się go w szczególności do uzupełniania danych sekwencyjnych: ponieważ partie danych muszą być ciągłe, wszystkie sekwencje w partii danych sekwencyjnych muszą mieć tę samą długość, więc krótsze sekwencje powinny być uzupełniane do długości najdłuższej sekwencji. Jeśli chcemy utworzyć partię danych z sekwencjami c(5, 7, 124, 4, 89) i c(8, 34, 21), musiałaby ona wyglądać następująco:\n\nKodrbind(c(5,  7, 124, 4, 89),\n      c(8, 34,  21, 0,  0))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    5    7  124    4   89\n[2,]    8   34   21    0    0\n\n\nWszystkie omówione do tej pory kroki można w prosty sposób zaprogramować w R:\n\nKodlibrary(keras) # póki co potrzebny tylko by mieć %&gt;%\nnew_vectorizer &lt;- function() {\n  self &lt;- new.env(parent = emptyenv()) # Utworzenie nowego środowiska dla wektoryzatora\n  attr(self, \"class\") &lt;- \"Vectorizer\"   # Nadanie środowisku klasy \"Vectorizer\"\n\n  self$vocabulary &lt;- c(\"[UNK]\")         # Inicjalizacja słownika słów z tokenem \"[UNK]\" (nieznane słowo)\n\n  self$standardize &lt;- function(text) {\n    text &lt;- tolower(text)               # Przekształcenie tekstu na małe litery\n    gsub(\"[[:punct:]]\", \"\", text)       # Usunięcie znaków interpunkcyjnych z tekstu\n  }\n\n  self$tokenize &lt;- function(text) {\n    unlist(strsplit(text, \"[[:space:]]+\")) # Podział tekstu na tokeny (słowa) na podstawie spacji\n  }\n\n  self$make_vocabulary &lt;- function(text_dataset) {\n    tokens &lt;- text_dataset %&gt;%\n      self$standardize() %&gt;%\n      self$tokenize()                   # Standardyzacja i tokenizacja tekstu\n    self$vocabulary &lt;- unique(c(self$vocabulary, tokens)) # Aktualizacja słownika o unikalne tokeny\n  }\n\n  self$encode &lt;- function(text) {\n    tokens &lt;- text %&gt;%\n      self$standardize() %&gt;%\n      self$tokenize()                   # Standardyzacja i tokenizacja tekstu\n    match(tokens, table = self$vocabulary, nomatch = 1) # Zamiana tokenów na ich indeksy w słowniku\n  }\n\n  self$decode &lt;- function(int_sequence) {\n    vocab_w_mask_token &lt;- c(\"\", self$vocabulary) # Słownik z dodanym pustym tokenem na początku\n    vocab_w_mask_token[int_sequence + 1]         # Zamiana sekwencji indeksów na słowa\n  }\n\n  self # Zwrócenie środowiska wektoryzatora\n}\n\n\nA tak wygląda on w działaniu… Najpierw tworzymy słownik na podstawie prostego korpusu.\n\nKodvectorizer &lt;- new_vectorizer()\n\ndataset &lt;- c(\n    \"I write, erase, rewrite\",\n    \"Erase again, and then\",\n    \"A poppy blooms.\"\n)\n\nvectorizer$make_vocabulary(dataset)\nvectorizer$vocabulary\n\n [1] \"[UNK]\"   \"i\"       \"write\"   \"erase\"   \"rewrite\" \"again\"   \"and\"    \n [8] \"then\"    \"a\"       \"poppy\"   \"blooms\" \n\n\nA następnie wykorzystujemy go do nowego zdania.\n\nKodtest_sentence &lt;- \"I write, rewrite, and still rewrite again\"\nencoded_sentence &lt;- vectorizer$encode(test_sentence)\nprint(encoded_sentence)\n\n[1] 2 3 5 7 1 5 6\n\nKoddecoded_sentence &lt;- vectorizer$decode(encoded_sentence)\nprint(decoded_sentence)\n\n[1] \"i\"       \"write\"   \"rewrite\" \"and\"     \"[UNK]\"   \"rewrite\" \"again\"  \n\n\nChoć jak widać wszystko działa poprawnie, to w praktycznych zastosowaniach będziemy korzystali raczej z rozwiązań w keras typu warstwa layer_text_vectorization().\n\nKodtext_vectorization &lt;- layer_text_vectorization(output_mode = \"int\")\n\n\nDomyślnie, layer_text_vectorization() będzie używać ustawienia “konwertuj na małe litery i usuń znaki interpunkcyjne” do standaryzacji tekstu oraz “dziel względem znaków przerw (typu spacja)” do tokenizacji. Ale co ważne, można dostarczyć niestandardowe funkcje do standaryzacji i tokenizacji, co oznacza, że warstwa jest wystarczająco elastyczna, aby obsłużyć każdy przypadek użycia. Należy pamiętać, że takie niestandardowe funkcje powinny działać na tensorach typu tf.string, a nie na zwykłych wektorach znaków R! Na przykład, domyślne zachowanie warstwy jest równoważne następującemu:\n\nKodlibrary(tensorflow)\ncustom_standardization_fn &lt;- function(string_tensor) {\n  string_tensor %&gt;%\n    tf$strings$lower() %&gt;%\n    tf$strings$regex_replace(\"[[:punct:]]\", \"\")\n}\n\ncustom_split_fn &lt;- function(string_tensor) {\n  tf$strings$split(string_tensor)\n}\n\ntext_vectorization &lt;- layer_text_vectorization(\n  output_mode = \"int\",\n  standardize = custom_standardization_fn,\n  split = custom_split_fn\n)\n\n\nAby zindeksować słownictwo korpusu tekstowego, wystarczy wywołać metodę adapt() warstwy z obiektem TF Dataset, który daje ciągi znaków, lub po prostu z wektorem znaków R:\n\nKoddataset &lt;- c(\"I write, erase, rewrite\",\n             \"Erase again, and then\",\n             \"A poppy blooms.\")\nadapt(text_vectorization, dataset)\n\n\nNależy pamiętać, że obliczone słownictwo można pobrać za pomocą funkcji get_vocabulary(). Może to być przydatne, jeśli trzeba przekonwertować tekst zakodowany jako sekwencje liczb całkowitych z powrotem na słowa. Pierwsze dwa wpisy w słowniku to token maski (indeks 0) i token OOV (indeks 1). Wpisy na liście słownictwa są sortowane według częstotliwości, więc w przypadku zbioru danych z rzeczywistego świata bardzo popularne słowa, takie jak “the” lub “a”, będą na pierwszym miejscu.\n\nKodget_vocabulary(text_vectorization)\n\n [1] \"\"        \"[UNK]\"   \"erase\"   \"write\"   \"then\"    \"rewrite\" \"poppy\"  \n [8] \"i\"       \"blooms\"  \"and\"     \"again\"   \"a\"      \n\n\nNa potrzeby prezentacji, spróbujmy zakodować, a następnie zdekodować przykładowe zdanie:\n\nKodvocabulary &lt;- text_vectorization %&gt;% get_vocabulary()\ntest_sentence &lt;- \"I write, rewrite, and still rewrite again\"\nencoded_sentence &lt;- text_vectorization(test_sentence)\ndecoded_sentence &lt;- paste(vocabulary[as.integer(encoded_sentence) + 1],\n                          collapse = \" \")\n\nencoded_sentence\n\ntf.Tensor([ 7  3  5  9  1  5 10], shape=(7), dtype=int64)\n\nKoddecoded_sentence\n\n[1] \"i write rewrite and [UNK] rewrite again\"\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nPonieważ layer_text_vectorization() jest głównie operacją wyszukiwania słownika, która konwertuje tokeny na liczby całkowite, nie może być wykonywana na GPU (lub TPU) - tylko na CPU. Jeśli więc trenujemy nasz model na GPU, funkcja layer_text_vectorization() zostanie uruchomiona na CPU przed wysłaniem danych wyjściowych do GPU. Ma to istotny wpływ na wydajność.\nIstnieją dwa sposoby wykorzystania funkcji layer_text_vectorization(). Pierwszą opcją jest umieszczenie jej w potoku TF Dataset, tak jak poniżej:\n\nKodint_sequence_dataset &lt;- string_dataset %&gt;%\n  dataset_map(text_vectorization, num_parallel_calls = 4)\n\n\nDrugą opcją jest uczynienie go częścią modelu (w końcu jest to warstwa keras), jak poniżej (w pseudokodzie):\n\nKodtext_input &lt;- layer_input(shape = shape(), dtype = \"string\")\nvectorized_text &lt;- text_vectorization(text_input)\nembedded_input &lt;- vectorized_text %&gt;% layer_embedding(...)\noutput &lt;- embedded_input %&gt;% ...\nmodel &lt;- keras_model(text_input, output)\n\n\nJest między nimi ważna różnica: jeśli krok wektoryzacji jest częścią modelu, będzie on wykonywany synchronicznie z resztą modelu. Oznacza to, że na każdym etapie uczenia reszta modelu (umieszczona na GPU) będzie musiała poczekać, aż dane wyjściowe layer_text_vectorization() (umieszczone na CPU) będą gotowe, zanim będzie mogła rozpocząć pracę. Tymczasem umieszczenie warstwy w potoku TF Dataset umożliwia asynchroniczne wstępne przetwarzanie danych na CPU: podczas gdy GPU uruchamia model na jednej partii zwektoryzowanych danych, CPU pozostaje zajęty wektoryzacją następnej partii surowych ciągów.\nJeśli trenujemy model na GPU lub TPU, prawdopodobnie będziemy chcieli wybrać pierwszą opcję4, aby uzyskać najlepszą wydajność. Podczas trenowania na CPU, przetwarzanie synchroniczne jest w porządku: uzyskamy wówczas 100% wykorzystania rdzeni, niezależnie od wybranej opcji.\nTeraz, jeśli mielibyśmy wyeksportować nasz model do środowiska produkcyjnego, chcielibyśmy wysłać model, który akceptuje surowe ciągi znaków jako dane wejściowe, jak w powyższym fragmencie kodu dla drugiej opcji; w przeciwnym razie musielibyśmy ponownie wdrożyć standaryzację tekstu i tokenizację w naszym środowisku produkcyjnym (np. w JavaScript). Stanęlibyśmy w obliczu ryzyka wprowadzenia niewielkich zmian w przetwarzaniu wstępnym, które zaszkodziłyby dokładności modelu. Na szczęście funkcja layer_text_vectorization() umożliwia włączenie wstępnego przetwarzania tekstu bezpośrednio do modelu, co ułatwia jego wdrożenie, nawet jeśli pierwotnie warstwa była używana jako część potoku TF Dataset.\n\n\n4 czyli osadzenie wektoryzacji w potoku",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#one-hot-encoding",
    "href": "llm.html#one-hot-encoding",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.4 One-hot encoding",
    "text": "7.4 One-hot encoding\nKodowanie one-hot jest najczęstszym, najbardziej podstawowym sposobem przekształcenia tokena w wektor. Polega ono na skojarzeniu unikalnego indeksu z każdym słowem, a następnie przekształceniu tego indeksu \\(i\\) w wektor binarny o rozmiarze \\(N\\) (rozmiar słownika); wektor składa się ze wszystkich zer, z wyjątkiem \\(i\\)-tego wpisu, który jest 1.\n\nKodone_hot_encode_token &lt;- function(token) {\n  vector &lt;- array(0, dim = length(vocabulary))\n  token_index &lt;- match(token, vocabulary)\n  vector[token_index] &lt;- 1\n  vector\n}\n\n\nMetoda ta, zważywszy na swoją rzadką reprezentację (większość wartości to 0), rzadko stosowana w praktyce. Słaba wydajność tej techniki spowodowała powstanie embedingów. Zanim jednak przejdziemy do embedingów przyjrzymy się dokładniej dwóm podejściom do reprezentacji grup słów: zbiorom słów (ang. bag-of-words) i ciągom słów, w których kolejność jest ważna. Zrobimy to na przykładzie danych IMDB.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#tokenizacja-na-przykładach",
    "href": "llm.html#tokenizacja-na-przykładach",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.5 Tokenizacja na przykładach",
    "text": "7.5 Tokenizacja na przykładach\nSposób, w jaki model uczenia maszynowego powinien reprezentować poszczególne słowa, jest stosunkowo niekontrowersyjną kwestią: są to cechy kategorialne (wartości z predefiniowanego zestawu) i wiemy, jak sobie z nimi radzić. Powinny one być zakodowane jako wymiary w przestrzeni cech lub jako wektory kategorii (w tym przypadku wektory słów). Znacznie bardziej problematyczną kwestią jest jednak to, jak zakodować sposób, w jaki słowa są wplecione w zdania - kolejność słów.\nProblem kolejności w języku naturalnym jest interesujący. W przeciwieństwie do kroków szeregu czasowego, słowa w zdaniu nie mają naturalnej, kanonicznej kolejności. Różne języki porządkują podobne słowa na bardzo różne sposoby. Na przykład, struktura zdań w języku angielskim jest zupełnie inna niż w języku japońskim. Nawet w obrębie danego języka można zazwyczaj powiedzieć to samo na różne sposoby, zmieniając nieco kolejność słów. Co więcej, jeśli słowa w krótkim zadaniu ułożysz losowo, to nadal możesz w dużej mierze dowiedzieć się, co zostało powiedziane, choć w wielu przypadkach pojawia się dwuznaczność. Kolejność jest wyraźnie ważna, ale jej związek ze znaczeniem nie jest prosty.\nSposób reprezentowania kolejności słów jest kluczowym pytaniem, z którego wynikają różne rodzaje architektur NLP. Najprostszą rzeczą, jaką można zrobić, jest po prostu odrzucenie kolejności i traktowanie tekstu jako nieuporządkowanego zbioru słów - daje to modele worków słów. Można również zdecydować, że słowa powinny być przetwarzane ściśle w kolejności, w jakiej się pojawiają, pojedynczo, jak kroki w szeregu czasowym - można wtedy wykorzystać modele rekurencyjne z poprzedniego rozdziału. Wreszcie, możliwe jest również podejście hybrydowe: architektura transformer jest technicznie niezależna od kolejności, ale wprowadza informacje o pozycji słów do przetwarzanych reprezentacji, co pozwala jej jednocześnie patrzeć na różne części zdania (w przeciwieństwie do RNN), a jednocześnie jest świadoma kolejności. Ponieważ uwzględniają one kolejność słów, zarówno RNN, jak i transformatory nazywane są modelami sekwencyjnymi.\nHistorycznie rzecz ujmując, większość wczesnych zastosowań uczenia maszynowego w NLP obejmowała po prostu modele bag-of-words. Zainteresowanie modelami sekwencyjnymi zaczęło rosnąć dopiero w 2015 roku, wraz z odrodzeniem się rekurencyjnych sieci neuronowych. Obecnie oba podejścia pozostają istotne.\n\n7.5.1 Przygotowanie danych IMDB\nZacznijmy od pobrania danych.\n\nKod# Zdefiniowanie URL, z którego ma być pobrany plik\nurl &lt;- \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" \n\nfilename &lt;- paste0(\"/Users/majerek/\", basename(url)) # Pobranie nazwy pliku z URL, w tym przypadku 'aclImdb_v1.tar.gz'\n\noptions(timeout = 60*10) # Ustawienie limitu czasu na pobieranie na 10 minut (60 sekund * 10)\n\ndownload.file(url, destfile = filename) # Pobranie pliku z zadanego URL i zapisanie go pod nazwą \n\nuntar(filename, exdir = \"/Users/majerek/\") # Rozpakowanie pobranego pliku tar.gz\n\n\nTak wygląda struktura katalogu po rozpakowaniu.\n\nKodfs::dir_tree(\"/Users/majerek/aclImdb\", recurse = 1, type = \"directory\")\n\n/Users/majerek/aclImdb\n├── test\n│   ├── neg\n│   └── pos\n├── train\n│   ├── neg\n│   └── pos\n└── val\n    ├── neg\n    └── pos\n\n\nNa przykład katalog train/pos/ zawiera zestaw 12500 plików tekstowych, z których każdy zawiera tekst recenzji filmu o pozytywnym wydźwięku, który zostanie wykorzystany jako dane szkoleniowe. Recenzje o negatywnym wydźwięku znajdują się w katalogach “neg”. W sumie istnieje 25000 plików tekstowych do szkolenia i kolejne 25000 do testowania. Znajduje się tam również podkatalog train/unsup, którego nie potrzebujemy. Usuńmy go:\n\nKodfs::dir_delete(\"/Users/majerek/aclImdb/train/unsup/\")\n\n\nSpójrzmy na zawartość kilku z tych plików tekstowych.\n\nKodwriteLines(readLines(\"/Users/majerek/aclImdb/train/pos/4000_10.txt\", warn = FALSE))\n\nWow. Saw this last night and I'm still reeling from how good it was. Every character felt so real (although most of them petty, selfish a**holes) and the bizarre story - middle aged widow starts shagging her daughter's feckless boyfriend - felt utterly convincing. Top performances all round but hats off to Anne Reid and Our Friends in the North's Daniel Craig (the latter coming across as the next David Thewlis).&lt;br /&gt;&lt;br /&gt;And director Roger Michell? This is as far from Notting Hill as it's possible to be. Thank God.&lt;br /&gt;&lt;br /&gt;Watch this movie!!!\n\n\nNastępnie przygotujmy zestaw walidacyjny, oddzielając 20% treningowych plików tekstowych w nowym katalogu, aclImdb/val.\n\nKodlibrary(fs)\nset.seed(1337)\nbase_dir &lt;- path(\"/Users/majerek/aclImdb\")\n\nfor (category in c(\"neg\", \"pos\")) {\n  filepaths &lt;- dir_ls(base_dir / \"train\" / category)\n  num_val_samples &lt;- round(0.2 * length(filepaths))\n  val_files &lt;- sample(filepaths, num_val_samples)\n\n  dir_create(base_dir / \"val\" / category)\n  file_move(val_files,\n            base_dir / \"val\" / category)\n}\n\n\nW zagadnieniach z analizy obrazów używaliśmy funkcji image_dataset_from_directory() do utworzenia zbiorów obrazów i ich etykiet dla struktury katalogów. Dokładnie to samo można zrobić dla plików tekstowych za pomocą narzędzia text_dataset_from_directory(). Utwórzmy trzy obiekty TF Dataset do uczenia, walidacji i testowania:\n\nKodlibrary(keras)\nlibrary(tfdatasets)\n\ntrain_ds &lt;- text_dataset_from_directory(\"/Users/majerek/aclImdb/train\")\n\nFound 20000 files belonging to 2 classes.\n\nKodval_ds &lt;- text_dataset_from_directory(\"/Users/majerek/aclImdb/val\")\n\nFound 5000 files belonging to 2 classes.\n\nKodtest_ds &lt;- text_dataset_from_directory(\"/Users/majerek/aclImdb/test\")\n\nFound 25000 files belonging to 2 classes.\n\n\nTe zestawy danych tworzą dane wejściowe, które są tensorami tf.string i wyjścia, które są tensorami int32 kodującymi wartość “0” lub “1”.\n\nKodc(inputs, targets) %&lt;-% iter_next(as_iterator(train_ds))\nstr(inputs)\n\n&lt;tf.Tensor: shape=(32), dtype=string, numpy=…&gt;\n\nKodstr(targets)\n\n&lt;tf.Tensor: shape=(32), dtype=int32, numpy=…&gt;\n\nKodinputs[1]\n\ntf.Tensor(b'I think the movie was pretty good, will add it to my \"clasic collection\" after all this time. I believe I saw other posters who reminded some of the pickier people that it is still just a movie. Maybe some of the more esoteric points defy \"logic\", but a great many religious matters accepted \"on faith\" fail to pass the smell test. If you\\'re going to accept whatever faith you subscribe to you can certainly accept a movie. Is it just me or has anyone else noticed the Aja-Yee Dagger is the same possessed knife Lamonte Cranston had so much trouble gaining control of in \"The Shadow\". No mention of it in the trivia section for either movie here (IMDB), but I would bet a dollar to a donut it\\'s the same prop.', shape=(), dtype=string)\n\nKodtargets[1]\n\ntf.Tensor(1, shape=(), dtype=int32)\n\n\nWszystko gotowe. Teraz spróbujmy nauczyć się czegoś z tych danych.\n\n7.5.2 Modelowanie za pomocą bag-of-words\nNajprostszym sposobem zakodowania fragmentu tekstu do przetwarzania przez model uczenia maszynowego jest odrzucenie kolejności i potraktowanie go jako zbioru (“worka”) tokenów. Najczęściej stosuje się tu unigramy, czyli podział na pojedyncze słowa lub znaki. Zdarza się jednak, gdy chcemy zachować część informacji o kolejności słów, że stosuje się N-gramy.\nDla przywoływanego już przykładu “the cat sat on the mat” podział na unigramy jest następujący:\n\nKodc(\"cat\", \"mat\", \"on\", \"sat\", \"the\")\n\n[1] \"cat\" \"mat\" \"on\"  \"sat\" \"the\"\n\n\nGłówną zaletą tego kodowania jest możliwość reprezentowania całego tekstu jako pojedynczego wektora, gdzie każdy element jest wskaźnikiem obecności danego słowa. Na przykład, korzystając z kodowania binarnego (ang. multi-hot encoding), zakodujemy tekst jako wektor mający tyle współrzędnych, ile jest słów w naszym słowniku, z 0 niemal wszędzie i kilkoma 1 dla wymiarów, które kodują słowa obecne w tekście.\nNajpierw przetwórzmy nasze surowe zbiory danych tekstowych za pomocą warstwy layer_text_vectorization(), tak aby uzyskać wielokrotnie zakodowane binarnie wektory słów. Nasza warstwa będzie patrzeć tylko na pojedyncze słowa (czyli unigramy).\n\nKodtext_vectorization &lt;-\n  layer_text_vectorization(max_tokens = 20000,\n                           output_mode = \"multi_hot\")\n\ntext_only_train_ds &lt;- train_ds %&gt;%\n  dataset_map(function(x, y) x)\n\nadapt(text_vectorization, text_only_train_ds)\n\nbinary_1gram_train_ds &lt;- train_ds %&gt;%\n  dataset_map( ~ list(text_vectorization(.x), .y),\n               num_parallel_calls = 4)\nbinary_1gram_val_ds &lt;- val_ds %&gt;%\n  dataset_map( ~ list(text_vectorization(.x), .y),\n               num_parallel_calls = 4)\nbinary_1gram_test_ds &lt;- test_ds %&gt;%\n  dataset_map( ~ list(text_vectorization(.x), .y),\n               num_parallel_calls = 4)\n\n\nSprawdźmy dane wyjściowe dla jednego z tych zestawów.\n\nKodc(inputs, targets) %&lt;-% iter_next(as_iterator(binary_1gram_train_ds))\nstr(inputs)\n\n&lt;tf.Tensor: shape=(32, 20000), dtype=float32, numpy=…&gt;\n\nKodstr(targets)\n\n&lt;tf.Tensor: shape=(32), dtype=int32, numpy=…&gt;\n\nKodinputs[1, ]\n\ntf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000), dtype=float32)\n\nKodtargets[1]\n\ntf.Tensor(1, shape=(), dtype=int32)\n\n\nNapiszmy funkcję budowania modelu wielokrotnego użytku, której będziemy używać we wszystkich naszych eksperymentach w tej sekcji.\n\nKodget_model &lt;- function(max_tokens = 20000, hidden_dim = 16) {\n  inputs &lt;- layer_input(shape = c(max_tokens))\n  outputs &lt;- inputs %&gt;%\n    layer_dense(hidden_dim, activation = \"relu\") %&gt;%\n    layer_dropout(0.5) %&gt;%\n    layer_dense(1, activation = \"sigmoid\")\n  model &lt;- keras_model(inputs, outputs)\n  model %&gt;% compile(optimizer = \"rmsprop\",\n                    loss = \"binary_crossentropy\",\n                    metrics = \"accuracy\")\n  model\n}\n\n\nNa koniec wytrenujmy i przetestujmy nasz model.\n\nKodmodel &lt;- get_model()\nmodel\ncallbacks = list(\n  callback_model_checkpoint(\"models/binary_1gram\", save_best_only = TRUE)\n)\n\nmodel %&gt;% fit(\n  dataset_cache(binary_1gram_train_ds),\n  validation_data = dataset_cache(binary_1gram_val_ds),\n  epochs = 10,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/binary_1gram\")\ncat(sprintf(\n  \"Test acc: %.3f\\n\", evaluate(model, binary_1gram_test_ds)[\"accuracy\"]))\n\n782/782 - 3s - loss: 0.2856 - accuracy: 0.8868 - 3s/epoch - 3ms/step\nTest acc: 0.887\n\n\nDokładność modelu na zbiorze testowym jest na poziomie 88,7%: nieźle! Należy zauważyć, że w tym przypadku, ponieważ zbiór danych jest zrównoważonym dwuklasowym zbiorem danych klasyfikacyjnych (jest tyle samo próbek pozytywnych, co negatywnych), “naiwny poziom bazowy”, który moglibyśmy osiągnąć bez trenowania rzeczywistego modelu, wynosiłby tylko 50%. Tymczasem najlepszy wynik, jaki można osiągnąć na tym zbiorze danych bez wykorzystywania danych zewnętrznych, wynosi około 95% dokładności testu.\nOczywiście odrzucenie kolejności słów jest bardzo redukcyjne, ponieważ nawet pojęcia elementarne można wyrazić za pomocą wielu słów: termin “Stany Zjednoczone” przekazuje pojęcie, które jest zupełnie odmienne od znaczenia słów “stany” i “zjednoczone” rozpatrywanych osobno. Z tego powodu zwykle kończy się to ponownym wprowadzeniem informacji o lokalnym porządku do reprezentacji worka słów, patrząc na N-gramy, a nie na pojedyncze słowa (najczęściej bigramy).\nWarstwę layer_text_vectorization() można skonfigurować tak, aby zwracała dowolne N-gramy: bigramy, trygramy itd. Wystarczy przekazać argument ngrams = N, jak na poniższym listingu.\n\nKodtext_vectorization &lt;-\n  layer_text_vectorization(ngrams = 2,\n                           max_tokens = 20000,\n                           output_mode = \"multi_hot\")\n\n\nPrzetestujmy nasz model oparty na bigramach.\n\nKodadapt(text_vectorization, text_only_train_ds)\n\ndataset_vectorize &lt;- function(dataset) {\n  dataset %&gt;%\n    dataset_map(~ list(text_vectorization(.x), .y),\n                num_parallel_calls = 4)\n}\n\nbinary_2gram_train_ds &lt;- train_ds %&gt;% dataset_vectorize()\nbinary_2gram_val_ds &lt;- val_ds %&gt;% dataset_vectorize()\nbinary_2gram_test_ds &lt;- test_ds %&gt;% dataset_vectorize()\n\n\n\nKodmodel &lt;- get_model()\nmodel\ncallbacks = list(callback_model_checkpoint(\"models/binary_2gram\",\n                                           save_best_only = TRUE))\n\nmodel %&gt;% fit(\n  dataset_cache(binary_2gram_train_ds),\n  validation_data = dataset_cache(binary_2gram_val_ds),\n  epochs = 10,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/binary_2gram\")\nevaluate(model, binary_2gram_test_ds)[\"accuracy\"] %&gt;%\n  sprintf(\"Test acc: %.3f\\n\", .) %&gt;% cat()\n\n782/782 - 3s - loss: 0.2556 - accuracy: 0.9012 - 3s/epoch - 4ms/step\nTest acc: 0.901\n\n\nUzyskujemy teraz 90,1% dokładności testu, co stanowi znaczną poprawę! Okazuje się, że lokalna kolejność jest dość ważna.\nMożna również dodać nieco więcej informacji do tej reprezentacji, licząc, ile razy występuje każde słowo lub N-gram, czyli biorąc histogram słów w tekście:\n\nKodc(\"the\" = 2, \"the cat\" = 1, \"cat\" = 1, \"cat sat\" = 1, \"sat\" = 1,\n  \"sat on\" = 1, \"on\" = 1, \"on the\" = 1, \"the mat\" = 1, \"mat\" = 1)\n\n    the the cat     cat cat sat     sat  sat on      on  on the the mat     mat \n      2       1       1       1       1       1       1       1       1       1 \n\n\nJeśli przeprowadzamy klasyfikację tekstu, wiedza o tym, ile razy słowo występuje w próbce, ma kluczowe znaczenie: każda wystarczająco długa recenzja filmu może zawierać słowo “okropny” niezależnie od nastroju, ale recenzja zawierająca wiele wystąpień słowa “okropny” jest prawdopodobnie negatywna. Oto jak policzyć wystąpienia bigramów za pomocą layer_text_vectorization():\n\nKodtext_vectorization &lt;-\n  layer_text_vectorization(ngrams = 2,\n                           max_tokens = 20000,\n                           output_mode = \"count\")\n\n\nOczywiście niektóre słowa będą występować częściej niż inne, niezależnie od tego, o czym jest tekst. Słowa “the”, “a”, “is” i “are” zawsze będą dominować w histogramach liczby słów, zagłuszając inne słowa, mimo że są w zasadzie bezużytecznymi cechami w kontekście klasyfikacji. Jak możemy temu zaradzić?\nPoprzez normalizację. Moglibyśmy po prostu znormalizować liczbę słów, odejmując średnią i dzieląc przez wariancję (obliczoną dla całego zbioru danych szkoleniowych). To miałoby sens. Z wyjątkiem tego, że większość wektoryzowanych zdań składa się prawie wyłącznie z zer (nasz poprzedni przykład zawiera 12 niezerowych wpisów i 19 988 zerowych wpisów), co jest właściwością zwaną “rzadkością”. Jest to świetna właściwość, ponieważ znacznie zmniejsza obciążenie obliczeniowe i zmniejsza ryzyko przeuczenia. Gdybyśmy odjęli średnią od każdej cechy, “zniszczylibyśmy” rzadkość. W związku z tym każdy schemat normalizacji, którego używamy, powinien być oparty wyłącznie na dzieleniu. Czego zatem powinniśmy użyć jako mianownika? Najlepszą praktyką jest zastosowanie czegoś, co nazywa się normalizacją TF-IDF - co oznacza “częstotliwość terminów, odwrotność częstotliwości dokumentów”.\n\n\n\n\n\n\nAdnotacja\n\n\n\nIm częściej dany termin pojawia się w dokumencie, tym ważniejszy jest on dla zrozumienia jego treści. Jednocześnie częstotliwość, z jaką termin pojawia się we wszystkich dokumentach w zbiorze danych, również ma znaczenie: terminy, które pojawiają się w prawie każdym dokumencie (takie jak “the” lub “a”) nie są szczególnie pouczające, podczas gdy terminy, które pojawiają się tylko w niewielkim podzbiorze wszystkich tekstów (takich jak “Herzog”) są bardzo charakterystyczne, a zatem ważne. TF-IDF to metryka, która łączy te dwie koncepcje. Waży dany termin, biorąc “częstotliwość terminu”, ile razy termin pojawia się w bieżącym dokumencie i dzieląc go przez miarę “częstotliwości dokumentu”, która szacuje, jak często termin pojawia się w całym zbiorze danych. Można to obliczyć w następujący sposób:\n\nKodtf_idf &lt;- function(term, document, dataset) {\n  term_freq &lt;- sum(document == term)\n  doc_freqs &lt;- sapply(dataset, function(doc) sum(doc == term))\n  doc_freq &lt;- log(1 + sum(doc_freqs))\n  term_freq / doc_freq\n}\n\n\nTF-IDF jest tak powszechny, że jest wbudowany w funkcję layer_text_vectorization(). Wszystko, co musimy zrobić, aby zacząć go używać, to przełączyć argument output_mode na “tf_idf”.\n\nKodtext_vectorization &lt;-\n  layer_text_vectorization(ngrams = 2,\n                           max_tokens = 20000,\n                           output_mode = \"tf_idf\")\n\n\n\n\nPrzetrenujmy model z tym schematem.\n\nKod# Wysyłamy tę operację tylko do CPU, ponieważ wykorzystuje ona operacje, których urządzenie GPU jeszcze nie obsługuje.\nwith(tf$device(\"CPU\"), {\n  adapt(text_vectorization, text_only_train_ds)\n})\n\ntfidf_2gram_train_ds &lt;- train_ds %&gt;% dataset_vectorize()\ntfidf_2gram_val_ds &lt;- val_ds %&gt;% dataset_vectorize()\ntfidf_2gram_test_ds &lt;- test_ds %&gt;% dataset_vectorize()\n\n\n\nKodmodel &lt;- get_model()\nmodel\ncallbacks &lt;- list(callback_model_checkpoint(\"models/tfidf_2gram\",\n                                            save_best_only = TRUE))\nmodel %&gt;% fit(\n  dataset_cache(tfidf_2gram_train_ds),\n  validation_data = dataset_cache(tfidf_2gram_val_ds),\n  epochs = 10,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/tfidf_2gram\")\nevaluate(model, tfidf_2gram_test_ds)[\"accuracy\"] %&gt;%\n  sprintf(\"Test acc: %.3f\", .) %&gt;% cat(\"\\n\")\n\n782/782 - 3s - loss: 0.2973 - accuracy: 0.8940 - 3s/epoch - 4ms/step\nTest acc: 0.894 \n\n\nDaje nam to 89,4% dokładności testowej w zadaniu klasyfikacji IMDB: nie wydaje się to być szczególnie pomocne w tym przypadku. Jednak w przypadku wielu zestawów danych do klasyfikacji tekstu typowy byłby jednoprocentowy wzrost przy użyciu TF-IDF w porównaniu do zwykłego kodowania binarnego.\n\n\n\n\n\n\nAdnotacja\n\n\n\nW poprzednich przykładach przeprowadziliśmy standaryzację, podział i indeksowanie tekstu w ramach potoku TF Dataset. Jeśli jednak chcemy wyeksportować samodzielny model niezależny od tego potoku, powinniśmy upewnić się, że zawiera on własne wstępne przetwarzanie tekstu (w przeciwnym razie konieczne będzie ponowne wdrożenie w środowisku produkcyjnym, co może być trudne lub może prowadzić do subtelnych rozbieżności między danymi szkoleniowymi a danymi produkcyjnymi). Na szczęście jest to łatwe.\nWystarczy utworzyć nowy model, który ponownie wykorzysta warstwę text_vectorization i doda do niej właśnie wytrenowany model:\n\nKodinputs &lt;- layer_input(shape = c(1), dtype = \"string\")\noutputs &lt;- inputs %&gt;%\n  text_vectorization() %&gt;%\n  model()\ninference_model &lt;- keras_model(inputs, outputs)\n\n\nWynikowy model może przetwarzać partie nieprzetworzonych ciągów znaków:\n\nKodraw_text_data &lt;- \"That was an excellent movie, I loved it.\" %&gt;%\n  as_tensor(shape = c(-1, 1))\n\npredictions &lt;- inference_model(raw_text_data)\nstr(predictions)\n\n&lt;tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.8422111]], dtype=float32)&gt;\n\nKodcat(sprintf(\"%.2f percent positive\\n\",\n            as.numeric(predictions) * 100))\n\n84.22 percent positive\n\n\n\n\n\n7.5.3 Modelowanie za pomocą sekwencji\nTe kilka ostatnich przykładów wyraźnie pokazuje, że kolejność słów ma znaczenie: ręczna inżynieria funkcji opartych na kolejności, takich jak bigramy, zapewnia niewielki wzrost dokładności. Zapamiętajmy - historia głębokiego uczenia polega na odejściu od ręcznej inżynierii cech, w kierunku umożliwienia modelom uczenia się własnych cech na podstawie samej ekspozycji na dane. Co by było, gdybyśmy zamiast ręcznie tworzyć funkcje oparte na kolejności, wystawili model na surowe sekwencje słów i pozwolili mu samodzielnie wymyślić takie funkcje? O to właśnie chodzi w modelach opartych na sekwencji.\nAby zaimplementować model sekwencji, należy zacząć od reprezentowania próbek wejściowych jako sekwencji indeksów liczb całkowitych (jedna liczba całkowita oznacza jedno słowo). Następnie mapujemy każdą liczbę całkowitą na wektor, aby uzyskać sekwencje wektorowe. Wreszcie, te sekwencje wektorów należy wprowadzić do stosu warstw, które mogą korelować krzyżowo cechy z sąsiednich wektorów, takich jak konwolucje 1D, RNN lub Transformery.\nPrzez pewien czas, około 2016-2017 roku, dwukierunkowe RNN (w szczególności dwukierunkowe LSTM) były uważane za najnowocześniejsze rozwiązanie do modelowania sekwencji. Ponieważ jesteśmy już zaznajomieni z tą architekturą, to właśnie jej użyjemy w naszych pierwszych przykładach modeli sekwencji. Jednak obecnie modelowanie sekwencji jest prawie zawsze wykonywane za pomocą transformatorów, które omówimy wkrótce.\nWypróbujmy pierwszy model sekwencji w praktyce. Najpierw przygotujmy zestawy danych zwracające sekwencje liczb całkowitych.\n\nKodmax_length &lt;- 600\nmax_tokens &lt;- 20000\n\ntext_vectorization &lt;- layer_text_vectorization(\n  max_tokens = max_tokens,\n  output_mode = \"int\",\n  output_sequence_length = max_length\n)\n\nadapt(text_vectorization, text_only_train_ds)\n\nint_train_ds &lt;- train_ds %&gt;% dataset_vectorize()\nint_val_ds &lt;- val_ds %&gt;% dataset_vectorize()\nint_test_ds &lt;- test_ds %&gt;% dataset_vectorize()\n\n\nNastępnie stwórzmy model. Najprostszym sposobem konwersji naszych sekwencji liczb całkowitych na sekwencje wektorowe jest zakodowanie liczb całkowitych metodą one-hot (każdy wymiar reprezentowałby jeden możliwy termin w słowniku). Na tych wektorach one-hot dodamy prostą dwukierunkową sieć LSTM.\n\nKodinputs  &lt;- layer_input(shape(NULL), dtype = \"int64\")\nembedded &lt;- tf$one_hot(inputs, depth = as.integer(max_tokens))\noutputs &lt;- embedded %&gt;%\n  bidirectional(layer_lstm(units = 32)) %&gt;%\n  layer_dropout(.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel &lt;- keras_model(inputs, outputs)\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"binary_crossentropy\",\n                  metrics = \"accuracy\")\nmodel\n\nModel: \"model_1\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_2 (InputLayer)               [(None, None)]                  0           \n tf.one_hot (TFOpLambda)            (None, None, 20000)             0           \n bidirectional (Bidirectional)      (None, 64)                      5128448     \n dropout (Dropout)                  (None, 64)                      0           \n dense (Dense)                      (None, 1)                       65          \n================================================================================\nTotal params: 5128513 (19.56 MB)\nTrainable params: 5128513 (19.56 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks &lt;- list(\n  callback_model_checkpoint(\"models/one_hot_bidir_lstm.keras\",\n                            save_best_only = TRUE))\n\n\nPodczas uczenia modelu można poczynić dwie obserwacje. Po pierwsze, model ten trenuje się bardzo wolno, szczególnie w porównaniu do modeli z poprzedniego podrozdziału. Wynika to z faktu, że nasze dane wejściowe są dość duże: każda próbka wejściowa jest zakodowana jako macierz o rozmiarze (600, 20000) (600 słów na próbkę, 20 000 możliwych słów). To 12 000 000 wartości dla pojedynczej recenzji filmu. Po drugie, model osiąga tylko 84,7% dokładności testowej, więc nie radzi sobie tak dobrze, jak nasz najlepszy model.\n\nKod# aby nie przekroczyć zasobów pamięci GPU zmnieszamy wielkość paczek do 16\nint_train_ds_smaller &lt;- int_train_ds %&gt;%\n  dataset_unbatch() %&gt;%\n  dataset_batch(16)\n\nmodel %&gt;% fit(int_train_ds_smaller, validation_data = int_val_ds,\n              epochs = 10, callbacks = callbacks)\n\n\n\nKod# predykcja trwa długo dlatego nie pozwalam na wywołanie tego chunk-a\nmodel &lt;- load_model_tf(\"models/one_hot_bidir_lstm.keras\")\nsprintf(\"Test acc: %.3f\", evaluate(model, int_test_ds)[\"accuracy\"])\n\n\nNajwyraźniej użycie kodowania one-hot do przekształcenia słów w wektory, nie było dobrym pomysłem. Jest lepszy sposób - osadzanie słów.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#osadzenia",
    "href": "llm.html#osadzenia",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.6 Osadzenia",
    "text": "7.6 Osadzenia\nKiedy kodujemy coś za pomocą kodowania one-hot, podejmujemy decyzję o inżynierii cech. Wprowadzamy do naszego modelu fundamentalne założenie dotyczące struktury przestrzeni cech. Założeniem tym jest to, że różne tokeny, które kodujemy, są od siebie niezależne: w rzeczywistości wektory one-hot są względem siebie ortogonalne. W przypadku słów założenie to jest oczywiście błędne. Słowa tworzą ustrukturyzowaną przestrzeń: dzielą się ze sobą informacjami. Słowa “movie” i “film” są wymienne w większości zdań, więc wektor reprezentujący “movie” nie powinien być ortogonalny do wektora reprezentującego “film” - powinny być tym samym wektorem lub bardzo zbliżonym.\nPodchodząc do sprawy niecobardziej abstrakcyjnie, geometryczna relacja między dwoma wektorami słów powinna odzwierciedlać semantyczną relację między tymi słowami. Na przykład, w rozsądnej przestrzeni wektorów słów można oczekiwać, że synonimy będą osadzone w podobnych wektorach słów, a ogólnie rzecz biorąc, można oczekiwać, że odległość geometryczna (taka jak odległość cosinusowa lub odległość L2) między dowolnymi dwoma wektorami słów będzie odnosić się do “odległości semantycznej” między powiązanymi słowami. Słowa, które oznaczają różne pojęcia, powinny znajdować się daleko od siebie, podczas gdy podobne słowa powinny znajdować się bliżej.\nOsadzenia słów to wektorowe reprezentacje słów, które osiągają ten cel: mapują ludzki język na ustrukturyzowaną przestrzeń geometryczną. Podczas gdy wektory uzyskane w wyniku kodowania one-hot są binarne, rzadkie (w większości składają się z zer) i wielowymiarowe (taka sama wymiarowość jak liczba słów w słowniku), osadzenia słów są niskowymiarowymi wektorami typu float (tj. gęstymi wektorami, w przeciwieństwie do rzadkich wektorów) - patrz Rys. 7.2. W przypadku bardzo dużych słowników często spotyka się osadzenia słów, które są 256-wymiarowe, 512-wymiarowe lub 1024-wymiarowe. Z drugiej strony, kodowanie słów metodą one-hot zazwyczaj prowadzi do wektorów, które mają 20 000 wymiarów lub więcej (słownik składający się z 20,000 tokenów). Tak więc osadzanie słów zawiera więcej informacji w znacznie mniejszej liczbie wymiarów.\n\n\n\n\n\nRys. 7.2\n\n\nOprócz tego, że są gęstymi reprezentacjami, osadzenia słów są również reprezentacjami strukturalnymi, a ich struktura jest uczona na podstawie danych. Podobne słowa są osadzone w bliskich lokalizacjach, a ponadto określone kierunki w przestrzeni osadzania mają znaczenie. Aby to wyjaśnić, spójrzmy na konkretny przykład.\n\n\n\n\n\nRys. 7.3\n\n\nNa Rys. 7.3 cztery słowa są osadzone na płaszczyźnie 2D: cat, dog, wolf i tiger. Dzięki reprezentacjom wektorowym, które tutaj wybraliśmy, niektóre relacje semantyczne między tymi słowami można zakodować jako transformacje geometryczne. Na przykład ten sam wektor pozwala nam przejść od kota do tygrysa i od psa do wilka: wektor ten można interpretować jako wektor “od zwierzęcia domowego do dzikiego zwierzęcia”. Podobnie, inny wektor pozwala nam przejść od psa do kota i od wilka do tygrysa, co można interpretować jako wektor “od psa do kota”.\nW rzeczywistych przestrzeniach osadzania słów, typowymi przykładami znaczących transformacji geometrycznych są wektory “płci” i wektory “liczby mnogiej”. Na przykład, dodając wektor “żeński” do wektora “król”, otrzymujemy wektor “królowa”. Dodając wektor “liczby mnogiej”, otrzymujemy “królów”. Przestrzenie osadzania słów zazwyczaj zawierają tysiące takich interpretowalnych i potencjalnie użytecznych wektorów.\nJeszcze inny przykład osadzenia słów można dostrzec na Rys. 7.4\n\n\n\n\n\nRys. 7.4: Przykład osadzenia słów\n\n\nIstnieją dwa sposoby na uzyskanie osadzenia słów:\n\nUczenie się embeddingów wspólnie z głównym zadaniem (takim jak klasyfikacja dokumentów lub przewidywanie sentymentu). W tej konfiguracji zaczynamy od losowych wektorów słów, a następnie uczymy się wektorów słów w taki sam sposób, w jaki uczymy się wag sieci neuronowej.\nWczytanie do modelu osadzenia słów, które zostały wstępnie wytrenowane przy użyciu innego zadania uczenia maszynowego niż to, które próbujemy rozwiązać. Są to tzw. wstępnie wytrenowane osadzenia słów.\n\nPrzyjrzyjmy się obu tym metodom.\n\n7.6.1 Osadzenie połączone z siecią\nCzy istnieje jakaś idealna przestrzeń słowotwórcza, która doskonale odwzorowywałaby ludzki język i mogłaby być wykorzystana do każdego zadania związanego z przetwarzaniem języka naturalnego? Możliwe, ale nie udało nam się jeszcze takiej znaleźć. Nie ma też czegoś takiego jak język ludzki - istnieje wiele różnych języków i nie są one izomorficzne, ponieważ język jest odzwierciedleniem konkretnej kultury i konkretnego kontekstu. Bardziej pragmatycznie możemy stwierdzić, że to co czyni przestrzeń osadzania słów dobrą, zależy w dużej mierze od zadania: idealna przestrzeń osadzania słów dla anglojęzycznego modelu analizy sentymentów w recenzji filmowej może wyglądać inaczej niż idealna przestrzeń osadzania dla anglojęzycznego modelu klasyfikacji dokumentów prawnych, ponieważ znaczenie pewnych relacji semantycznych różni się w zależności od zadania.\nDlatego rozsądne jest uczenie się nowej przestrzeni osadzania z każdym nowym zadaniem. Na szczęście wsteczna propagacja to ułatwia, a pakiet keras czyni to jeszcze łatwiejszym. Chodzi o uczenie wag warstwy za pomocą layer_embedding.\n\nKodembedding_layer &lt;- layer_embedding(input_dim = max_tokens, output_dim = 256)\n\n\nFunkcja layer_embedding() jest najlepiej rozumiana jako słownik, który mapuje indeksy liczb całkowitych (które oznaczają określone słowa) na wektory gęste. Przyjmuje liczby całkowite jako dane wejściowe, wyszukuje tych liczb całkowitych w wewnętrznym słowniku i zwraca powiązane wektory.\nWarstwa osadzania przyjmuje jako dane wejściowe tensor liczb całkowitych, o kształcie (batch_size, sequence_length), gdzie każdy wpis jest sekwencją liczb całkowitych. Następnie warstwa zwraca tensor zmiennoprzecinkowy 3D o kształcie (batch_size, sequence_length, embedding_dimensionality).\nPo utworzeniu instancji layer_embedding(), jej wagi (wewnętrzny słownik wektorów tokenów) są początkowo losowe, tak jak w przypadku każdej innej warstwy. Podczas uczenia te wektory słów są stopniowo dostosowywane za pomocą wstecznej propagacji, strukturyzując przestrzeń w coś, co może wykorzystać dalszy model. Po pełnym wytrenowaniu, przestrzeń osadzania będzie wykazywać dużą strukturę - rodzaj struktury wyspecjalizowanej dla konkretnego problemu, dla którego trenujemy nasz model.\nZbudujmy model zawierający funkcję layer_embedding() i przetestujmy go w naszym zadaniu.\n\nKodinputs &lt;- layer_input(shape(NA), dtype = \"int64\")\nembedded &lt;- inputs %&gt;%\n  layer_embedding(input_dim = max_tokens, output_dim = 256)\noutputs &lt;- embedded %&gt;%\n  bidirectional(layer_lstm(units = 32)) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\nmodel &lt;- keras_model(inputs, outputs)\nmodel %&gt;%\n  compile(optimizer = \"rmsprop\",\n          loss = \"binary_crossentropy\",\n          metrics = \"accuracy\")\nmodel\n\nModel: \"model_2\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_3 (InputLayer)               [(None, None)]                  0           \n embedding_1 (Embedding)            (None, None, 256)               5120000     \n bidirectional_1 (Bidirectional)    (None, 64)                      73984       \n dropout_1 (Dropout)                (None, 64)                      0           \n dense_1 (Dense)                    (None, 1)                       65          \n================================================================================\nTotal params: 5194049 (19.81 MB)\nTrainable params: 5194049 (19.81 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks = list(callback_model_checkpoint(\"models/embeddings_bidir_lstm.keras\",\n                                           save_best_only = TRUE))\n\n\n\nKodmodel %&gt;%\n  fit(int_train_ds,\n      validation_data = int_val_ds,\n      epochs = 10,\n      callbacks = callbacks)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/embeddings_bidir_lstm.keras\")\nevaluate(model, int_test_ds)[\"accuracy\"] %&gt;%\n  sprintf(\"Test acc: %.3f\\n\", .) %&gt;% cat(\"\\n\")\n\n782/782 - 26s - loss: 0.4049 - accuracy: 0.8601 - 26s/epoch - 33ms/step\nTest acc: 0.860\n \n\n\nModel ten uczy się znacznie szybciej niż model one-hot (ponieważ LSTM musi przetwarzać tylko 256-wymiarowe wektory zamiast 20 000-wymiarowych), a jego dokładność na zbiorze testowym jest porównywalna (86,0%). Jednak wciąż jesteśmy daleko od wyników naszego podstawowego modelu bigramowego. Częściowo wynika to z faktu, że model ten analizuje nieco mniej danych: model bigramowy przetwarza pełne recenzje, podczas gdy nasz model sekwencji obcina sekwencje po 600 słowach.\nJedną z rzeczy, która nieco obniża wydajność modelu, jest to, że nasze sekwencje wejściowe są pełne zer. Wynika to z naszego użycia opcji output_sequence_length = max_length w layer_text_vectorization() (z max_length równą 600) - zdania dłuższe niż 600 tokenów są obcinane do długości 600 tokenów, a zdania krótsze niż 600 tokenów są wypełniane zerami na końcu, aby można je było połączyć z innymi sekwencjami w celu utworzenia ciągłych partii.\nUżywamy dwukierunkowej sieci RNN - dwie warstwy RNN działające równolegle, z których jedna przetwarza tokeny w ich naturalnej kolejności, a druga przetwarza te same tokeny w odwrotnej kolejności. RNN, która patrzy na tokeny w ich naturalnej kolejności, poświęci swoje ostatnie iteracje na “oglądaniu” tylko tych wektory, które kodują padding - prawdopodobnie przez kilkaset iteracji, jeśli oryginalne zdanie było krótkie. Informacje przechowywane w wewnętrznym stanie RNN będą stopniowo zanikać, gdy będą narażone na bezsensowne dane wejściowe.\nPotrzebujemy zatem jakiegoś sposobu, aby przekazać RNN, że powinna pominąć te iteracje. Jest do tego API - maskowanie. layer_embedding() jest w stanie wygenerować “maskę”, która odpowiada danym wejściowym. Maska ta jest tensorem jedynek i zer (lub TRUE/FALSE), o kształcie (batch_size, sequence_length), gdzie wpis mask[i, t] odpowiada, czy krok czasowy t próbki i powinien zostać pominięty, czy nie (krok czasowy zostanie pominięty, jeśli mask[i, t] ma wartość 0 lub FALSE, i przetworzony w przeciwnym razie).\nDomyślnie opcja ta nie jest aktywna - można ją włączyć, przekazując mask_zero = TRUE do funkcji layer_embedding(). Maskę można pobrać za pomocą metody compute_mask():\n\nKodembedding_layer &lt;- layer_embedding(input_dim = 10, output_dim = 256,\n                                   mask_zero = TRUE)\nsome_input &lt;- rbind(c(4, 3, 2, 1, 0, 0, 0),\n                    c(5, 4, 3, 2, 1, 0, 0),\n                    c(2, 1, 0, 0, 0, 0, 0))\nmask &lt;- embedding_layer$compute_mask(some_input)\nmask\n\ntf.Tensor(\n[[ True  True  True  True False False False]\n [ True  True  True  True  True False False]\n [ True  True False False False False False]], shape=(3, 7), dtype=bool)\n\n\nW praktyce prawie nigdy nie będziemy musieli ręcznie zarządzać maskami. Zamiast tego, keras automatycznie przekaże maskę do każdej warstwy, która jest w stanie ją przetworzyć (jako metadane dołączone do sekwencji, którą reprezentuje). Maska ta będzie używana przez warstwy RNN do pomijania zamaskowanych kroków. Jeśli model zwraca całą sekwencję, maska będzie również używana przez funkcję straty do pomijania zamaskowanych kroków w sekwencji wyjściowej. Spróbujmy przeformułować nasz model na model z włączoną funkcją maskowania.\n\nKodinputs &lt;- layer_input(c(NA), dtype = \"int64\")\nembedded &lt;- inputs %&gt;%\n  layer_embedding(input_dim = max_tokens,\n                  output_dim = 256,\n                  mask_zero = TRUE)\n\noutputs &lt;- embedded %&gt;%\n  bidirectional(layer_lstm(units = 32)) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel &lt;- keras_model(inputs, outputs)\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"binary_crossentropy\",\n                  metrics = \"accuracy\")\nmodel\n\nModel: \"model_3\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_4 (InputLayer)               [(None, None)]                  0           \n embedding_3 (Embedding)            (None, None, 256)               5120000     \n bidirectional_2 (Bidirectional)    (None, 64)                      73984       \n dropout_2 (Dropout)                (None, 64)                      0           \n dense_2 (Dense)                    (None, 1)                       65          \n================================================================================\nTotal params: 5194049 (19.81 MB)\nTrainable params: 5194049 (19.81 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks = list(\n  callback_model_checkpoint(\"models/embeddings_bidir_lstm_with_masking.keras\",\n                            save_best_only = TRUE)\n)\n\n\n\nKodmodel %&gt;% fit(\n  int_train_ds,\n  validation_data = int_val_ds,\n  epochs = 10,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/embeddings_bidir_lstm_with_masking.keras\")\ncat(sprintf(\"Test acc: %.3f\\n\",\n            evaluate(model, int_test_ds)[\"accuracy\"]))\n\n782/782 - 30s - loss: 2.4718 - accuracy: 0.5019 - 30s/epoch - 39ms/step\nTest acc: 0.502\n\n\nTym razem osiągnęliśmy 86,8% dokładności na zbiorze testowym - niewielka, ale zauważalna poprawa.\n\n7.6.2 Użycie wstępnie wytrenowanego osadzenia\nCzasami dostępnych jest tak mało danych szkoleniowych, aby użyć ich do nauczenia się odpowiedniego, specyficznego dla danego zadania osadzenia słownictwa. W takich przypadkach, zamiast uczyć osadzania słów razem z głównym problemem, który chcemy rozwiązać, możemy załadować wektory osadzania ze wstępnie obliczonej przestrzeni osadzania, o której wiemy, że jest wysoce ustrukturyzowana i wykazuje użyteczne właściwości - takie, które wychwytują ogólne aspekty struktury języka. Uzasadnienie używania wstępnie wytrenowanych osadzeń słów w przetwarzaniu języka naturalnego jest takie samo, jak w przypadku używania wstępnie wytrenowanych sieci konwolucyjnych w klasyfikacji obrazów: nie mamy wystarczającej ilości danych, aby samodzielnie nauczyć się odpowiednich funkcji, ale spodziewamy się, że funkcje, których potrzebujemy, są dość ogólne - to znaczy, że mają wspólne cechy wizualne lub cechy semantyczne. W takim przypadku sensowne jest ponowne wykorzystanie cech wyuczonych dla innego problemu.\nTakie osadzenia słów są zazwyczaj tworzone przy użyciu statystyk występowania słów (obserwacje dotyczące tego, jakie słowa współwystępują w zdaniach lub dokumentach), przy użyciu różnych technik, z których niektóre obejmują sieci neuronowe, a inne nie. Idea gęstej, niskowymiarowej przestrzeni osadzania słów, obliczanej w sposób nienadzorowany, została początkowo zbadana przez Bengio i in. na początku XXI wieku (Bengio i in., b.d.), ale zaczęła się rozwijać w badaniach i zastosowaniach przemysłowych dopiero po wydaniu jednego z najbardziej znanych i udanych schematów osadzania słów: algorytmu Word2Vec (https://code.google.com/archive/p/word2vec), opracowanego przez Tomasa Mikolova w Google w 2013 roku. Osadzenie Word2Vec wychwytuje specyficzne właściwości semantyczne, takie jak płeć.\nMożna pobrać różne wstępnie wytrenowane bazy danych osadzania słów i użyć ich w funkcji Keras layer_embedding(). Word2Vec jest jedną z nich. Inną popularną wersją osadzenia słów jest Global Vectors for Word Representation (GloVe, https://nlp.stanford.edu/projects/glove), która została opracowana przez naukowców ze Stanford w 2014 roku. Ta technika osadzania opiera się na faktoryzacji macierzy statystyk współwystępowania słów. Jej twórcy udostępnili wstępnie obliczone osadzenia dla milionów angielskich tokenów, uzyskanych z Wikipedii i danych Common Crawl5.\n5 Common Crawl to non-profit organizacja udostępniająca darmowe, ogromne archiwa danych internetowych do wykorzystania w badaniach, analizie i projektach z zakresu technologii informatycznych.Przyjrzyjmy się, jak można rozpocząć korzystanie z GloVe embeddings w modelu Keras. Ta sama metoda jest odpowiednia dla osadzeń Word2Vec lub dowolnej innej bazy danych osadzeń słów. Zaczniemy od pobrania plików GloVe i przeanalizowania ich. Następnie załadujemy wektory słów do warstwy Keras layer_embedding(), której użyjemy do zbudowania nowego modelu.\nNajpierw pobierzmy osadzenia słów GloVe wstępnie obliczone na zbiorze danych angielskiej Wikipedii z 2014 roku. Jest to plik zip o rozmiarze 822 MB zawierający 100-wymiarowe wektory osadzania dla 400 000 słów (lub tokenów niebędących słowami):\n\nKoddownload.file(\"http://nlp.stanford.edu/data/glove.6B.zip\",\n              destfile = \"glove.6B.zip\")\nzip::unzip(\"glove.6B.zip\")\n\n\nPrzeanalizujmy rozpakowany plik (plik .txt), aby utworzyć indeks, który mapuje słowa (jako ciągi) na ich reprezentację wektorową.\n\nKodpath_to_glove_file &lt;- \"~/Downloads/glove.6B/glove.6B.100d.txt\"\nembedding_dim &lt;- 100\n\ndf &lt;- readr::read_table(\n  path_to_glove_file,\n  col_names = FALSE,\n  col_types = paste0(\"c\", strrep(\"n\", 100))\n)\nembeddings_index &lt;- as.matrix(df[, -1])\nrownames(embeddings_index) &lt;- df[[1]]\ncolnames(embeddings_index) &lt;- NULL\nrm(df)\n\n\nOto jak wygląda embedding_matrix:\n\nKodstr(embeddings_index)\n\n num [1:400000, 1:100] -0.0382 -0.1077 -0.3398 -0.1529 -0.1897 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:400000] \"the\" \",\" \".\" \"of\" ...\n  ..$ : NULL\n\n\nNastępnie zbudujmy macierz osadzania, którą można załadować do funkcji layer_embedding(), Musi to być macierz o kształcie (max_words, embedding_dim), gdzie każdy wpis i zawiera wektor wymiaru embedding_dim dla słowa o indeksie i w indeksie słowa referencyjnego (zbudowanym podczas tokenizacji).\n\nKodvocabulary &lt;- text_vectorization %&gt;% get_vocabulary()\nstr(vocabulary)\n\n chr [1:20000] \"\" \"[UNK]\" \"the\" \"a\" \"and\" \"of\" \"to\" \"is\" \"in\" \"it\" \"i\" ...\n\nKodtokens &lt;- head(vocabulary[-1], max_tokens)\n\ni &lt;- match(vocabulary, rownames(embeddings_index),\n           nomatch = 0)\n\nembedding_matrix &lt;- array(0, dim = c(max_tokens, embedding_dim))\nembedding_matrix[i != 0, ] &lt;- embeddings_index[i, ]\nstr(embedding_matrix)\n\n num [1:20000, 1:100] 0 0 -0.0382 -0.2709 -0.072 ...\n\n\nNa koniec użyjemy funkcji initializer_constant(), aby załadować wstępnie wytrenowane osadzenia do layer_embedding(). Aby nie zakłócać wstępnie wytrenowanych reprezentacji podczas szkolenia, zamrażamy warstwę za pomocą trainable = FALSE:\n\nKodembedding_layer &lt;- layer_embedding(\n  input_dim = max_tokens,\n  output_dim = embedding_dim,\n  embeddings_initializer = initializer_constant(embedding_matrix),\n  trainable = FALSE,\n  mask_zero = TRUE\n)\n\n\nJesteśmy teraz gotowi do trenowania nowego modelu - identycznego z naszym poprzednim modelem, ale wykorzystującego 100-wymiarowe wstępnie wytrenowane osadzenia GloVe zamiast 128-wymiarowych wyuczonych osadzeń.\n\nKodinputs &lt;- layer_input(shape(NA), dtype=\"int64\")\nembedded &lt;- embedding_layer(inputs)\noutputs &lt;- embedded %&gt;%\n  bidirectional(layer_lstm(units = 32)) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\nmodel &lt;- keras_model(inputs, outputs)\n\nmodel %&gt;% compile(optimizer = 'rmsprop',\n                  loss = \"binary_crossentropy\",\n                  metrics = \"accuracy\")\nmodel\n\nModel: \"model_4\"\n________________________________________________________________________________\n Layer (type)                  Output Shape               Param #    Trainable  \n================================================================================\n input_5 (InputLayer)          [(None, None)]             0          Y          \n embedding_4 (Embedding)       (None, None, 100)          2000000    N          \n bidirectional_3 (Bidirection  (None, 64)                 34048      Y          \n al)                                                                            \n dropout_3 (Dropout)           (None, 64)                 0          Y          \n dense_3 (Dense)               (None, 1)                  65         Y          \n================================================================================\nTotal params: 2034113 (7.76 MB)\nTrainable params: 34113 (133.25 KB)\nNon-trainable params: 2000000 (7.63 MB)\n________________________________________________________________________________\n\nKodcallbacks &lt;- list(callback_model_checkpoint(\"models/glove_embeddings_sequence_model2.keras\",\n                            save_best_only = TRUE))\n\n\n\nKodmodel %&gt;%\n  fit(int_train_ds, validation_data = int_val_ds,\n      epochs = 10, callbacks = callbacks)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/glove_embeddings_sequence_model2.keras\")\ncat(sprintf(\n  \"Test acc: %.3f\\n\", evaluate(model, int_test_ds)[\"accuracy\"]))\n\n782/782 - 31s - loss: 0.5601 - accuracy: 0.7365 - 31s/epoch - 39ms/step\nTest acc: 0.736\n\n\nPrzekonaliśmy się, że w tym konkretnym zadaniu wstępnie wytrenowane osadzenia nie są zbyt pomocne (dopasowanie na zbiorze testowym na poziomie 73,6%), ponieważ zbiór danych zawiera wystarczającą liczbę próbek, aby można było nauczyć się od podstaw wystarczająco wyspecjalizowanej przestrzeni osadzania. Jednak wykorzystanie wstępnie wytrenowanych osadzeń może być bardzo pomocne, gdy pracujesz z mniejszym zbiorem danych.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#transformery",
    "href": "llm.html#transformery",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.7 Transformery",
    "text": "7.7 Transformery\nPocząwszy od 2017 roku, nowa architektura modelu zaczęła wyprzedzać rekurencyjne sieci neuronowe w większości zadań przetwarzania języka naturalnego. Jest nią transformer lub transformator (ang. transformer). Transformery zostały wprowadzone w przełomowym artykule “Attention Is All You Need” autorstwa Vaswani i in. (b.d.). Sedno artykułu znajduje się w tytule: jak się okazało, prosty mechanizm zwany “uwagą neuronową” (ang. neural attention) można wykorzystać do zbudowania potężnych modeli sekwencji, które nie zawierają żadnych warstw rekurencyjnych ani warstw splotowych.\nOdkrycie to zapoczątkowało rewolucję w przetwarzaniu języka naturalnego i nie tylko. Uwaga neuronowa szybko stała się jedną z najbardziej wpływowych idei w uczeniu głębokim. W tym podrozdziale przybliżymy jak to działa i dlaczego okazało się tak skuteczne w przypadku danych sekwencyjnych. Następnie wykorzystamy samouczenie się do stworzenia kodera Transformer, jednego z podstawowych komponentów architektury Transformer i zastosujemy go do zadania klasyfikacji recenzji filmów IMDB.\n\n7.7.1 Warstwy atencji\nCzytając tę książkę, możesz jedynie przeglądać niektóre jej części, a inne uważnie czytać, w zależności od tego, jakie są twoje cele lub zainteresowania. Co by było, gdyby modele robiły to samo? Wykorzystamy prosty pomysł: nie wszystkie informacje wejściowe widziane przez model są równie ważne dla danego zadania, więc modele powinny “zwracać większą uwagę” na niektóre funkcje i “zwracać mniejszą uwagę” na inne funkcje. Czy to brzmi znajomo? Z podobną koncepcją spotkałeś się już dwukrotnie:\n\n\nMax pooling w sieciach splotowych patrzy na pulę cech w regionie przestrzennym i wybiera tylko jedną cechę do zachowania. Jest to forma uwagi “wszystko albo nic” - zachowaj najważniejszą cechę i odrzuć resztę.\nNormalizacja TF-IDF przypisuje tokenom współczynnik ważności w oparciu o to, ile informacji mogą przenosić różne tokeny. Ważne tokeny są wzmacniane, podczas gdy nieistotne tokeny są wygaszane. Jest to ciągła forma uwagi.\n\nIstnieje wiele różnych form uwagi, które można sobie wyobrazić, ale wszystkie zaczynają się od znalezienia wskaźnika uwagi dla zestawu cech, z wyższymi wskaźnikami dla bardziej istotnych cech i niższymi dla mniej istotnych (patrz Rys. 7.5). Sposób obliczania tych wskaźników i to, co należy z nimi zrobić, będzie się różnić w zależności od podejścia.\n\n\n\n\n\nRys. 7.5: Ogólna koncepcja atencji w uczeniu głębokim: Cechom wejściowym przypisywane są wskaźniki uwagi, które można przekazać do następnej reprezentacji danych wejściowych.\n\n\nCo najważniejsze, ten rodzaj mechanizmu uwagi może być wykorzystywany do czegoś więcej niż tylko podkreślania lub wymazywania pewnych cech. Można go wykorzystać do uświadomienia kontekstu funkcji. Właśnie dowiedzieliśmy się o osadzaniu słów, czyli przestrzeniach wektorowych, które przechwytują “strukturę” relacji semantycznych między różnymi słowami. W przestrzeni osadzania pojedyncze słowo ma stałą pozycję - stały zestaw relacji z każdym innym słowem w przestrzeni. Ale nie do końca tak działa język - znaczenie słowa jest zwykle zależne od kontekstu. Kiedy zaznaczamy datę w kalendarzu (ang. date), nie mówimy o “randce”. Kiedy mówimy “I’ll see you soon”, znaczenie słowa “see” jest subtelnie inne niż “see” w “I see what you mean” lub “I’ll see this project to its end”. I oczywiście znaczenie zaimków takich jak “he”, “it”, “you” i tak dalej jest całkowicie zależne od zdania i mogą zmieniać się wielokrotnie w jednym zdaniu.\nOczywiście inteligentna przestrzeń osadzania zapewniłaby inną reprezentację wektorową dla słowa w zależności od innych otaczających go słów. W tym miejscu pojawia się samo-atencja. Celem samo-uwagi jest modulowanie reprezentacji tokena poprzez wykorzystanie reprezentacji powiązanych tokenów w sekwencji. W ten sposób powstają reprezentacje tokenów świadome kontekstu. Rozważmy przykładowe zdanie: “The train left the station on time”. Rozważmy teraz jedno słowo w zdaniu: station. O jakiej stacji mówimy? Czy może to być stacja radiowa? Może Międzynarodowa Stacja Kosmiczna? Rozważmy to algorytmicznie za pomocą samo-atencji (patrz Rys. 7.6)\n\n\n\n\n\nRys. 7.6: Samo-atencja. Wskaźniki uwagi są obliczane między “station” a każdym innym słowem w sekwencji, a następnie są używane do ważenia sumy wektorów słów, które stają się nowym wektorem “station”.\n\n\nKrok 1 polega na obliczeniu wskaźników trafności między wektorem dla “station” a każdym innym słowem w zdaniu. Są to nasze “wskaźniki uwagi”. Zamierzamy zastosować iloczyn skalarny pomiędzy dwoma wektorami słów jako miary siły ich związku. Jest to bardzo wydajna obliczeniowo funkcja odległości i była już standardowym sposobem powiązania ze sobą dwóch osadzeń słów na długo przed transformerami. W praktyce wyniki te będą również przechodzić przez funkcję skalowania i softmax, ale na razie jest to tylko szczegół implementacji.\nKrok 2 polega na obliczeniu sumy wszystkich wektorów słów w zdaniu, ważonych przez nasze wskaźniki atencji. Słowa blisko związane ze słowem “station” będą miały większy udział w sumie (w tym samo słowo “station”), podczas gdy słowa nieistotne nie wniosą prawie nic. Wynikowy wektor jest naszą nową reprezentacją dla “station”, reprezentacją, która uwzględnia otaczający kontekst. W szczególności zawiera ona część wektora “train”, wyjaśniając, że w rzeczywistości jest to “train station”.\nPowtarzamy ten proces dla każdego słowa w zdaniu, tworząc nową sekwencję wektorów kodujących zdanie. Zobaczmy to w pseudokodzie R:\n\nKodself_attention &lt;- function(input_sequence) {\n  c(sequence_len, embedding_size) %&lt;-% dim(input_sequence)\n\n  output &lt;- array(0, dim(input_sequence))\n\n  for (i in 1:sequence_len) {\n\n    pivot_vector &lt;- input_sequence[i, ]\n\n    scores &lt;- sapply(1:sequence_len, function(j)\n      pivot_vector %*% input_sequence[j, ])\n\n    scores &lt;- softmax(scores / sqrt(embedding_size))\n\n    broadcast_scores &lt;- as.matrix(scores)[,rep(1, embedding_size)]\n\n    new_pivot_representation &lt;- colSums(input_sequence * broadcast_scores)\n\n    output[i, ] &lt;- new_pivot_representation\n  }\n\n  output\n}\n\nsoftmax &lt;- function(x) {\n   e &lt;- exp(x - max(x))\n   e / sum(e)\n}\n\n\n\nKodsequence_length &lt;- 20\nembed_dim &lt;- 256\ninputs &lt;- layer_input(c(sequence_length, embed_dim))\n\n\nOczywiście, w praktyce można użyć implementacji zwektoryzowanej. Keras ma wbudowaną warstwę do obsługi tego - layer_multi_head_attention(). Oto jak można jej użyć:\n\nKodnum_heads &lt;- 4\nembed_dim &lt;- 256\n\nmha_layer &lt;- layer_multi_head_attention(num_heads = num_heads,\n                                        key_dim = embed_dim)\noutputs &lt;- mha_layer(inputs, inputs, inputs)\n\n\nCzytając to, prawdopodobnie zastanawiasz się: dlaczego przekazujemy dane wejściowe do warstwy trzy razy? Wydaje się to zbędne. Czym są te “wielokrotne głowy”? Oba te pytania mają proste odpowiedzi.\n\n7.7.2 Uogólnienie samo-atencji\nDo tej pory rozważaliśmy tylko jedną sekwencję wejściową. Jednak architektura transformera została pierwotnie opracowana dla tłumaczenia maszynowego, gdzie mamy do czynienia z dwiema sekwencjami wejściowymi: sekwencją źródłową, którą aktualnie tłumaczysz (np. “How’s the weather today?”) i sekwencją docelową, na którą ją konwertujesz (np. “¿Qué tiempo hace hoy?”). Transformer jest modelem dziłającym od sekwencji do sekwencji. Został zaprojektowany do konwersji jednej sekwencji na inną.\nTeraz cofnijmy się o krok. Mechanizm samo-uwagi, tak jak go przedstawiliśmy, wykonuje następujące czynności, schematycznie:\n\n\n\n\n\nRys. 7.7\n\n\nOznacza to, że “dla każdego tokena w wejściach (A) obliczamy, jak bardzo token jest powiązany z każdym tokenem w wejściach (B) i używamy tych wyników do ważenia sumy tokenów z wejść (C)”. Co najważniejsze, nie wymgamy, aby A, B i C odnosiły się do tej samej sekwencji wejściowej. W ogólnym przypadku można to zrobić z trzema różnymi sekwencjami. Nazwiemy je “zapytaniem” (ang. query), “kluczami” (ang. key) i “wartościami” (ang. value). Operacja ta zamienia się w “dla każdego elementu w zapytaniu oblicz, jak bardzo element jest powiązany z każdym kluczem i użyj tych wyników do ważenia sumy wartości”:\n\nKodoutputs &lt;- sum( values * pairwise_scores( query, keys ))\n\n\nTerminologia ta pochodzi z wyszukiwarek i systemów rekomendacji (patrz Rys. 7.8). Wyobraźmy sobie, że wpisujemy zapytanie, aby pobrać zdjęcie ze swojej kolekcji, “dogs on the beach”. Wewnętrznie, każde z naszych zdjęć w bazie danych jest opisane przez zestaw słów kluczowych - “cat”, “dog”, “party” i tak dalej. Nazwiemy je “kluczami”. Wyszukiwarka rozpocznie od porównania zapytania z kluczami w bazie danych. “dog” daje dopasowanie 1 dla naszego wyszukiwania, a “cat” daje dopasowanie 0. Następnie algorytm szereguje te klucze według siły dopasowania - trafności - i zwraca zdjęcia powiązane z \\(N\\) najlepszymi dopasowaniami, w kolejności trafności.\n\n\n\n\n\nRys. 7.8: Pobieranie obrazów z bazy danych. “Zapytanie” jest porównywane z zestawem “kluczy”, a wyniki dopasowania są wykorzystywane do uszeregowania “wartości” (obrazów).\n\n\nChcąc wyrazić dokładnie wskaźniki atencji wzorem matematycznym, napisalibyśmy:\n\\[\n\\operatorname{AS}(Q,K,V)=\\operatorname{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V,\n\\]\ngdzie \\(Q,K,V\\) oznaczają odpowiednio query, key i value, \\(d_k\\) oznacza długość reprezentacji tokenu w osadzeniu.\nKoncepcyjnie, to właśnie te czynności wykonuje warstwa atencji w transformerach. Mamy sekwencję referencyjną, która opisuje coś, czego szukamy: zapytanie. Mamy zbiór wiedzy, z którego próbujemy wydobyć informacje: wartości. Każda wartość ma przypisany klucz, który opisuje wartość w formacie, który można łatwo porównać z zapytaniem. Wystarczy dopasować zapytanie do kluczy. Następnie zwracana jest ważona suma wartości.\nW praktyce klucze i wartości to często ta sama sekwencja. Na przykład w tłumaczeniu maszynowym zapytanie byłoby sekwencją docelową, a sekwencja źródłowa odgrywałaby rolę zarówno kluczy, jak i wartości: dla każdego elementu celu (takiego jak “tiempo”) chcemy wrócić do źródła (“How’s the weather today?”) i zidentyfikować różne bity, które są z nim powiązane (“tiempo” i “weather” powinny mieć silne dopasowanie). Oczywiście, jeśli wykonujemy tylko klasyfikację sekwencji, to zapytanie, klucze i wartości są takie same: porównujemy sekwencję z samą sobą, aby wzbogacić każdy token o kontekst z całej sekwencji.\nTo wyjaśnia, dlaczego musieliśmy przekazać dane wejściowe trzy razy do naszej warstwy layer_multi_head_attention(). Ale po co atencja typu multi-head?\n\n7.7.3 Atencja typu multi-head\n\nMulti-head attention to dodatkowe ulepszenie mechanizmu samo-uwagi, wprowadzone w “Attention Is All You Need”. Przydomek multi-head odnosi się do faktu, że przestrzeń wyjściowa warstwy samo-uwagi jest podzielona na zestaw niezależnych podprzestrzeni, uczonych oddzielnie: początkowe zapytanie, klucz i wartość są wysyłane przez trzy niezależne zestawy gęstych projekcji, co skutkuje trzema oddzielnymi wektorami. Każdy wektor jest przetwarzany przez warstwę atencji, a trzy wyjścia są łączone z powrotem w jedną sekwencję wyjściową. Każda taka podprzestrzeń nazywana jest “głową”. Pełny obraz został przedstawiony na (ig-attention5?).\n\n\n\n\n\nRys. 7.9: Multi-head attention\n\n\nObecność gęstych projekcji, których można uczyć, pozwala warstwie faktycznie się czegoś nauczyć, w przeciwieństwie do bycia czysto bezstanową transformacją, która wymagałaby dodatkowych warstw przed lub po niej, aby była użyteczna. Ponadto posiadanie niezależnych “głów” pomaga warstwie uczyć się różnych grup cech dla każdego tokena, gdzie cechy w jednej grupie są ze sobą skorelowane, ale są w większości niezależne od cech w innej grupie.\nJest to zasadniczo podobne do tego, co robią konwolucje separowalne. W konwolucji separowalnej przestrzeń wyjściowa konwolucji jest podzielona na wiele podprzestrzeni (po jednej na kanał wejściowy), które są uczone niezależnie. Artykuł “Attention Is All You Need” został napisany w czasie, gdy wykazano, że idea faktoryzacji przestrzeni cech na niezależne podprzestrzenie zapewnia ogromne korzyści dla komputerowych modeli wizyjnych, zarówno w przypadku konwolucji separowalnych, jak i w przypadku blisko spokrewnionego podejścia, konwolucji grupowych. Multi-head attention jest po prostu zastosowaniem tego samego pomysłu do samo-atencji.\n\n7.7.4 Enkoder transformera\nJeśli dodanie dodatkowych gęstych projekcji jest tak przydatne, dlaczego nie zastosujemy jednej lub dwóch do wyjścia mechanizmu uwagi? Nasz model zaczyna realizować wiele funkcji, więc możemy chcieć dodać połączenia resztkowe, aby upewnić się, że po drodze nie utracimy żadnych cennych informacji. Dodatkowo aby przyspieszyć proces uczenia można dodać warstwy normalizacji.\nTaki proces myślowy, który, rozwijał się w umysłach twórców architektury Transformer. Dzielenie danych wyjściowych na wiele niezależnych przestrzeni, dodawanie połączeń resztkowych, dodawanie warstw normalizacji - wszystko to są standardowe wzorce architektury, które warto wykorzystać w każdym złożonym modelu.\n\n\n\n\n\nRys. 7.10: Transformer-Encoder łączy layer_multi_head_attention() z gęstą projekcją i dodaje normalizację oraz połączenia resztkowe.\n\n\nOryginalna architektura Transformer składa się z dwóch części: kodera transformera, który przetwarza sekwencję źródłową, oraz dekodera transformera, który wykorzystuje sekwencję źródłową do wygenerowania przetłumaczonej wersji. Co najważniejsze, część kodera może być używana do klasyfikacji tekstu. Jest to bardzo ogólny moduł, który przyjmuje sekwencję i uczy się przekształcać ją w bardziej użyteczną reprezentację. Zaimplementujmy koder Transformer (taki jak na Rys. 7.10) i wypróbujemy go w zadaniu klasyfikacji sentymentu recenzji filmowej.\n\nKodlayer_transformer_encoder &lt;- new_layer_class(\n  classname = \"TransformerEncoder\",\n  initialize = function(embed_dim, dense_dim, num_heads, ...) {\n    super$initialize(...)\n    self$embed_dim &lt;- embed_dim # rozmiar tokenów wejściowych\n    self$dense_dim &lt;- dense_dim # liczba neuronów w sieci gęstej\n    self$num_heads &lt;- num_heads # liczba głów w warstwie atencji\n    self$attention &lt;-\n      layer_multi_head_attention(num_heads = num_heads,\n                                 key_dim = embed_dim)\n\n    self$dense_proj &lt;- keras_model_sequential() %&gt;%\n      layer_dense(dense_dim, activation = \"relu\") %&gt;%\n      layer_dense(embed_dim)\n\n    self$layernorm_1 &lt;- layer_layer_normalization()\n    self$layernorm_2 &lt;- layer_layer_normalization()\n  },\n\n  call = function(inputs, mask = NULL) { # określenie wywołania\n    if (!is.null(mask)) # dostosowanie rozmiaru maski z 2D do 3D lub 4D\n      mask &lt;- mask[, tf$newaxis, ]\n\n    inputs %&gt;%\n      { self$attention(., ., attention_mask = mask) + . } %&gt;% # połączenie rezydualne\n      self$layernorm_1() %&gt;%\n      { self$dense_proj(.) + . } %&gt;% # połączenie projekcji gęstej z rezydualną\n      self$layernorm_2()\n  },\n\n  get_config = function() { # implementacja serializacji potrzebna do zapisu modelu\n    config &lt;- super$get_config()\n    for(name in c(\"embed_dim\", \"num_heads\", \"dense_dim\"))\n      config[[name]] &lt;- self[[name]]\n    config\n  }\n)\n\n\n\n\n\n\n\n\nWskazówka\n\n\n\nPodczas pisania niestandardowych warstw należy zaimplementować metodę get_config(). Umożliwia to ponowne utworzenie warstwy z jej konfiguracji, co jest przydatne podczas zapisywania i ładowania modelu. Metoda powinna zwracać nazwaną listę R, która zawiera wartości argumentów konstruktora użytych do utworzenia warstwy.\nWszystkie warstwy Keras mogą być serializowane i deserializowane w następujący sposób:\n\nKodconfig &lt;- layer$get_config()\nnew_layer &lt;- do.call(layer_&lt;type&gt;, config)\n\n\ngdzie layer_&lt;type&gt; jest oryginalnym konstruktorem warstwy. Na przykład:\n\nKodlayer &lt;- layer_dense(units = 10)\nconfig &lt;- layer$get_config()\nnew_layer &lt;- do.call(layer_dense, config)\n\n\nMożna również uzyskać dostęp do rozpakowanego oryginalnego konstruktora warstwy z dowolnej istniejącej warstwy bezpośrednio za pomocą specjalnego symbolu __class__ (choć rzadko trzeba to robić):\n\nKodlayer$`__class__`\nnew_layer &lt;- layer$`__class__`$from_config(config)\n\n\nZdefiniowanie metody get_config() w niestandardowych klasach warstw umożliwia określenie przepływu pracy. Na przykład:\n\nKodlayer &lt;- layer_transformer_encoder(embed_dim = 256, dense_dim = 32,\n                                   num_heads = 2)\nconfig &lt;- layer$get_config()\n\nnew_layer &lt;- do.call(layer_transformer_encoder, config)\n# -- lub --\nnew_layer &lt;- layer$`__class__`$from_config(config)\n\n\nPodczas zapisywania modelu zawierającego niestandardowe warstwy, zapisany plik będzie zawierał te konfiguracje. Podczas ładowania modelu z pliku należy dostarczyć niestandardowe klasy warstw do procesu ładowania, aby mógł on zrozumieć obiekty konfiguracyjne:\n\nKodmodel &lt;- save_model_tf(model, filename)\nmodel &lt;- load_model_tf(filename,\n                       custom_objects = list(layer_transformer_encoder))\n\n\nZwróćmy uwagę, że jeśli lista dostarczona do custom_objects jest nazwana, to nazwy są dopasowywane do argumentu classname, który został podany podczas konstruowania niestandardowego obiektu:\n\nKodmodel &lt;- load_model_tf(\n  filename,\n  custom_objects = list(TransformerEncoder = layer_transformer_encoder))\n\n\n\n\nZauważyliśmy już pewnie, że warstwy normalizacji, których tu używamy, nie są layer_batch_normalization(), jak te, których używaliśmy wcześniej w modelach do obrazów. To dlatego, że layer_batch_normalization() nie działa dobrze w przypadku danych sekwencyjnych. Zamiast tego używamy layer_layer_normalization(), która normalizuje każdą sekwencję niezależnie od innych sekwencji w partii. Tak to wygląda w pseudokodzie R:\n\nKodlayer_normalization &lt;- function(batch_of_sequences) {\n  c(batch_size, sequence_length, embedding_dim) %&lt;-% dim(batch_of_sequences)\n  means &lt;- variances &lt;-\n    array(0, dim = dim(batch_of_sequences))\n  for (b in seq(batch_size))\n    for (s in seq(sequence_length)) { # obliczanie statystyk po ostatniej osi osadzeń\n      embedding &lt;- batch_of_sequences[b, s, ]\n      means[b, s, ] &lt;- mean(embedding)\n      variances[b, s, ] &lt;- var(embedding)\n    }\n  (batch_of_sequences - means) / variances\n}\n\n\nMożna to porównać z klasyczną layer_batch_normalization():\n\nKodbatch_normalization &lt;- function(batch_of_images) {\n  c(batch_size, height, width, channels) %&lt;-% dim(batch_of_images)\n  means &lt;- variances &lt;-\n    array(0, dim = dim(batch_of_images))\n  for (ch in seq(channels)) { # statystyki liczone są po partiach\n    channel &lt;- batch_of_images[, , , ch] # dla każdego kanału liczone oddzielnie\n    means[, , , ch] &lt;- mean(channel)\n    variances[, , , ch] &lt;- var(channel)\n  }\n  (batch_of_images - means) / variances\n}\n\n\nFunkcja batch_normalization() zbiera informacje z wielu próbek w celu uzyskania statystyk średnich i wariancji cech, a funkcja layer_normalization() gromadzi dane w każdej sekwencji osobno, co jest bardziej właściwe dla danych sekwencyjnych.\nTeraz wykorzystamy zbudowany transformer do naszego zadania:\n\nKodvocab_size &lt;- 20000\nembed_dim &lt;- 256\nnum_heads &lt;- 2\ndense_dim &lt;- 32\n\ninputs &lt;- layer_input(shape(NA), dtype = \"int64\")\noutputs &lt;- inputs %&gt;%\n  layer_embedding(vocab_size, embed_dim) %&gt;%\n  layer_transformer_encoder(embed_dim, dense_dim, num_heads) %&gt;%\n  layer_global_average_pooling_1d() %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\nmodel &lt;-  keras_model(inputs, outputs)\nmodel %&gt;% compile(optimizer = \"rmsprop\",\n                  loss = \"binary_crossentropy\",\n                  metrics = \"accuracy\")\nmodel\n\nModel: \"model_5\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_7 (InputLayer)               [(None, None)]                  0           \n embedding_5 (Embedding)            (None, None, 256)               5120000     \n transformer_encoder (TransformerE  (None, None, 256)               543776      \n ncoder)                                                                        \n global_average_pooling1d (GlobalA  (None, 256)                     0           \n veragePooling1D)                                                               \n dropout_4 (Dropout)                (None, 256)                     0           \n dense_4 (Dense)                    (None, 1)                       257         \n================================================================================\nTotal params: 5664033 (21.61 MB)\nTrainable params: 5664033 (21.61 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks = list(callback_model_checkpoint(\"models/transformer_encoder.keras\",\n                                           save_best_only = TRUE))\n\n\n\nKodmodel %&gt;% fit(\n  int_train_ds,\n  validation_data = int_val_ds,\n  epochs = 20,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\"models/transformer_encoder.keras\",\n                       custom_objects = list(layer_transformer_encoder))\n\nsprintf(\"Test acc: %.3f\", evaluate(model, int_test_ds)[\"accuracy\"])\n\n782/782 - 150s - loss: 0.2902 - accuracy: 0.8876 - 150s/epoch - 191ms/step\n\n\n[1] \"Test acc: 0.888\"\n\n\nOsiągamy dopasowanie na poziomie 88,8% dla zbioru testowego. W tym momencie powinniśmy zacząć odczuwać lekki niepokój. Coś tu nie gra. Co jest nie tak?\nTen rozdział rzekomo dotyczy “modeli sekwencji”. Zaczęliśmy od podkreślenia znaczenia kolejności słów. Powiedzieliśmy, że transformer to architektura przetwarzania sekwencji, pierwotnie opracowana na potrzeby tłumaczenia maszynowego. A jednak… koder transformera, który właśnie zobaczyliśmy w akcji, wcale nie był modelem sekwencyjnym. Składa się on z gęstych warstw, które przetwarzają tokeny sekwencji niezależnie od siebie, oraz warstwy uwagi, która traktuje tokeny jako zestaw. Możesz zmienić kolejność tokenów w sekwencji, a otrzymamy dokładnie takie same wyniki wzajemnej uwagi i dokładnie takie same reprezentacje kontekstu. Gdybyśmy całkowicie wymieszali słowa w każdej recenzji filmu, model by tego nie zauważył i nadal uzyskałbyś dokładnie taką samą dokładność. Samoatencja jest mechanizmem przetwarzania zbiorów, skoncentrowanym na relacjach między parami elementów sekwencji (patrz Tab. 7.1) - jest ślepa na to, czy elementy te występują na początku, na końcu czy w środku sekwencji. Dlaczego więc mówimy, że Transformer jest modelem sekwencji? I w jaki sposób może on być dobry dla tłumaczenia maszynowego, jeśli nie bierze pod uwagę kolejności słów?\nWskazaliśmy rozwiązanie wcześniej w tym rozdziale. Mimochodem wspominaliśmy, że transformer jest podejściem hybrydowym, które jest technicznie niezależne od kolejności, ale ręcznie wprowadza informacje o kolejności do przetwarzanych reprezentacji. To jest brakujący składnik! Nazywa się to kodowaniem pozycyjnym. Przyjrzyjmy się temu.\n\n\nTab. 7.1: Cechy modeli wykorzystywanych do NLP\n\n\n\n\nŚwiadomość kolejności słów\nŚwiadomość kontekstu\n\n\n\nBag-of-unigrams\nNie\nNie\n\n\nBag-of-bigrams\nBardzo ograniczona\nNie\n\n\nRNN\nTak\nNie\n\n\nSelf-attention\nNie\nTak\n\n\nTransformer\nTak\nTak\n\n\n\n\n\n\n\n7.7.5 Kodowanie pozycyjne\nIdea kodowania pozycyjnego jest bardzo prosta. Aby zapewnić modelowi dostęp do informacji o kolejności słów, zamierzamy dodać pozycję słowa w zdaniu do każdego osadzenia słowa. Nasze wejściowe osadzenia słów będą miały dwa komponenty: zwykły wektor słów, który reprezentuje słowo niezależnie od konkretnego kontekstu, oraz wektor pozycji, który reprezentuje pozycję słowa w bieżącym zdaniu.\nNajprostszym schematem, jaki można wymyślić, byłoby połączenie pozycji słowa z jego wektorem osadzania. Dodalibyśmy oś “pozycja” do wektora i wypełnili ją wartością 0 dla pierwszego słowa w sekwencji, 1 dla drugiego i tak dalej. Może to jednak nie być idealne rozwiązanie, ponieważ pozycje mogą być potencjalnie bardzo dużymi liczbami całkowitymi, co zakłóci zakres wartości w wektorze osadzania. Jak wiadomo, sieci neuronowe nie lubią bardzo dużych wartości wejściowych ani dyskretnych rozkładów danych wejściowych.\nW oryginalnym artykule “Attention Is All You Need” zastosowano interesującą sztuczkę do kodowania pozycji słów: dodano do osadzania słów wektor zawierający wartości z zakresu [-1, 1], które zmieniały się cyklicznie w zależności od pozycji (wykorzystano do tego funkcje cosinusowe). Ta sztuczka pozwala jednoznacznie scharakteryzować dowolną liczbę całkowitą w dużym zakresie za pomocą wektora małych wartości. To sprytne, ale nie tego zamierzamy użyć w naszym przypadku. Zrobimy coś prostszego i bardziej efektywnego: nauczymy się wektorów osadzania pozycji w ten sam sposób, w jaki uczymy się osadzać indeksy słów. Następnie dodamy nasze osadzenia pozycji do odpowiednich osadzeń słów, aby uzyskać osadzenie słów uwzględniające pozycję. Technika ta nazywana jest “osadzaniem pozycyjnym”.\n\nKodlayer_positional_embedding &lt;- new_layer_class(\n  classname = \"PositionalEmbedding\",\n\n  initialize = function(sequence_length, input_dim, output_dim, ...) {\n    super$initialize(...)\n    self$token_embeddings &lt;-\n      layer_embedding(input_dim = input_dim,\n                      output_dim = output_dim)\n    self$position_embeddings &lt;-\n      layer_embedding(input_dim = sequence_length,\n                      output_dim = output_dim)\n    self$sequence_length &lt;- sequence_length\n    self$input_dim &lt;- input_dim\n    self$output_dim &lt;- output_dim\n  },\n\n  call = function(inputs) {\n    length &lt;- tf$shape(inputs)[-1]\n    positions &lt;- tf$range(start = 0L, limit = length, delta = 1L)\n    embedded_tokens &lt;- self$token_embeddings(inputs)\n    embedded_positions &lt;- self$position_embeddings(positions)\n    embedded_tokens + embedded_positions\n  },\n\n  compute_mask = function(inputs, mask = NULL) {\n    inputs != 0\n  },\n\n  get_config = function() {\n    config &lt;- super$get_config()\n    for(name in c(\"output_dim\", \"sequence_length\", \"input_dim\"))\n      config[[name]] &lt;- self[[name]]\n    config\n  }\n) \n\n\nMożna teraz użyć funkcji layer_positional_embedding() tak jak zwykłej layer_embedding().\n\nKodvocab_size &lt;- 20000\nsequence_length &lt;- 600\nembed_dim &lt;- 256\nnum_heads &lt;- 2\ndense_dim &lt;- 32\n\ninputs &lt;- layer_input(shape(NULL), dtype = \"int64\")\n\noutputs &lt;- inputs %&gt;%\n  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %&gt;%\n  layer_transformer_encoder(embed_dim, dense_dim, num_heads) %&gt;%\n  layer_global_average_pooling_1d() %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(1, activation = \"sigmoid\")\n\nmodel &lt;-\n  keras_model(inputs, outputs) %&gt;%\n  compile(optimizer = \"rmsprop\",\n          loss = \"binary_crossentropy\",\n          metrics = \"accuracy\")\n\nmodel\n\nModel: \"model_6\"\n________________________________________________________________________________\n Layer (type)                       Output Shape                    Param #     \n================================================================================\n input_8 (InputLayer)               [(None, None)]                  0           \n positional_embedding (PositionalE  (None, None, 256)               5273600     \n mbedding)                                                                      \n transformer_encoder_1 (Transforme  (None, None, 256)               543776      \n rEncoder)                                                                      \n global_average_pooling1d_1 (Globa  (None, 256)                     0           \n lAveragePooling1D)                                                             \n dropout_5 (Dropout)                (None, 256)                     0           \n dense_9 (Dense)                    (None, 1)                       257         \n================================================================================\nTotal params: 5817633 (22.19 MB)\nTrainable params: 5817633 (22.19 MB)\nNon-trainable params: 0 (0.00 Byte)\n________________________________________________________________________________\n\nKodcallbacks &lt;- list(\n  callback_model_checkpoint(\"models/full_transformer_encoder.keras\",\n                            save_best_only = TRUE)\n)\n\n\n\nKodmodel %&gt;% fit(\n  int_train_ds,\n  validation_data = int_val_ds,\n  epochs = 20,\n  callbacks = callbacks\n)\n\n\n\nKodmodel &lt;- load_model_tf(\n  \"models/full_transformer_encoder.keras\",\n  custom_objects = list(layer_transformer_encoder,\n                        layer_positional_embedding))\n\ncat(sprintf(\n  \"Test acc: %.3f\\n\", evaluate(model, int_test_ds)[\"accuracy\"]))\n\n782/782 - 149s - loss: 0.2802 - accuracy: 0.8857 - 149s/epoch - 191ms/step\nTest acc: 0.886\n\n\nOsiągnęliśmy dokładność testową na poziomie 88,6% - to nieco gorzej niż w przypadku transformera bez osadzeń pozycyjnych. Jest on jednak wciąż nieco gorszy od podejścia opartego na bag-of-words.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#wskazówki-praktyczne-dotyczące-wyboru-modelu",
    "href": "llm.html#wskazówki-praktyczne-dotyczące-wyboru-modelu",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.8 Wskazówki praktyczne dotyczące wyboru modelu",
    "text": "7.8 Wskazówki praktyczne dotyczące wyboru modelu\nCzasami można usłyszeć, że metody bag-of-words są przestarzałe i że modele sekwencji oparte na transformatach są najlepszym rozwiązaniem, niezależnie od zadania lub zbioru danych. Zdecydowanie tak nie jest. Mały stos gęstych warstw na szczycie bag-of-bigrams pozostaje całkowicie poprawnym i odpowiednim podejściem w wielu przypadkach. W rzeczywistości, spośród różnych technik, które wypróbowaliśmy na zbiorze danych IMDB w tym rozdziale, jak dotąd najlepiej sprawdzał się bag-of-bigrams! Kiedy więc należy preferować jedno podejście nad drugim?\nW 2017 roku zespół pod kierownictwem Cholleta przeprowadził systematyczną analizę wydajności różnych technik klasyfikacji tekstu w wielu różnych typach zbiorów danych tekstowych i odkrył niezwykłą i zaskakującą zasadę podejmowania decyzji, czy wybrać model bag-of-words, czy model sekwencji (http://mng.bz/AOzK) - swego rodzaju złotą proporcję. Okazuje się, że zabierając się do nowego zadania klasyfikacji tekstu, należy zwrócić szczególną uwagę na stosunek liczby próbek w danych treningowych do średniej liczby słów na próbkę (patrz Rys. 7.11). Jeśli stosunek ten jest niewielki - mniejszy niż 1500 - wówczas model bag-of-bigrams będzie działał lepiej (i jako bonus, będzie znacznie szybszy do trenowania). Jeśli współczynnik ten jest wyższy niż 1500, należy wybrać model sekwencyjny. Innymi słowy, modele sekwencyjne działają najlepiej, gdy dostępnych jest wiele danych szkoleniowych i gdy każda próbka jest stosunkowo krótka.\n\n\n\n\n\nRys. 7.11\n\n\nTak więc, jeśli klasyfikujemy dokumenty o długości 1000 słów i mamy ich 100 000 (stosunek 100), powinniśmy wybrać model bigramowy. Jeśli klasyfikujemy tweety o średniej długości 40 słów i mamy ich 50 000 (stosunek 1250), powinniśmy również wybrać model bigramowy. Ale jeśli zwiększymy rozmiar zbioru danych do 500 000 tweetów (stosunek 12 500), wybierzmy wówczas transformery. A co z zadaniem klasyfikacji recenzji filmów IMDB? Mieliśmy 20 000 próbek treningowych i średnią liczbę słów wynoszącą 233, więc nasza zasada kciuka wskazuje na model bigramowy, co potwierdza to, co znaleźliśmy w praktyce.\nIntuicyjnie ma to sens: dane wejściowe modelu sekwencji reprezentują bogatszą i bardziej złożoną przestrzeń, a zatem potrzeba więcej danych, aby ją odwzorować; tymczasem zwykły zestaw terminów jest przestrzenią tak prostą, że można na niej trenować regresję logistyczną przy użyciu zaledwie kilkuset lub tysięcy próbek. Ponadto, im krótsza jest próbka, tym mniej model może sobie pozwolić na odrzucenie jakichkolwiek zawartych w niej informacji - w szczególności kolejność słów staje się ważniejsza, a odrzucenie jej może powodować niejednoznaczność. Zdania “ten film jest bombą” i “ten film był bombą” mają bardzo zbliżone reprezentacje unigramów, co może zmylić model bag-of-words, ale model sekwencji może stwierdzić, który z nich jest negatywny, a który pozytywny. Przy dłuższej próbce statystyki słów stałyby się bardziej wiarygodne, a temat lub sentyment byłyby bardziej widoczne na podstawie samego histogramu słów.\nNależy pamiętać, że ta heurystyczna reguła została opracowana specjalnie dla klasyfikacji tekstu. Niekoniecznie musi się ona sprawdzać w innych zadaniach NLP - na przykład, jeśli chodzi o tłumaczenie maszynowe, Transformer wyróżnia się szczególnie w przypadku bardzo długich sekwencji, w porównaniu do sieci RNN. Nasza heurystyka jest również tylko praktyczną zasadą, a nie naukowym prawem, więc spodziewaj się, że będzie działać przez większość czasu, ale niekoniecznie za każdym razem.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "llm.html#tłumaczenia-typu-sequence-to-sequence",
    "href": "llm.html#tłumaczenia-typu-sequence-to-sequence",
    "title": "\n6  Modele językowe\n",
    "section": "\n7.9 Tłumaczenia typu sequence-to-sequence",
    "text": "7.9 Tłumaczenia typu sequence-to-sequence\nPosiadamy już wszystkie narzędzia, których będziemy potrzebować, aby poradzić sobie z większością zadań przetwarzania języka naturalnego. Jednakże, widzieliśmy te narzędzia w akcji tylko w jednym problemie: klasyfikacji tekstu. Jest to niezwykle popularny przypadek użycia, ale NLP to znacznie więcej niż klasyfikacja. W tym podrozdziale poszerzymy swoją wiedzę, poznając modele sequence-to-sequence.\nModel sekwencja-sekwencja przyjmuje jedną sekwencję jako dane wejściowe (często zdanie lub akapit) i tłumaczy ją na drugą sekwencję. Jest to zadanie leżące u podstaw wielu najbardziej udanych zastosowań NLP:\n\nTłumaczenie maszynowe - konwersja akapitu w języku źródłowym na jego odpowiednik w języku docelowym.\nPodsumowanie tekstu - konwersja długiego dokumentu na krótszą wersję, która zachowuje najważniejsze informacje.\nOdpowiadanie na pytania - konwersja pytania wejściowego na odpowiedź.\nChatboty - konwertowanie monitu dialogowego na odpowiedź na ten monit lub konwertowanie historii konwersacji na następną odpowiedź w konwersacji.\nGenerowanie tekstu - konwertowanie monitu tekstowego na akapit, który uzupełnia monit.\nI tak dalej.\n\n\n\n\n\n\nRys. 7.12: Uczenie sekwencyjne. Sekwencja źródłowa jest przetwarzana przez koder, a następnie przesyłana do dekodera. Dekoder analizuje dotychczasową sekwencję docelową i przewiduje przesunięcie sekwencji docelowej o jeden krok w przyszłość. Podczas wnioskowania generujemy jeden token docelowy na raz i przekazujemy go z powrotem do dekodera.\n\n\nOgólny szablon modeli sekwencja-sekwencja został opisany na Rys. 7.12. Podczas szkolenia:\n\nModel kodera zamienia sekwencję źródłową w reprezentację pośrednią.\nDekoder jest szkolony, aby przewidzieć następny token \\(i\\) w sekwencji docelowej, patrząc zarówno na poprzednie tokeny (od 1 do \\(i - 1\\)), jak i zakodowaną sekwencję źródłową.\n\nPodczas wnioskowania nie mamy dostępu do sekwencji docelowej - próbujemy przewidzieć ją od zera. Będziemy musieli generować ją po jednym tokenie na raz:\n\nOtrzymujemy zakodowaną sekwencję źródłową z kodera.\nDekoder zaczyna od patrzenia na zakodowaną sekwencję źródłową, a także początkowy token “seed” (taki jak ciąg “[start]”) i używa go do przewidzenia pierwszego prawdziwego tokena w sekwencji.\nPrzewidywana sekwencja jest przekazywana z powrotem do dekodera, który generuje następny token i tak dalej, aż do wygenerowania tokenu zatrzymania (takiego jak ciąg “[end]”).\n\nZasadę działania takiego modelu przedstawimy na przykładzie.\n\n7.9.1 Przykład tłumaczenia maszynowego\nZademonstrujemy modelowanie sekwencja do sekwencji, w zadaniu tłumaczenia maszynowego. Tłumaczenie maszynowe jest dokładnie tym, do czego transformer został opracowany! Zaczniemy od rekurencyjnego modelu sekwencji, a następnie wykorzystamy pełną architekturę transformera. Będziemy pracować z zestawem danych tłumaczeń z angielskiego na hiszpański dostępnym na stronie http://www.manythings.org/anki/. Pobierzmy go:\n\nKoddownload.file(\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n              destfile = \"data/spa-eng.zip\")\nzip::unzip(\"data/spa-eng.zip\", exdir = \"data/\")\n\n\nPlik tekstowy zawiera jeden przykład w wierszu: zdanie w języku angielskim, po którym następuje znak tabulacji, a następnie odpowiadające mu zdanie w języku hiszpańskim. Użyjmy readr::read_tsv(), ponieważ mamy wartości oddzielone tabulatorami:\n\nKodtext_file &lt;- \"data/spa-eng/spa.txt\"\ntext_pairs &lt;- text_file %&gt;%\n  readr::read_tsv(col_names = c(\"english\", \"spanish\"),\n                  col_types = c(\"cc\")) %&gt;%\n  within(spanish %&lt;&gt;% paste(\"[start]\", ., \"[end]\"))\n\n\nNasz przykładowy tekst wygląda teraz tak:\n\nKodstr(text_pairs[sample(nrow(text_pairs), 1), ])\n\ntibble [1 × 2] (S3: tbl_df/tbl/data.frame)\n $ english: chr \"That's just an excuse.\"\n $ spanish: chr \"[start] Esa solo es una excusa. [end]\"\n\n\nPrzetasujmy je i podzielmy na zwykłe zestawy treningowe, walidacyjne i testowe:\n\nKodnum_test_samples &lt;- num_val_samples &lt;-\n  round(0.15 * nrow(text_pairs))\nnum_train_samples &lt;- nrow(text_pairs) - num_val_samples - num_test_samples\n\npair_group &lt;- sample(c(\n  rep(\"train\", num_train_samples),\n  rep(\"test\", num_test_samples),\n  rep(\"val\", num_val_samples)\n))\n\ntrain_pairs &lt;- text_pairs[pair_group == \"train\", ]\ntest_pairs &lt;- text_pairs[pair_group == \"test\", ]\nval_pairs &lt;- text_pairs[pair_group == \"val\", ]\n\n\nNastępnie przygotujmy dwie oddzielne warstwy TextVectorization: jedną dla języka angielskiego i jedną dla hiszpańskiego. Będziemy musieli dostosować sposób wstępnego przetwarzania ciągów znaków:\n\nMusimy zachować wstawione przez nas tokeny “[start]” i “[end]”. Domyślnie znaki [ i ] zostaną usunięte, ale chcemy je zachować, abyśmy mogli odróżnić słowo “start” od tokenu startowego “[start]”.\nInterpunkcja różni się w zależności od języka! W hiszpańskiej warstwie wektoryzacji tekstu, jeśli zamierzamy usunąć znaki interpunkcyjne, musimy również usunąć znak ¿.\n\nZauważmy, że w przypadku modelu tłumaczenia innego niż toy-model, traktowalibyśmy znaki interpunkcyjne jako oddzielne tokeny, zamiast je usuwać, ponieważ chcielibyśmy być w stanie generować poprawnie interpunkcyjne zdania. W naszym przypadku, dla uproszczenia, pozbędziemy się całej interpunkcji.\nPrzygotowujemy niestandardową funkcję standaryzacji ciągów znaków dla hiszpańskiej warstwy TextVectorization - zachowuje ona [ i ], ale usuwa ¿, ¡ i wszystkie inne znaki z klasy [:punct:].\n\nKodpunctuation_regex &lt;- \"[^[:^punct:][\\\\]]|[¡¿]\"\n\nlibrary(tensorflow)\ncustom_standardization &lt;- function(input_string) {\n  input_string %&gt;%\n    tf$strings$lower() %&gt;%\n    tf$strings$regex_replace(punctuation_regex, \"\")\n}\n\ninput_string &lt;- as_tensor(\"[start] ¡corre! [end]\")\ncustom_standardization(input_string)\n\ntf.Tensor(b'[start] corre [end]', shape=(), dtype=string)\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nWyrażenia regularne TensorFlow różnią się nieznacznie w stosunku do silnika wyrażeń regularnych R. Więcej szczegółów można znaleźć pod adresem https://github.com/google/re2/wiki/Syntax.\n\n\n\nKodvocab_size &lt;- 15000\nsequence_length &lt;- 20\n\nsource_vectorization &lt;- layer_text_vectorization(\n  max_tokens = vocab_size,\n  output_mode = \"int\",\n  output_sequence_length = sequence_length\n)\n\ntarget_vectorization &lt;- layer_text_vectorization(\n  max_tokens = vocab_size,\n  output_mode = \"int\",\n  output_sequence_length = sequence_length + 1,\n  standardize = custom_standardization\n)\n\nadapt(source_vectorization, train_pairs$english)\nadapt(target_vectorization, train_pairs$spanish)\n\n\nNa koniec możemy przekształcić nasze dane w potok TF Dataset. Chcemy, aby zwracał on parę (inputs, target), gdzie inputs jest nazwaną listą z dwoma wpisami, angielskim zdaniem (wejście kodera) i hiszpańskim zdaniem (wejście dekodera), a target jest hiszpańskim zdaniem przesuniętym o jeden krok do przodu.\n\nKodformat_pair &lt;- function(pair) {\n  eng &lt;- source_vectorization(pair$english)\n  spa &lt;- target_vectorization(pair$spanish)\n\n  inputs &lt;- list(english = eng,\n                 spanish = spa[NA:-2])\n  targets &lt;- spa[2:NA]\n  list(inputs, targets)\n}\n\n\nbatch_size &lt;- 64\n\nlibrary(tfdatasets)\nmake_dataset &lt;- function(pairs) {\n    tensor_slices_dataset(pairs) %&gt;%\n    dataset_map(format_pair, num_parallel_calls = 4) %&gt;%\n    dataset_cache() %&gt;%\n    dataset_shuffle(2048) %&gt;%\n    dataset_batch(batch_size) %&gt;%\n    dataset_prefetch(16)\n}\ntrain_ds &lt;-  make_dataset(train_pairs)\nval_ds &lt;- make_dataset(val_pairs)\n\n\nOto jak wyglądają dane wyjściowe naszego zestawu danych:\n\nKodc(inputs, targets) %&lt;-% iter_next(as_iterator(train_ds))\nstr(inputs)\n\nList of 2\n $ english:&lt;tf.Tensor: shape=(64, 20), dtype=int64, numpy=…&gt;\n $ spanish:&lt;tf.Tensor: shape=(64, 20), dtype=int64, numpy=…&gt;\n\nKodstr(targets)\n\n&lt;tf.Tensor: shape=(64, 20), dtype=int64, numpy=…&gt;\n\n\nDane są teraz gotowe - czas zbudować kilka modeli. Zaczniemy od rekurencyjnego modelu sekwencja-sekwencja, zanim przejdziemy do transformera.\n\n7.9.2 Uczenie sekwencyjne z wykorzystaniem sieci RNN\nRekurencyjne sieci neuronowe zdominowały uczenie sekwencyjne w latach 2015-2017, zanim zostały wyprzedzone przez transformery. Były one podstawą wielu rzeczywistych systemów tłumaczenia maszynowego. Google Translate około 2017 roku był zasilany przez stos siedmiu dużych warstw LSTM. Dziś również warto zapoznać się z tym podejściem, ponieważ stanowi ono łatwy punkt wejścia do zrozumienia modeli sekwencja-sekwencja.\nNajprostszym, naiwnym sposobem wykorzystania RNN do przekształcenia sekwencji w inną sekwencję jest zachowanie danych wyjściowych RNN w każdym kroku czasowym. W języku keras wyglądałoby to następująco:\n\nKodinputs &lt;- layer_input(shape = c(sequence_length), dtype = \"int64\")\noutputs &lt;- inputs %&gt;%\n  layer_embedding(input_dim = vocab_size, output_dim = 128) %&gt;%\n  layer_lstm(32, return_sequences = TRUE) %&gt;%\n  layer_dense(vocab_size, activation = \"softmax\")\nmodel &lt;- keras_model(inputs, outputs)\n\n\nPodejście to ma jednak dwa główne problemy:\n\nSekwencja docelowa musi być zawsze tej samej długości co sekwencja źródłowa. W praktyce rzadko ma to miejsce. Z technicznego punktu widzenia nie jest to istotne, ponieważ zawsze można uzupełnić sekwencję źródłową lub docelową, aby ich długości się zgadzały.\nZe względu na charakter przetwarzania zadania przez sieć RNN krok po kroku, model będzie patrzył tylko na tokeny \\(1\\ldots N\\) w sekwencji źródłowej, aby przewidzieć token \\(N\\) w sekwencji docelowej. To ograniczenie sprawia, że ta konfiguracja nie nadaje się do większości zadań, w szczególności do tłumaczenia. Rozważmy tłumaczenie “The weather is nice today” na francuski - byłoby to “Il fait beau aujourd’hui”. Musiałbyś być w stanie przewidzieć “Il” z samego “The”, “Il fait” z samego “The weather” i tak dalej, co jest po prostu niemożliwe.\n\nJeśli jesteśmy tłumaczami, zaczynamy od przeczytania całego zdania źródłowego, zanim zaczniemy je tłumaczyć. Jest to szczególnie ważne, jeśli mamy do czynienia z językami, które mają bardzo różną kolejność słów, jak angielski i japoński. I dokładnie to robią standardowe modele sekwencja-sekwencja.\nW prawidłowej konfiguracji sekwencja-sekwencja (patrz Rys. 7.13), najpierw należy użyć RNN (kodera), aby przekształcić całą sekwencję źródłową w pojedynczy wektor (lub zestaw wektorów). Może to być ostatnie wyjście RNN lub alternatywnie jego końcowe wektory stanu wewnętrznego. Następnie należy użyć tego wektora (lub wektorów) jako stanu początkowego innej sieci RNN (dekodera), która przyjrzy się elementom \\(1\\ldots N\\) w sekwencji docelowej i spróbuje przewidzieć krok \\(N+1\\) w sekwencji docelowej.\n\n\n\n\n\nRys. 7.13: Model RNN sekwencja-sekwencja. Koder RNN jest używany do tworzenia wektora, który koduje całą sekwencję źródłową, która jest używana jako stan początkowy dla dekodera RNN.\n\n\nZaimplementujmy to w Keras za pomocą koderów i dekoderów opartych na GRU. Wybór GRU zamiast LSTM nieco upraszcza sprawę, ponieważ GRU ma tylko jeden wektor stanu, podczas gdy LSTM ma ich wiele. Zacznijmy od kodera.\n\nKodembed_dim &lt;- 256\nlatent_dim &lt;- 1024\n\nsource &lt;- layer_input(c(NA), dtype = \"int64\", name = \"english\")\nencoded_source &lt;- source %&gt;%\n  layer_embedding(vocab_size, embed_dim, mask_zero = TRUE) %&gt;%\n  bidirectional(layer_gru(units = latent_dim), merge_mode = \"sum\")\n\n\nNastępnie dodajmy dekoder - prostą warstwę GRU, która jako stan początkowy przyjmuje zakodowane zdanie źródłowe. Na wierzchu dodajemy layer_dense(), która tworzy dla każdego kroku wyjściowego rozkład prawdopodobieństwa w hiszpańskim słownictwie.\n\nKoddecoder_gru &lt;- layer_gru(units = latent_dim, return_sequences = TRUE)\n\npast_target &lt;- layer_input(shape = c(NA), dtype = \"int64\", name = \"spanish\")\ntarget_next_step &lt;- past_target %&gt;%\n  layer_embedding(vocab_size, embed_dim, mask_zero = TRUE) %&gt;%\n  decoder_gru(initial_state = encoded_source) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(vocab_size, activation = \"softmax\")\nseq2seq_rnn &lt;- keras_model(inputs = list(source, past_target),\n                           outputs = target_next_step)\n\n\nPodczas treningu dekoder przyjmuje jako dane wejściowe całą sekwencję docelową, ale dzięki krokowej naturze RNN, patrzy tylko na tokeny \\(1\\ldots N\\) na wejściu, aby przewidzieć token \\(N\\) na wyjściu (co odpowiada następnemu tokenowi w sekwencji, ponieważ wyjście ma być przesunięte o jeden krok). Oznacza to, że używamy tylko informacji z przeszłości do przewidywania przyszłości, tak jak powinniśmy; w przeciwnym razie oszukiwalibyśmy, a nasz model nie działałby w czasie wnioskowania.\n\nKodseq2seq_rnn %&gt;% compile(optimizer = \"rmsprop\",\n                        loss = \"sparse_categorical_crossentropy\",\n                        metrics = \"accuracy\")\n\nseq2seq_rnn %&gt;% fit(train_ds, epochs = 15, validation_data = val_ds)\n\n\nWybraliśmy accuracy jako niezbyt fortunny sposób monitorowania wydajności zestawu walidacyjnego podczas treningu. Średnio model przewiduje następne słowo w hiszpańskim zdaniu poprawnie w 63,2% przypadków. Jednak w praktyce dokładność następnego tokenu nie jest optymalną miarą dla modeli tłumaczenia maszynowego, w szczególności dlatego, że zakłada, że prawidłowe tokeny docelowe od \\(0\\) do \\(N\\) są już znane podczas przewidywania tokenu \\(N+1\\). W rzeczywistości podczas wnioskowania generujemy zdanie docelowe od zera i nie możemy polegać na tym, że wcześniej wygenerowane tokeny są w 100% poprawne. Jeśli pracujemy nad rzeczywistym systemem tłumaczenia maszynowego, prawdopodobnie użyjemy metryki BLEU6 do oceny swoich modeli - metryki, która analizuje całe wygenerowane sekwencje i wydaje się dobrze korelować z ludzkim postrzeganiem jakości tłumaczenia.\n6 BLEU (ang. BiLingual Evaluation Understudy) to wskaźnik służący do automatycznej oceny tekstu przetłumaczonego maszynowo. Wynik BLEU to liczba od zera do jednego, która mierzy podobieństwo tekstu przetłumaczonego maszynowo do zestawu wysokiej jakości tłumaczeń referencyjnych. Wartość 0 oznacza, że tekst przetłumaczony maszynowo nie pokrywa się z tłumaczeniem referencyjnym (niska jakość), podczas gdy wartość 1 oznacza, że tekst idealnie pokrywa się z tłumaczeniem referencyjnym (wysoka jakość).Na koniec użyjmy naszego modelu do wnioskowania. Wybierzemy kilka zdań z zestawu testowego i sprawdzimy, jak nasz model je tłumaczy. Zaczniemy od tokenu początkowego “[start]” i wprowadzimy go do modelu dekodera wraz z zakodowanym angielskim zdaniem źródłowym. Pobierzemy predykcję następnego tokena i ponownie wprowadzimy ją do dekodera, próbkując jeden nowy token docelowy w każdej iteracji, aż dojdziemy do “[end]” lub osiągniemy maksymalną długość zdania.\n\nKodspa_vocab &lt;- get_vocabulary(target_vectorization)\nmax_decoded_sentence_length &lt;- 20\n\ndecode_sequence &lt;- function(input_sentence) {\n  tokenized_input_sentence &lt;-\n    source_vectorization(array(input_sentence, dim = c(1, 1)))\n  decoded_sentence &lt;- \"[start]\"\n  for (i in seq(max_decoded_sentence_length)) {\n    tokenized_target_sentence &lt;-\n      target_vectorization(array(decoded_sentence, dim = c(1, 1)))\n    next_token_predictions &lt;- seq2seq_rnn %&gt;%\n      predict(list(tokenized_input_sentence,\n                   tokenized_target_sentence))\n    sampled_token_index &lt;- which.max(next_token_predictions[1, i, ])\n    sampled_token &lt;- spa_vocab[sampled_token_index]\n    decoded_sentence &lt;- paste(decoded_sentence, sampled_token)\n    if (sampled_token == \"[end]\")\n      break\n  }\n  decoded_sentence\n}\n\nfor (i in seq(5)) {\n    input_sentence &lt;- sample(test_pairs$english, 1)\n    print(\"-\")\n    print(input_sentence)\n    print(decode_sequence(input_sentence))\n}\n\n[1] \"-\"\n[1] \"He went to school by car.\"\n1/1 - 1s - 1s/epoch - 1s/step\n1/1 - 0s - 33ms/epoch - 33ms/step\n1/1 - 0s - 34ms/epoch - 34ms/step\n1/1 - 0s - 33ms/epoch - 33ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 33ms/epoch - 33ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n[1] \"[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos\"\n[1] \"-\"\n[1] \"That is just typical of him.\"\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 26ms/epoch - 26ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 26ms/epoch - 26ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 26ms/epoch - 26ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n[1] \"[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos\"\n[1] \"-\"\n[1] \"Tom did well for a beginner.\"\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 26ms/epoch - 26ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n[1] \"[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos\"\n[1] \"-\"\n[1] \"It started a chain reaction.\"\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 42ms/epoch - 42ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n[1] \"[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos\"\n[1] \"-\"\n[1] \"Tomorrow is a holiday.\"\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 29ms/epoch - 29ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 27ms/epoch - 27ms/step\n1/1 - 0s - 28ms/epoch - 28ms/step\n1/1 - 0s - 32ms/epoch - 32ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 31ms/epoch - 31ms/step\n1/1 - 0s - 30ms/epoch - 30ms/step\n[1] \"[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos\"\n\n\ndecode_sequence() działa dobrze, choć być może nieco wolniej niż byśmy chcieli. Jednym z łatwych sposobów na przyspieszenie działania takiego kodu jest użycie funkcji tf_function(). Przepiszmy funkcję decode_sentence() tak, aby była kompilowana przez tf_function(). Oznacza to, że zamiast używać natywnych funkcji R, takich jak seq(), predict() i which.max(), użyjemy odpowiedników TensorFlow, takich jak tf$range(), wywołując bezpośrednio model() i tf$argmax().\nPonieważ tf$range() i tf$argmax() zwracają wartość równą 0, ustawimy lokalną opcję funkcji: option(tensorflow.extract.style = \"python\"). Zmieni to zachowanie [ dla tensorów, aby również startowały od 0.\n\nKodtf_decode_sequence &lt;- tf_function(function(input_sentence) {\n\n  withr::local_options(tensorflow.extract.style = \"python\")\n\n  tokenized_input_sentence &lt;- input_sentence %&gt;%\n    as_tensor(shape = c(1, 1)) %&gt;%\n    source_vectorization()\n\n  spa_vocab &lt;- as_tensor(spa_vocab)\n\n  decoded_sentence &lt;- as_tensor(\"[start]\", shape = c(1, 1))\n\n  for (i in tf$range(as.integer(max_decoded_sentence_length))) {\n\n    tokenized_target_sentence &lt;- decoded_sentence %&gt;%\n      target_vectorization()\n\n    next_token_predictions &lt;-\n      seq2seq_rnn(list(tokenized_input_sentence,\n                       tokenized_target_sentence))\n\n    sampled_token_index &lt;- tf$argmax(next_token_predictions[0, i, ])\n    sampled_token &lt;- spa_vocab[sampled_token_index]\n    decoded_sentence &lt;-\n      tf$strings$join(c(decoded_sentence, sampled_token),\n                      separator = \" \")\n\n    if (sampled_token == \"[end]\")\n      break\n  }\n\n  decoded_sentence\n\n})\n\nfor (i in seq(5)) {\n    input_sentence &lt;- sample(test_pairs$english, 1)\n    cat(\"-\\n\")\n    cat(input_sentence, \"\\n\")\n    cat(input_sentence %&gt;% as_tensor() %&gt;%\n          tf_decode_sequence() %&gt;% as.character(), \"\\n\")\n}\n\n-\nThat's amazing! \n[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos \n-\nHow many students are there in your university? \n[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos \n-\nIs everything ready? \n[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos \n-\nI like to play tennis. \n[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos \n-\nWhat kind of sports we play depends on the weather and the season. \n[start] nomás caca matrimonio ningún enormemente subas disculpe recorrimos tripa cenicienta cansada oímos composiciones promete promete colocar colocar atónito ahogarse modernos \n\n\nNasza funkcja tf_decode_sentence() jest około 10 razy szybsza od wersji natywnej.\nNależy zauważyć, że ta konfiguracja wnioskowania, choć bardzo prosta, jest raczej nieefektywna, ponieważ ponownie przetwarzamy całe zdanie źródłowe i całe wygenerowane zdanie docelowe za każdym razem, gdy próbkujemy nowe słowo. W praktycznym zastosowaniu koder i dekoder byłyby traktowane jako dwa oddzielne modele, a dekoder wykonywałby tylko jeden krok w każdej iteracji próbkowania tokenów, ponownie wykorzystując swój poprzedni stan wewnętrzny.\nOto wyniki naszego tłumaczenia. Nasz model działa przyzwoicie jak na model typu toy, choć nadal popełnia wiele podstawowych błędów.\nIstnieje wiele sposobów na ulepszenie “zabawkowego” modelu: moglibyśmy użyć głębokiego stosu warstw rekurencyjnych zarówno dla kodera, jak i dekodera (należy pamiętać, że w przypadku dekodera zarządzanie stanem jest nieco bardziej skomplikowane). Moglibyśmy użyć LSTM zamiast GRU. I tak dalej. Poza takimi poprawkami, podejście RNN do uczenia sekwencyjnego ma jednak kilka fundamentalnych ograniczeń:\n\nReprezentacja sekwencji źródłowej musi być przechowywana w całości w wektorze (wektorach) stanu kodera, co nakłada znaczne ograniczenia na rozmiar i złożoność zdań, które można przetłumaczyć. To trochę tak, jakby człowiek tłumaczył zdanie w całości z pamięci, nie patrząc dwa razy na zdanie źródłowe podczas tworzenia tłumaczenia.\nRNN mają problem z radzeniem sobie z bardzo długimi sekwencjami, ponieważ mają tendencję do stopniowego zapominania o przeszłości - do czasu osiągnięcia setnego tokena w dowolnej sekwencji, niewiele informacji pozostaje o początku sekwencji. Oznacza to, że modele oparte na RNN nie są w stanie utrzymać długoterminowego kontekstu, który może być niezbędny do tłumaczenia długich dokumentów.\n\nOgraniczenia te sprawiły, że społeczność zajmująca się uczeniem maszynowym przyjęła architekturę transformer do rozwiązywania problemów typu sekwencja-sekwencja. Przyjrzyjmy się temu.\n\n7.9.3 Uczenie się sekwencyjne z transformerem\nUczenie sekwencyjne to zadanie, w którym transformer naprawdę błyszczy. Atencja neuronowa umożliwia modelom transformer właściwe przetwarzanie sekwencji, które są znacznie dłuższe i bardziej złożone niż te, z którymi radzą sobie sieci RNN.\nJako człowiek tłumaczący z angielskiego na hiszpański, nie czytamy angielskiego zdania po jednym słowie na raz, tylko zachowujemy jego znaczenie w pamięci, a następnie generujemy hiszpańskie zdanie po jednym słowie na raz. Może to zadziałać w przypadku zdania składającego się z pięciu słów, ale jest mało prawdopodobne, by zadziałało w przypadku całego akapitu. Zamiast tego, prawdopodobnie będziemy chcieli przeskakiwać między zdaniem źródłowym a tłumaczeniem i zwracać uwagę na różne słowa w źródle podczas zapisywania różnych części tłumaczenia.\nDokładnie to można osiągnąć za pomocą atencji neuronowej i transformerów. Znamy już koder transformera, który wykorzystuje samo-atencję (w odmianie z wieloma głowami) do tworzenia reprezentacji kontekstowych każdego tokena w sekwencji wejściowej. W transformatorze sekwencja-sekwencja, koder transformera naturalnie odgrywałby rolę kodera, który odczytuje sekwencję źródłową i tworzy jej zakodowaną reprezentację. Jednak w przeciwieństwie do naszego poprzedniego kodera RNN, koder transformer utrzymuje zakodowaną reprezentację w formacie sekwencji: jest to sekwencja wektorów osadzania z uwzględnieniem kontekstu.\nDruga część modelu to dekoder transformera. Podobnie jak dekoder RNN, odczytuje on tokeny \\(1\\ldots N\\) w sekwencji docelowej i próbuje przewidzieć token \\(N + 1\\). Co najważniejsze, robiąc to, wykorzystuje atencję neuronową, aby zidentyfikować, które tokeny w zakodowanym zdaniu źródłowym są najbardziej powiązane z tokenem docelowym, który obecnie próbuje przewidzieć. Przypomnijmy model atencji posiadał query, key i value. W dekoderze transformerowym sekwencja docelowa służy jako query, które jest używane do zwracania większej uwagi na różne części sekwencji źródłowej (sekwencja źródłowa odgrywa role zarówno key, jak i value).\n\n7.9.3.1 Dekoder transformerowy\nRys. 7.14 przedstawia pełny transformator sekwencja do sekwencji. Przyjrzyjmy się wewnętrznym elementom dekodera. Rozpoznamy, że wygląda on bardzo podobnie do kodera transformera, z wyjątkiem tego, że dodatkowy blok atencji jest wstawiony pomiędzy blok samo-atencji zastosowany do sekwencji docelowej i gęstych warstw bloku wyjściowego.\n\n\n\n\n\nRys. 7.14: TransformerDecoder jest podobny do TransformerEncoder, z wyjątkiem tego, że zawiera dodatkowy blok atencji, w którym key i value są sekwencją źródłową zakodowaną przez TransformerEncoder. Razem, koder i dekoder tworzą transformator end-to-end.\n\n\nZaimplementujmy to. Podobnie jak w przypadku TransformerEncoder, będziemy tworzyć nową klasę warstw.\nMetoda call() jest prostym odwzorowaniem połączeń diagramu z Rys. 7.14. Jest jednak dodatkowy szczegół, który musimy wziąć pod uwagę - przyczynowe wypełnianie (ang. casual padding). Wypełnianie przyczynowe jest absolutnie krytyczne dla pomyślnego uczenia transformatora sekwencja do sekwencji. W przeciwieństwie do RNN, który patrzy na swoje dane wejściowe po jednym kroku na raz, a zatem będzie miał dostęp tylko do kroków \\(1\\ldots N\\), aby wygenerować krok wyjściowy \\(N\\) (który jest tokenem \\(N+1\\) w sekwencji docelowej), natomiast TransformerDecoder jest niezależny od kolejności: patrzy na całą sekwencję docelową naraz. Gdyby pozwolić mu na wykorzystanie całego wejścia, po prostu nauczyłby się kopiować krok wejściowy \\(N+1\\) do lokalizacji \\(N\\) na wyjściu. W ten sposób model osiągnąłby idealną dokładność treningu, ale oczywiście podczas wnioskowania byłby całkowicie bezużyteczny, ponieważ kroki wejściowe poza \\(N\\) nie są dostępne.\nRozwiązanie jest proste - zamaskujemy górną połowę macierzy atencji parami, aby uniemożliwić modelowi zwracanie uwagi na informacje z przyszłości - informacje tylko z tokenów \\(1\\ldots N\\) w sekwencji docelowej powinny być używane podczas generowania tokena docelowego \\(N+1\\). Aby to zrobić, dodamy metodę get_causal_attention_mask(inputs) do naszego TransformerDecoder, aby pobrać maskę uwagi, którą możemy przekazać do naszych warstw MultiHeadAttention.\n\nKodlayer_transformer_decoder &lt;- new_layer_class(\n  classname = \"TransformerDecoder\",\n\n  initialize = function(embed_dim, dense_dim, num_heads, ...) {\n    super$initialize(...)\n    self$embed_dim &lt;- embed_dim\n    self$dense_dim &lt;- dense_dim\n    self$num_heads &lt;- num_heads\n    self$attention_1 &lt;- layer_multi_head_attention(num_heads = num_heads,\n                                                   key_dim = embed_dim)\n    self$attention_2 &lt;- layer_multi_head_attention(num_heads = num_heads,\n                                                   key_dim = embed_dim)\n    self$dense_proj &lt;- keras_model_sequential() %&gt;%\n      layer_dense(dense_dim, activation = \"relu\") %&gt;%\n      layer_dense(embed_dim)\n\n    self$layernorm_1 &lt;- layer_layer_normalization()\n    self$layernorm_2 &lt;- layer_layer_normalization()\n    self$layernorm_3 &lt;- layer_layer_normalization()\n    self$supports_masking &lt;- TRUE\n  },\n\n  get_config = function() {\n    config &lt;- super$get_config()\n    for (name in c(\"embed_dim\", \"num_heads\", \"dense_dim\"))\n      config[[name]] &lt;- self[[name]]\n    config\n  },\n\n  get_causal_attention_mask = function(inputs) {\n    c(batch_size, sequence_length, encoding_length) %&lt;-%\n      tf$unstack(tf$shape(inputs))\n\n    x &lt;- tf$range(sequence_length)\n    i &lt;- x[, tf$newaxis]\n    j &lt;- x[tf$newaxis, ]\n    mask &lt;- tf$cast(i &gt;= j, \"int32\")\n\n    tf$tile(mask[tf$newaxis, , ],\n            tf$stack(c(batch_size, 1L, 1L)))\n  },\n\n  call = function(inputs, encoder_outputs, mask = NULL) {\n\n    causal_mask &lt;- self$get_causal_attention_mask(inputs)\n\n    if (is.null(mask))\n      mask &lt;- causal_mask\n    else\n      mask %&lt;&gt;% { tf$minimum(tf$cast(.[, tf$newaxis, ], \"int32\"),\n                             causal_mask) }\n\n    inputs %&gt;%\n      { self$attention_1(query = ., value = ., key = .,\n                         attention_mask = causal_mask) + . } %&gt;%\n      self$layernorm_1() %&gt;%\n\n      { self$attention_2(query = .,\n                         value = encoder_outputs,\n                         key = encoder_outputs,\n                         attention_mask = mask) + . } %&gt;%\n      self$layernorm_2() %&gt;%\n\n      { self$dense_proj(.) + . } %&gt;%\n      self$layernorm_3()\n\n  }\n)\n\n\nBędziemy trenować model transformator typu end-to-end. Mapuje on sekwencję źródłową i docelową do sekwencji docelowej o jeden krok w przyszłości. W prosty sposób łączy elementy, które zbudowaliśmy do tej pory: warstwy PositionalEmbedding, TransformerEncoder i TransformerDecoder. Zwróćmy uwagę, że zarówno TransformerEncoder, jak i TransformerDecoder są niezmienne pod względem kształtu7, więc możemy połączyć wiele z nich, aby stworzyć bardziej wydajny koder lub dekoder. W naszym przykładzie będziemy trzymać się pojedynczej instancji każdego z nich.\n7 wejście i wyjście jest tych samych rozmiarów\nKodembed_dim &lt;- 256\ndense_dim &lt;- 2048\nnum_heads &lt;- 8\n\nencoder_inputs &lt;- layer_input(shape(NA), dtype = \"int64\", name = \"english\")\nencoder_outputs &lt;- encoder_inputs %&gt;%\n  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %&gt;%\n  layer_transformer_encoder(embed_dim, dense_dim, num_heads)\n\ntransformer_decoder &lt;-\n  layer_transformer_decoder(NULL, embed_dim, dense_dim, num_heads)\n\ndecoder_inputs &lt;-  layer_input(shape(NA), dtype = \"int64\", name = \"spanish\")\ndecoder_outputs &lt;- decoder_inputs %&gt;%\n  layer_positional_embedding(sequence_length, vocab_size, embed_dim) %&gt;%\n  transformer_decoder(., encoder_outputs) %&gt;%\n  layer_dropout(0.5) %&gt;%\n  layer_dense(vocab_size, activation=\"softmax\")\n\ntransformer &lt;- keras_model(list(encoder_inputs, decoder_inputs),\n                           decoder_outputs)\n\n\n\nKodtransformer %&gt;%\n  compile(optimizer = \"rmsprop\",\n          loss = \"sparse_categorical_crossentropy\",\n          metrics = \"accuracy\")\n\ntransformer %&gt;%\n  fit(train_ds, epochs = 30, validation_data = val_ds)\n\nsave_model_tf(transformer, filepath = \"models/end_to_end_transformer.keras\")\n\n\nOsiągamy dokładność 65,3%, czyli istotnie lepiej niż z modelu opartego na GRU.\nNa koniec spróbujmy użyć naszego modelu do przetłumaczenia nigdy wcześniej nie widzianych angielskich zdań z zestawu testowego. Konfiguracja jest identyczna z tą, której użyliśmy do modelu RNN typu sekwencja-sekwencja. Zastąpimy tylko seq2seq_rnn transformatorem i usuniemy dodatkowego tokena, który skonfigurowaliśmy do dodania w warstwie target_vectorization().\n\nKodtransformer &lt;- load_model_tf(\n  \"models/end_to_end_transformer.keras\",\n  custom_objects = list(\n    layer_positional_embedding,\n    layer_transformer_decoder,\n    layer_transformer_encoder\n  )\n)\n\n\n\nKodtf_decode_sequence &lt;- tf_function(function(input_sentence) {\n  withr::local_options(tensorflow.extract.style = \"python\")\n\n  tokenized_input_sentence &lt;- input_sentence %&gt;%\n    as_tensor(shape = c(1, 1)) %&gt;%\n    source_vectorization()\n  spa_vocab &lt;- as_tensor(spa_vocab)\n  decoded_sentence &lt;- as_tensor(\"[start]\", shape = c(1, 1))\n\n  for (i in tf$range(as.integer(max_decoded_sentence_length))) {\n\n    tokenized_target_sentence &lt;-\n      target_vectorization(decoded_sentence)[,NA:-1]\n\n    next_token_predictions &lt;-\n      transformer(list(tokenized_input_sentence,\n                       tokenized_target_sentence))\n\n    sampled_token_index &lt;- tf$argmax(next_token_predictions[0, i, ])\n    sampled_token &lt;- spa_vocab[sampled_token_index]\n    decoded_sentence &lt;-\n      tf$strings$join(c(decoded_sentence, sampled_token),\n                      separator = \" \")\n\n    if (sampled_token == \"[end]\")\n      break\n  }\n\n  decoded_sentence\n\n})\n\nfor (i in seq(20)) {\n\n    c(input_sentence, correct_translation) %&lt;-%\n      test_pairs[sample.int(nrow(test_pairs), 1), ]\n    cat(\"-\\n\")\n    cat(input_sentence, \"\\n\")\n    cat(correct_translation, \"\\n\")\n    cat(input_sentence %&gt;% as_tensor() %&gt;%\n          tf_decode_sequence() %&gt;% as.character(), \"\\n\")\n}\n\n-\nThis book is new. \n[start] Este libro es nuevo. [end] \n[start] se ha estuvo déjame no es [end] \n-\nThe coffee was so hot that I nearly burned my tongue. \n[start] El café estaba tan caliente que casi me quemé la lengua. [end] \n[start] no sabía que nada sea vos de que [UNK] de francés [UNK] [end] \n-\nHaven't I already told you about this before? \n[start] ¿No te lo he dicho ya antes? [end] \n[start] sin fue tengo que te habría [end] \n-\nI thought Tom had a dog. \n[start] Pensé que Tom tenía un perro. [end] \n[start] quiere cuando estuvo tom [end] \n-\nI am hanging up a picture of my grandmother. \n[start] Estoy colgando una foto de mi abuela. [end] \n[start] yo pero de la había [end] \n-\nTom didn't know that Mary liked to cook. \n[start] Tom no sabía que a Mary le gustaba cocinar. [end] \n[start] tom no sabía que para tenía estáis estaba japón [end] \n-\nIt's difficult to understand his theory. \n[start] Es difícil entender su teoría. [end] \n[start] deberías se ha estuvo a mi casi discusión [end] \n-\nPlease say a prayer for those who were onboard. \n[start] Por favor diga una plegaria por aquellos que estuvieron a bordo. [end] \n[start] por todos equivocado en sueño de fácil se [UNK] [end] \n-\nI am never free on Sundays. \n[start] Nunca estoy libre los domingos. [end] \n[start] yo tengo mayor diste por vi diste [end] \n-\nWhy did you call me at this unearthly hour? \n[start] ¿Para qué me llamas a esta hora imprudente? [end] \n[start] de nuestro te [UNK] antes que todo tengo usar [end] \n-\nHe drinks a lot of milk every day. \n[start] Él toma mucha leche todos los días. [end] \n[start] su eso niño habló he pronto tan mi tren [end] \n-\nHe's three years older than she is. \n[start] Él tiene tres años más que ella. [end] \n[start] tan mi perder pie hace [end] \n-\nI feel like throwing up. \n[start] Tengo ganas de vomitar. [end] \n[start] me policía en mira [end] \n-\nTom worked as a farmer. \n[start] Tomás trabajaba como granjero. [end] \n[start] tom preguntaba a un piensas [UNK] [end] \n-\nMy father likes fishing, and so do I. \n[start] A mi padre le gusta pescar y a mí también. [end] \n[start] el toda béisbol [UNK] y algunas [end] \n-\nTom is deeply in love with Mary. \n[start] Tom está perdidamente enamorado de Mary. [end] \n[start] tom es posible en mis estas para [end] \n-\nThe man slumped to the floor. \n[start] El hombre se derrumbó sobre el suelo. [end] \n[start] se te buscando era [end] \n-\nMove along now. \n[start] Muévase ya. [end] \n[start] dijo estás madre ojos [end] \n-\nDo you want to kill me? \n[start] ¿Quieres matarme? [end] \n[start] desde a este terminado [end] \n-\nThis is not enough. \n[start] No es suficiente. [end] \n[start] trabajo no se ha estuvo [end] \n\n\nNa tym kończy się ten rozdział o przetwarzaniu języka naturalnego. W ramach niego przeszliśmy od podstaw do w rozwiązania współcześnie używanego, czyli Transformera, który może tłumaczyć zdania z angielskiego na hiszpański.\n\n\n\n\nBengio, Yoshua, Réjean Ducharme, Pascal Vincent, i Christian Jauvin. b.d. „A Neural Probabilistic Language Model”.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, i Illia Polosukhin. b.d. „Attention Is All You Need”. https://doi.org/10.48550/arXiv.1706.03762.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Modele językowe</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Bengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Jauvin.\nn.d. “A Neural Probabilistic Language Model.”\n\n\nBengio, Y., P. Simard, and P. Frasconi. 1994. “Learning Long-Term\nDependencies with Gradient Descent Is Difficult.” IEEE\nTransactions on Neural Networks 5 (2): 157–66. https://doi.org/10.1109/72.279181.\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, and Pedro Larrañaga.\n2015. “A Survey on Multi-Output Regression.” WIREs Data\nMining and Knowledge Discovery 5 (5): 216–33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, and Charles J. Stone.\n2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nCho, Kyunghyun, Bart van Merrienboer, Dzmitry Bahdanau, and Yoshua\nBengio. n.d. “On the Properties of Neural Machine Translation:\nEncoder-Decoder Approaches.” https://doi.org/10.48550/arXiv.1409.1259.\n\n\nEvgeniou, Theodoros, and Massimiliano Pontil. 2004. “Regularized\nMulti–Task Learning.” Proceedings of the Tenth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\nAugust. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Rémi Gilleron, and Fabien\nTorre. 2012. “Learning Multiple Tasks with Boosted Decision\nTrees.” In Proceedings of the 2012th European Conference on\nMachine Learning and Knowledge Discovery in Databases - Volume Part\ni, 681–96. ECMLPKDD’12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, and Remi\nGilleron. 2010. “Boosting Multi-Task Weak Learners with\nApplications to Textual and Social Data.” In 2010 Ninth\nInternational Conference on Machine Learning and Applications,\n367–72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nGlocker, Ben, Olivier Pauly, Ender Konukoglu, and Antonio Criminisi.\n2012. “Joint Classification-Regression Forests for Spatially\nStructured Multi-Object Segmentation.” In Computer Vision –\nECCV 2012, edited by Andrew Fitzgibbon, Svetlana Lazebnik, Pietro\nPerona, Yoichi Sato, and Cordelia Schmid, 7575:870–81. Springer Berlin\nHeidelberg. http://link.springer.com/10.1007/978-3-642-33765-9_62.\n\n\nHochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long\nShort-Term Memory.” Neural Computation 9\n(8): 1735–80. https://doi.org/10.1162/neco.1997.9.8.1735.\n\n\nIzenman, Alan Julian. 1975. “Reduced-Rank Regression for the\nMultivariate Linear Model.” Journal of Multivariate\nAnalysis 5 (2): 248–64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, and Sašo Džeroski. 2013.\n“Tree Ensembles for Predicting Structured Outputs.”\nPattern Recognition 46 (3): 817–33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, and Sebastián Ventura.\n2017. “Multi-Target Support Vector Regression via Correlation\nRegressor Chains.” Information Sciences 415–416\n(November): 53–69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning.\nMorgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello\nMastelini, Fabio Luiz Melquiades, and Sylvio Barbon Jr. 2020.\n“Improved Prediction of Soil Properties with Multi-Target Stacked\nGeneralisation on EDXRF Spectra.” arXiv Preprint\narXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. “Tree-Structured Methods for\nLongitudinal Data.” Journal of the American Statistical\nAssociation 87 (418): 407–18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves,\nand Ioannis Vlahavas. 2016. “Multi-Target Regression\nvia Input Space Expansion: Treating Targets as\nInputs.” Machine Learning 104 (1): 55–98.\nhttps://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, and Sašo Džeroski. 2006. “Constraint Based Induction\nof Multi-Objective Regression Trees.” In Knowledge Discovery\nin Inductive Databases, edited by Francesco Bonchi and\nJean-François Boulicaut, 222–33. Lecture Notes in Computer Science.\nSpringer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, and Victor Sheng. 2013. “Empirical Comparison of\nMulti-Label Classification Algorithms.” In Proceedings of the\nAAAI Conference on Artificial\nIntelligence, 27:1645–46.\n\n\nTsoumakas, Grigorios, and Ioannis Katakis. 2007. “Multi-Label\nClassification: An Overview.” International\nJournal of Data Warehousing and Mining (IJDWM) 3 (3): 1–13.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. n.d.\n“Attention Is All You Need.” https://doi.org/10.48550/arXiv.1706.03762.\n\n\nVazquez, Emmanuel, and Eric Walter. 2003. “Multi-Output Suppport\nVector Regression.” IFAC Proceedings Volumes, 13th IFAC\nsymposium on system identification (SYSID 2003), rotterdam, the\nnetherlands, 27-29 august, 2003, 36 (16): 1783–88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, and Laisheng Wang. 2013. “A Twin Multi-Class\nClassification Support Vector Machine.” Cognitive\nComputation 5 (4): 580–88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, and Cheolkon Jung. n.d. “GBDT-MO: Gradient\nBoosted Decision Trees for Multiple Outputs.” https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "Literatura"
    ]
  }
]