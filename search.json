[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zaawansowane metody uczenia maszynowego",
    "section": "",
    "text": "WstÄ™p\nKsiÄ…Å¼ka ta jest napisana na potrzeby prowadzenia zajÄ™Ä‡ na kierunku InÅ¼ynieria i analiza danych z przedmiotu Zaawansowane metody uczenia maszynowego. Jest swego rodzaju autorskim podejÅ›ciem do tematu, przedstawiajÄ…cym wybrane metody uczenia maszynowego, ktÃ³re rzadziej wystÄ™pujÄ… w opracowaniach na temat uczenia maszynowego.\nUczenie maszynowe stanowi obszar intensywnego rozwoju, ktÃ³ry obejmuje szereg technik umoÅ¼liwiajÄ…cych bardziej skomplikowane i wydajne modele predykcyjne. WÅ›rÃ³d tych metod warto wyrÃ³Å¼niÄ‡ gÅ‚Ä™bokie sieci neuronowe, zwÅ‚aszcza konwolucyjne sieci neuronowe (CNN) i rekurencyjne sieci neuronowe (RNN). CNN sÄ… wykorzystywane w zadaniach przetwarzania obrazÃ³w, gdzie potrafiÄ… efektywnie ekstrahowaÄ‡ hierarchiczne cechy z danych wejÅ›ciowych, natomiast RNN sÄ… efektywne w analizie sekwencji danych, takich jak jÄ™zyk naturalny. Ponadto, metody uczenia maszynowego obejmujÄ… techniki transferu wiedzy, uczenie ze wzmocnieniem, generatywne modele, takie jak generatywne sieci przeciwdziedzinowe (GAN), czy teÅ¼ autokodery. Te nowoczesne podejÅ›cia umoÅ¼liwiajÄ… modelom uczÄ…cym siÄ™ wykonywanie bardziej zÅ‚oÅ¼onych zadaÅ„, a takÅ¼e adaptacjÄ™ do rÃ³Å¼norodnych danych wejÅ›ciowych, co sprawia, Å¼e sÄ… one stosowane w obszarach takich jak rozpoznawanie obrazÃ³w, przetwarzanie jÄ™zyka naturalnego, czy nawet w autonomicznych systemach decyzyjnych.\nPonadto, zaawansowane metody uczenia maszynowego obejmujÄ… takÅ¼e techniki regularyzacji, optymalizacji i inÅ¼ynieriÄ™ cech. Regularyzacja ma na celu zapobieganie przeuczeniu poprzez kontrolowanie zÅ‚oÅ¼onoÅ›ci modelu, natomiast optymalizacja skupia siÄ™ na dostosowywaniu wag modelu w celu minimalizacji funkcji straty. InÅ¼ynieria cech polega na rÄ™cznym lub automatycznym dostosowywaniu danych wejÅ›ciowych w celu uzyskania lepszych wynikÃ³w modelu. DziÄ™ki tym zaawansowanym metodom, uczenie maszynowe staje siÄ™ coraz bardziej potÄ™Å¼nym narzÄ™dziem w analizie danych i podejmowaniu skomplikowanych decyzji w rÃ³Å¼nych dziedzinach.\nModele predykcyjne dla wielu wyjÅ›Ä‡, czyli tzw. multi-target regression and classification, stanowiÄ… kolejny istotny obszar w dziedzinie uczenia maszynowego. W przypadku multi-target regression, celem jest przewidywanie wielu wartoÅ›ci wyjÅ›ciowych dla danego zestawu wejÅ›ciowego, co czÄ™sto spotyka siÄ™ w zÅ‚oÅ¼onych problemach predykcyjnych, takich jak prognozowanie wielu parametrÃ³w jednoczeÅ›nie. Z kolei w przypadku multi-target classification, model ma za zadanie przypisanie jednego lub wiÄ™cej klas dla kaÅ¼dego przykÅ‚adu wejÅ›ciowego. Te modele sÄ… powszechnie stosowane w rÃ³Å¼nych dziedzinach, takich jak bioinformatyka, finanse czy przemysÅ‚, gdzie jednoczesne przewidywanie wielu zmiennych jest kluczowe dla skutecznego rozwiÄ…zania problemu. WdroÅ¼enie takich zaawansowanych modeli predykcyjnych wymaga starannej obrÃ³bki danych, odpowiedniego dostosowania architektury modelu oraz precyzyjnej oceny wynikÃ³w, co sprawia, Å¼e sÄ… one istotnym narzÄ™dziem w obszarze analizy danych i podejmowania decyzji.\nModele jÄ™zykowe stanowiÄ… jeszcze jeden kluczowy obszar w dziedzinie uczenia maszynowego, skoncentrowany na zrozumieniu i generowaniu ludzkiego jÄ™zyka naturalnego. GÅ‚Ä™bokie sieci neuronowe, zwÅ‚aszcza rekurencyjne sieci neuronowe (RNN) i transformery, zostaÅ‚y skutecznie wykorzystane do tworzenia modeli jÄ™zykowych o zdolnoÅ›ciach przetwarzania i generowania tekstu na poziomie zbliÅ¼onym do ludzkiego. Te modele zdolne sÄ… do zrozumienia kontekstu, analizy gramatyki, a takÅ¼e generowania spÃ³jnych i sensownych odpowiedzi. Wykorzystywane sÄ… w rÃ³Å¼norodnych zastosowaniach, takich jak tÅ‚umaczenie maszynowe, generowanie tekstu, czy analiza nastroju w tekÅ›cie. Ponadto, pre-trenowane modele jÄ™zykowe, takie jak BERT czy GPT (Generative Pre-trained Transformer), zdobywajÄ… popularnoÅ›Ä‡, umoÅ¼liwiajÄ…c dostosowanie ich do rÃ³Å¼nych zadaÅ„ poprzez fine-tuning. W miarÄ™ postÄ™pu badaÅ„ i rozwoju w tej dziedzinie, modele jÄ™zykowe stajÄ… siÄ™ coraz bardziej zaawansowane, co przyczynia siÄ™ do doskonalenia komunikacji miÄ™dzy maszynami a ludÅºmi oraz do rozwijania nowych moÅ¼liwoÅ›ci w dziedzinie przetwarzania jÄ™zyka naturalnego.\nWspomniane powyÅ¼ej metody i modele bÄ™dÄ… stanowiÄ‡ treÅ›Ä‡ wykÅ‚adÃ³w z wspomnianego na wstÄ™pie przedmiotu.",
    "crumbs": [
      "WstÄ™p"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1Â  Wprowadzenie",
    "section": "",
    "text": "Witam w Å›wiecie zaawansowanych metod uczenia maszynowego ğŸ¤–, prezentowanej w niniejszej publikacji. KsiÄ…Å¼ka ta skupia siÄ™ na trzech gÅ‚Ã³wnych obszarach, zaczynajÄ…c od wielowymiarowych problemÃ³w predykcyjnych, przechodzÄ…c przez kompleksowe modele gÅ‚Ä™bokich sieci neuronowych, a koÅ„czÄ…c na zaawansowanych modelach jÄ™zykowych. Koncepcyjnie rozpoczniemy od omÃ³wienia multiple target regression and classification, gdzie przedstawimy skomplikowane zadania predykcyjne wymagajÄ…ce jednoczesnej prognozy wielu zmiennych. Przeanalizujemy praktyczne zastosowania tych modeli w obszarach, takich jak nauki spoÅ‚eczne, biologia i finanse.\nNastÄ™pnie poÅ›wiÄ™cimy uwagÄ™ gÅ‚Ä™bokim sieciom neuronowym, gÅ‚Ã³wnemu filarowi nowoczesnej sztucznej inteligencji. OmÃ³wimy ewolucjÄ™ od konwolucyjnych sieci neuronowych (CNN) do rekurencyjnych sieci neuronowych (RNN), zwracajÄ…c uwagÄ™ na ich zdolnoÅ›Ä‡ do efektywnego przetwarzania obrazÃ³w, sekwencji danych i rozwiÄ…zania bardziej zÅ‚oÅ¼onych problemÃ³w. W ramach tego obszaru, przyjrzymy siÄ™ rÃ³wnieÅ¼ technikom transferu wiedzy, uczenia ze wzmocnieniem oraz generatywnym modelom, takim jak generatywne sieci przeciwdziedzinowe (GAN), ktÃ³re poszerzajÄ… granice moÅ¼liwoÅ›ci maszynowego uczenia siÄ™.\n\n\n\nTrzeci kluczowy obszar, ktÃ³ry bÄ™dzie przedmiotem analizy, to modele jÄ™zykowe. RozwaÅ¼ania rozpoczniemy od gÅ‚Ä™bokich sieci neuronowych, a nastÄ™pnie skoncentrujemy siÄ™ na transformatorach, ktÃ³re rewolucjonizujÄ… przetwarzanie jÄ™zyka naturalnego ğŸ‘…. Przedstawimy praktyczne zastosowania tych modeli, zwÅ‚aszcza w tÅ‚umaczeniu maszynowym, generowaniu tekstu i analizie sentymentu. Ponadto, omÃ³wimy pre-trenowane modele jÄ™zykowe, takie jak BERT czy GPT, jako kluczowe narzÄ™dzia adaptacyjne, zdolne do fine-tuningu w zaleÅ¼noÅ›ci od konkretnego zadania.\nKaÅ¼dy podejmowany temat bÄ™dzie wzbogacony o implementacjÄ™ analizowanych metod w realnych scenariuszach. OmÃ³wimy kroki od obrÃ³bki danych, przez dostosowywanie architektury modelu, aÅ¼ po ocenÄ™ wynikÃ³w. W tym kontekÅ›cie poruszymy takÅ¼e aspekty etyczne i wyzwania zwiÄ…zane z zastosowaniem zaawansowanych modeli uczenia maszynowego.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Wprowadzenie</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html",
    "href": "multi_target_models.html",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "",
    "text": "2.1 Typy modeli z wieloma zmiennymi wynikowymi\nWÅ›rÃ³d nadzorowanych modeli uczenia maszynowego z wieloma zmiennymi wynikowymi moÅ¼na wymieniÄ‡ zarÃ³wno te dedykowane do klasyfikacji, jak i regresji. Modele te sÄ… znane jako modele z wieloma wyjÅ›ciami (klasyfikacyjne) lub modele z wieloma wyjÅ›ciami (regresyjne), w zaleÅ¼noÅ›ci od rodzaju problemu, ktÃ³ry rozwiÄ…zujÄ….",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "href": "multi_target_models.html#typy-modeli-z-wieloma-zmiennymi-wynikowymi",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "",
    "text": "Modele z wieloma wyjÅ›ciami (klasyfikacyjne)\nW przypadku klasyfikacji, gdy mamy wiele kategorii (klas) jako zmiennÄ… wynikowÄ…, modele te sÄ… nazywane modelami z wieloma wyjÅ›ciami. PrzykÅ‚ady obejmujÄ… algorytmy takie jak regresja logistyczna, metoda k najbliÅ¼szych sÄ…siadÃ³w (k-NN) czy algorytmy drzew decyzyjnych, ktÃ³re zostaÅ‚y dostosowane do obsÅ‚ugi wielu klas.\nPrzykÅ‚adowe zadanie: ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych dotyczÄ…cy rÃ³Å¼nych rodzajÃ³w owocÃ³w (np. jabÅ‚ek, pomaraÅ„czy, bananÃ³w) i chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje gatunek owocu oraz kolor owocu. Mamy wiÄ™c dwie zmienne wynikowe: gatunek (klasyfikacja wieloklasowa) i kolor (klasyfikacja wieloklasowa).\nModele z wieloma wyjÅ›ciami (regresyjne).\nW przypadku regresji, gdzie zmiennÄ… wynikowÄ… jest wektor wartoÅ›ci numerycznych, modele te sÄ… nazywane modelami z wieloma wyjÅ›ciami. PrzykÅ‚ady obejmujÄ… algorytmy regresji liniowej lub nieliniowej, algorytmy oparte na drzewach decyzyjnych, czy teÅ¼ bardziej zaawansowane modele, takie jak sieci neuronowe.\nPrzykÅ‚adowe zadanie: ZakÅ‚adamy, Å¼e mamy zbiÃ³r danych zawierajÄ…cy informacje o pracownikach, takie jak doÅ›wiadczenie zawodowe, poziom wyksztaÅ‚cenia, liczba godzin pracy tygodniowo itp. Chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje zarobki pracownikÃ³w oraz ich poziom satysfakcji zawodowej.\nModele wielozadaniowe.\nModele wielozadaniowe to rodzaj nadzorowanego uczenia maszynowego, w ktÃ³rym model jest trenowany jednoczeÅ›nie do rozwiÄ…zania kilku zadaÅ„. Te zadania mogÄ… obejmowaÄ‡ zarÃ³wno klasyfikacjÄ™, jak i regresjÄ™. DziÄ™ki wspÃ³lnemu trenowaniu modelu na wielu zadaniach, moÅ¼na uzyskaÄ‡ korzyÅ›ci w postaci wspÃ³lnego wykorzystywania wiedzy miÄ™dzy zadaniami.\nPrzykÅ‚adowe zadanie: ZaÅ‚Ã³Å¼my, Å¼e mamy zbiÃ³r danych dotyczÄ…cy zakupÃ³w klientÃ³w w sklepie internetowym. Dla kaÅ¼dego klienta mamy informacje o rÃ³Å¼nych aspektach zakupÃ³w, takich jak czas dostawy, Å‚atwoÅ›Ä‡ obsÅ‚ugi strony, jakoÅ›Ä‡ produktÃ³w itp. Chcemy stworzyÄ‡ model, ktÃ³ry jednoczeÅ›nie przewiduje dwie zmienne wynikowe: jakoÅ›Ä‡ obsÅ‚ugi klienta (skala jakoÅ›ciowa, np. â€œNiskaâ€, â€œÅšredniaâ€, â€œWysokaâ€) oraz caÅ‚kowity wydatek klienta (zmienna iloÅ›ciowa, np. kwota zakupÃ³w).\nModele hierarchiczne.\nW niektÃ³rych przypadkach, szczegÃ³lnie gdy mamy hierarchiÄ™ zmiennych wynikowych, modele te mogÄ… byÄ‡ budowane w sposÃ³b hierarchiczny. PrzykÅ‚adowo, w problemie klasyfikacji obrazÃ³w z hierarchiÄ… kategorii (na przykÅ‚ad rozpoznawanie gatunkÃ³w zwierzÄ…t), model moÅ¼e byÄ‡ zaprojektowany do rozpoznawania zarÃ³wno ogÃ³lnych, jak i bardziej szczegÃ³Å‚owych kategorii.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "multi_target_models.html#rÃ³Å¼nie-podejÅ›cia-do-modelowania-z-wieloma-wyjÅ›ciami",
    "href": "multi_target_models.html#rÃ³Å¼nie-podejÅ›cia-do-modelowania-z-wieloma-wyjÅ›ciami",
    "title": "2Â  Modele z wieloma wyjÅ›ciami",
    "section": "2.2 RÃ³Å¼nie podejÅ›cia do modelowania z wieloma wyjÅ›ciami",
    "text": "2.2 RÃ³Å¼nie podejÅ›cia do modelowania z wieloma wyjÅ›ciami\nIstniejÄ… dwa ogÃ³lne podejÅ›cia do rozwiÄ…zywania problemÃ³w wieloetykietowych: transformacja problemu i adaptacja algorytmu. Transformacja problemu polega na manipulowaniu zbiorem danych w taki sposÃ³b, Å¼e problem wieloetykietowy staje siÄ™ jednym lub kilkoma problemami jednoetykietowymi (Tawiah i Sheng 2013). Adaptacja algorytmu polega na tym, Å¼e sam algorytm jest w stanie poradziÄ‡ sobie bezpoÅ›rednio z problemem wieloetykietowym. Okazuje siÄ™, Å¼e wiele, choÄ‡ nie wszystkie, metody adaptacji algorytmÃ³w metod adaptacji algorytmÃ³w w rzeczywistoÅ›ci wykorzystuje transformacjÄ™ problemu (Tsoumakas i Katakis 2007).\n\n2.2.1 Transformacja problemu\nTechniki te przewidujÄ… stworzenie indywidualnego modelu dla kaÅ¼dego celu, a nastÄ™pnie poÅ‚Ä…czenie oddzielnych modeli w celu uzyskania ogÃ³lnej prognozy. Metody transformacji problemÃ³w okazaÅ‚y siÄ™ lepsze od metod adaptacji algorytmÃ³w pod wzglÄ™dem dokÅ‚adnoÅ›ci (Spyromitros-Xioufis i in. 2016). Co wiÄ™cej, podstawowa zasada sprawia, Å¼e metody transformacji problemu sÄ… niezaleÅ¼ne od algorytmu. W konsekwencji, moÅ¼na je Å‚atwo dostosowaÄ‡ do danego problemu poprzez zastosowanie odpowiednich bazowych metod uczÄ…cych. Punkt ten ma rÃ³wnieÅ¼ szczegÃ³lne znaczenie dla modeli typu ensemble, ktÃ³re Å‚Ä…czÄ… oszacowania z wielu potencjalnie rÃ³Å¼nych algorytmÃ³w w ostatecznÄ… prognozÄ™. Niedawno Spyromitros-Xioufis i in. (2016) zaproponowali rozszerzenie znanych metod transformacji klasyfikacji wieloetykietowej, aby poradziÄ‡ sobie z problemem regresji wielowynikowej i modelowaÄ‡ zaleÅ¼noÅ›ci miÄ™dzy celami. W szczegÃ³lnoÅ›ci wprowadzili oni dwa nowe podejÅ›cia do regresji wielocelowej, skÅ‚adanie regresorÃ³w wielocelowych i Å‚aÅ„cuchy regresorÃ³w, inspirowane popularnymi i skutecznymi podejÅ›ciami do klasyfikacji wieloznaczeniowej.\nPodstawowÄ… koncepcjÄ… w metodach transformacji problemÃ³w jest wykorzystanie poprzednich modeli do nowego przewidywania poprzez rozszerzonÄ… przestrzeÅ„ cech (Borchani i in. 2015). Stacked generalization to podejÅ›cie do meta-uczenia, ktÃ³re wykorzystuje dane wyjÅ›ciowe wczeÅ›niej wyuczonych modeli do uczenia siÄ™ nowego modelu. W zwiÄ…zku z tym poczÄ…tkowe dane wyjÅ›ciowe modelu sÄ… traktowane jako nowe cechy i sÄ… ukÅ‚adane w stos do poczÄ…tkowego wektora cech przed ponownym uczeniem. W oryginalnym sformuÅ‚owaniu przewidziano tylko dwuetapowÄ… procedurÄ™, tj. poczÄ…tkowe modele wyuczone z poczÄ…tkowego wektora cech odpowiadajÄ… odpowiednio modelom i danym poziomu 0, a powiÄ™kszony wektor cech i ponownie wyuczony model sÄ… okreÅ›lane odpowiednio jako dane poziomu 1 i generalizator. JednakÅ¼e, rozsÄ…dnie rzecz biorÄ…c, ten proces ukÅ‚adania pojedynczego celu (ang. Single-target Stacking - STS) moÅ¼e byÄ‡ rÃ³wnieÅ¼ przeprowadzany w wielu iteracjach. Aby wdroÅ¼yÄ‡ tÄ™ zasadÄ™ dla problemÃ³w z wieloma celami, w ktÃ³rych kodowane sÄ… rÃ³wnieÅ¼ moÅ¼liwe korelacje miÄ™dzy zmiennymi docelowymi, wprowadzono koncepcjÄ™ ukÅ‚adania wielu celÃ³w (ang. Multi-target Stacking - MTS) (Borchani i in. 2015). Analogicznie do STS, szkolenie modelu MTS moÅ¼na uznaÄ‡ za procedurÄ™ dwuetapowÄ…. W pierwszym etapie uczone sÄ… niezaleÅ¼ne modele dla kaÅ¼dej zmiennej docelowej. NastÄ™pnie uczone sÄ… meta-modele dla kaÅ¼dej zmiennej docelowej z rozszerzonymi wektorami cech, ktÃ³re zawierajÄ… poczÄ…tkowe wektory cech, a takÅ¼e oszacowania poziomu 0 pozostaÅ‚ych zmiennych docelowych. Podobne pomysÅ‚y byÅ‚y rÃ³wnieÅ¼ stosowane w kontekÅ›cie modeli zespoÅ‚owych, tj. uczenia siÄ™ kilku modeli poziomu 0 dla kaÅ¼dej zmiennej docelowej, ktÃ³re sÄ… Å‚Ä…czone w procedurze uogÃ³lniania poziomu 1 dla wielu zmiennych docelowych (Santana i in. 2020).\n\n2.2.1.1 Single-target stacking\nMetoda ta jest stosowana przede wszystkim z zadaniach regresyjnych z wieloma wyjÅ›ciami. RozwaÅ¼my zbiÃ³r danych \\(D = \\left\\{\\left(\\mathbf{x}^{(1)}, \\mathbf{y}^{(1)}\\right), \\ldots, \\left(\\mathbf{x}^{(N)}, \\mathbf{y}^{(N)}\\right)\\right\\}\\), skÅ‚adajÄ…cy siÄ™ z \\(N\\) obserwacji, ktÃ³re sÄ… realizacjami zmiennych losowych \\(X_1,\\ldots,X_m, Y_1,\\ldots,Y_d\\). Zatem kaÅ¼de wejÅ›cie do modelu jest charakteryzowane przez \\(m\\) zmiennych \\(\\mathbf{x}{(l)}=\\left(x_1^{(l)},\\ldots, x_j^{(l)}, \\ldots, x_m^{(l)} \\right)\\) oraz \\(d\\) odpowiadajÄ…cych im wyjÅ›Ä‡ \\(\\mathbf{y}{(l)}=\\left(y_1^{(l)},\\ldots, y_i^{(l)}, \\ldots, y_d^{(l)} \\right)\\), gdzie \\(l\\in\\{1,\\ldots,N\\}, j\\in\\{1,\\ldots,m\\}, i\\in\\{1,\\ldots,d\\}\\). Naszym celem w zadaniu regresyjnym (MTR - Multi-target Regression) jest nauczenie takiego modelu \\(h\\), ktÃ³ry przeksztaÅ‚ca \\(\\mathbf{x}\\) w \\(\\mathbf{y}\\).\nW podejÅ›ciu STS w pierwszym kroku budowanych jest \\(d\\) niezaleÅ¼nych modeli przewidujÄ…cych pojedyncze wyjÅ›cie. Po tej czynnoÅ›ci meta-model jest trenowany na zbiorze \\(D_i'\\), ktÃ³ry jest wzbogaconym zbiorem \\(D_i\\) o predykcje zmiennej \\(Y_i\\), czyli\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)} \\right)\\). W zaleÅ¼noÅ›ci czy rozpatrujemy algorytm STS niekumulatywny, czy kumulatywny, drugi krok iteracji wyglÄ…da nieco inaczej:\n\nniekumulatywny\n\\[\n\\bar{D}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i'^{(l)} \\right)\\)\nkumulatywny\n\\[\n\\bar{\\bar{D}}_i''=\\left\\{\\left(\\mathbf{x}''^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}''^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}''^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_i^{(l)},\\hat{y}_i'^{(l)} \\right)\\).\n\n\n\n\nSingle-target stacking\n\n\n\n\n2.2.1.2 Multi-target stacking\nW przeciwieÅ„stwie do STS, MTS zostaÅ‚ zaprojektowany do dzielenia siÄ™ wiedzÄ… w skorelowanych zmiennych docelowych w ramach procedury Å‚Ä…czenia w stosy. Podobnie, najpierw uczone sÄ… modele pojedynczego celu. NastÄ™pnie tworzony jest zestaw meta-modeli, ktÃ³re zawierajÄ… model dla kaÅ¼dej zmiennej docelowej \\(Y_i,\\) \\(i \\in \\{1, \\ldots, d\\}\\). W ten sposÃ³b uwzglÄ™dniane sÄ… szacunki dotyczÄ…ce pozostaÅ‚ych zmiennych docelowych z pierwszego etapu, tj. model jest uczony z przeksztaÅ‚conego zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, \\hat{y}_1^{(l)},\\ldots,\\hat{y}_d^{(l)} \\right)\\). W metodzie MTS istniejÄ… rÃ³wnieÅ¼ dwa sposoby skÅ‚adania kolejnych iteracji. PrzebiegajÄ… one w podobny sposÃ³b jak w przypadku STS.\n\n\n\nMulti-target stacking\n\n\nIstnieje jeszcze trzecia metoda powszechnie stosowana do predykcji wielowyniowej zwana Regressor Chains lub Classifier Chains w zaleÅ¼noÅ›ci od celu zadania. IdÄ™ dziaÅ‚ania tej metody przedstawiÄ™ na przykÅ‚adzie modelu regresyjnego.\n\n\n2.2.1.3 Regressor Chains\nRC opierajÄ… siÄ™ na idei dopasowywania modeli pojedynczego celu wzdÅ‚uÅ¼ wybranej permutacji, tj. Å‚aÅ„cucha. Najpierw losowana jest permutacja w odniesieniu do zmiennych docelowych. Proces ten moÅ¼na przeprowadziÄ‡ w sposÃ³b losowy (Spyromitros-Xioufis i in. 2016) lub uporzÄ…dkowany (Melki i in. 2017). Wybrana permutacja jest wykorzystywana do zbudowania oddzielnego modelu regresji dla zmiennych docelowych zgodnie z kolejnoÅ›ciÄ… permutacji. Aby wykorzystaÄ‡ tÄ™ strukturÄ™ do MTR, rzeczywiste wartoÅ›ci zmiennych docelowych sÄ… dostarczane do kolejnych modeli podczas uczenia siÄ™ wzdÅ‚uÅ¼ Å‚aÅ„cucha. Na podstawie peÅ‚nego Å‚aÅ„cucha lub wybranego zestawu \\(C = (Y_1,\\ldots,Y_d)\\), pierwszy model jest ograniczony do ustalenia predykcji dla \\(Y_1\\). NastÄ™pnie, kolejno dla \\(Y_i\\) uczone sÄ… modele na podstawie zbioru\n\\[\nD_i'=\\left\\{\\left(\\mathbf{x}'^{(1)}, \\mathbf{y}_i^{(1)}\\right), \\ldots, \\left(\\mathbf{x}'^{(N)}, \\mathbf{y}_i^{(N)}\\right)\\right\\},\n\\]\ngdzie \\(\\mathbf{x}'^{(l)} =\\left(x_1^{(l)},\\ldots, x_m^{(l)}, y_1^{(l)},\\ldots, y_{i-1}^{(l)} \\right)\\). Ten algorytm ma rÃ³wnieÅ¼ dwie odmiany (niekumulatywnÄ… i kumulatywnÄ…) w zaleÅ¼noÅ›ci od ksztaÅ‚tu kolejnych iteracji.\n\n\n\nRegressor chains\n\n\nPoniewaÅ¼, jak moÅ¼na siÄ™ spodziewaÄ‡ wyniki modelowania w znaczny sposÃ³b zaleÅ¼Ä… od wylosowanej permutacji, to w metodzie zaproponowanej przez Melki i in. (2017) aby uniknÄ…Ä‡ tego efektu buduje siÄ™ \\(k\\) modeli dla rÃ³Å¼nych permutacji i Å‚Ä…czy siÄ™ wyniki w podobny sposÃ³b jak w lasach losowych.\n\n\n\n\n\n\n\n\n\nAdnotacja\n\n\n\nSÅ‚owo komentarza jeÅ›li chodzi o dostÄ™pnoÅ›Ä‡ tych metod w jÄ™zykach programowania. Niestety wspomniane metody w R nie sÄ… zaimplementowane w sposÃ³b, ktÃ³ry pozwalaÅ‚by na bezpieczne uÅ¼ywanie przygotowanych rozwiÄ…zaÅ„. Istnieje kilka wzmianek (na dzieÅ„ dzisiejszy, czyli poczÄ…tek 2024 roku) na ten temat. TwÃ³rcy dwÃ³ch gÅ‚Ã³wnych frameworkÃ³w do uczenia maszynowego, czyli mlr3 oraz tidymodels przygotowujÄ… implementacje tych metod. Dodatkowo istnieje rozwiÄ…zanie w wersji eksperymentalnej mtr-toolkit, ktÃ³re pozwala na wykonanie modelowania z wieloma wyjÅ›ciami, ktÃ³rym moÅ¼na siÄ™ posiÅ‚kowaÄ‡.\nNiestety w przypadku Python-a nie jest duÅ¼o lepiej. Wprawdzie w pakiecie scikit-learn istniejÄ… implementacje pozwalajÄ…ce na predykcje wielowyjÅ›ciowe w obu typach zadaÅ„ poprzez MultiOutputRegressor i MultiOutputClassifier, ale dokonujÄ… one predykcji naiwnej poprzez zÅ‚oÅ¼enie w listÄ™ wynikÃ³w pojedynczych modeli dla kaÅ¼dej zmiennej. Nieco lepiej sprawa wyglÄ…da w przypadku metod Å‚aÅ„cuchowych, poniewaÅ¼ zarÃ³wno dla klasyfikacji, jak i regresji sÄ… metody to realizujÄ…ce (ClassifierChain i RegressorChain).\n\n\n\n\n\n2.2.2 Adaptacja algorytmu\nProstota podejÅ›cia transformacji problemu sprawia, Å¼e jest ono odpowiednie dla problemÃ³w, w ktÃ³rych jego wady majÄ… niewielki lub Å¼aden wpÅ‚yw - jednak dla zÅ‚oÅ¼onych problemÃ³w podejÅ›cie adaptacji algorytmu moÅ¼e okazaÄ‡ siÄ™ bardziej efektywne. Dodatkowo, dowody empiryczne sugerujÄ…, Å¼e uczenie siÄ™ powiÄ…zanych zadaÅ„ jednoczeÅ›nie, a nie niezaleÅ¼nie, moÅ¼e poprawiÄ‡ wyniki predykcyjne (Evgeniou i Pontil 2004). Z drugiej strony, jeÅ›li zadania sÄ… bardzo odmienne, wydajnoÅ›Ä‡ predykcyjna moÅ¼e ucierpieÄ‡, gdy zadania sÄ… uczone razem, a nie niezaleÅ¼nie (Faddoul i in. 2010). W zwiÄ…zku z tym moÅ¼emy wyciÄ…gnÄ…Ä‡ nastÄ™pujÄ…ce wnioski:\n\njeÅ›li zadania, ktÃ³rych nasz predyktor ma siÄ™ nauczyÄ‡, sÄ… powiÄ…zane, powinniÅ›my dÄ…Å¼yÄ‡ do znalezienia odpowiedniej metody adaptacji algorytmu;\njeÅ›li zadania, ktÃ³rych chcemy siÄ™ nauczyÄ‡, nie sÄ… powiÄ…zane, powinniÅ›my zamiast tego dÄ…Å¼yÄ‡ do znalezienia odpowiedniej metody transformacji problemu.\n\nWreszcie, powinniÅ›my wziÄ…Ä‡ pod uwagÄ™ rozmiar problemu i zdaÄ‡ sobie sprawÄ™, Å¼e gdy zadania sÄ… niepowiÄ…zane, istnieje potencjalny kompromis miÄ™dzy efektywnoÅ›ciÄ… czasowÄ… a wydajnoÅ›ciÄ… predykcyjnÄ… przy wyborze metody transformacji problemu lub metody adaptacji algorytmu. W przypadku niepowiÄ…zanych ze sobÄ… zadaÅ„, metody transformacji problemu mogÄ… zwiÄ™kszaÄ‡ skutecznoÅ›Ä‡ predykcyjnÄ…, ale zmniejszaÄ‡ wydajnoÅ›Ä‡ czasowÄ… w przypadku duÅ¼ych problemÃ³w i odwrotnie.\nNiestety tej metody nie da siÄ™ zastosowaÄ‡ do kaÅ¼dego typu modelu. Rodzina modeli, ktÃ³rych adaptacja jest wykonana caÅ‚y czas roÅ›nie. Adaptacja modelu polega na przeksztaÅ‚ceniu go do postaci, w ktÃ³rej da siÄ™ wykonaÄ‡ predykcjÄ™ dla wielu wyjÅ›Ä‡. WÅ›rÃ³d modeli, ktÃ³rych wersje native multi-target istniejÄ… naleÅ¼y wymieniÄ‡:\n\nregresja wieloraka (Izenman 1975)\nkNN\ndrzewo decyzyjne (Struyf i DÅ¾eroski 2006)\nlas losowy (Kocev i in. 2013)\nbagging (Kocev i in. 2013)\ngradient boosting (Zhang i Jung, b.d.; Faddoul i in. 2012)\nSVM (Xu, Guo, i Wang 2013; Vazquez i Walter 2003)\nno i oczywiÅ›cie sieci neuronowe.\n\nNie sposÃ³b przedstawiÄ‡ w jaki sposÃ³b wprowadzone zostaÅ‚y zmiany we wszystkich algorytmach. SkupiÄ™ siÄ™ jednak na pokazaniu adaptacji drzew decyzyjnych do predykcji wielu wyjÅ›Ä‡ jednoczeÅ›nie, poniewaÅ¼ jest to meta-model modeli takich jak lasy losowe, bagging czy boosting.\n\n2.2.2.1 Adaptacja klasyfikacyjnego drzewa decyzyjnego\nFaddoul i in. (2012) zaproponowali zmodyfikowanÄ… wersjÄ™ algorytmu drzewa decyzyjnego C4.5 (Quinlan 1993), ktÃ³ra bezpoÅ›rednio obsÅ‚uguje problemy klasyfikacji wielowyjÅ›ciowej. Zmodyfikowana wersja (nazwana MT-DT) rÃ³Å¼ni siÄ™ od standardowej implementacji C4.5 w dwÃ³ch krytycznych aspektach: kryteriach podziaÅ‚u wÄ™zÅ‚Ã³w i procesie decyzyjnym. Faddoul i in. (2012) proponujÄ… trzy rÃ³Å¼ne podejÅ›cia do Å‚Ä…czenia wielu miar przyrostu informacji w jednÄ… miarÄ™: wspÃ³lny przyrost informacji, suma niewaÅ¼ona i maksymalny przyrost informacji. WspÃ³lny przyrost informacyjny jest definiowany przy uÅ¼yciu konkatenacji wszystkich poszczegÃ³lnych zadaÅ„, tj. wzglÄ™dnej rÃ³Å¼nicy w entropii mierzonej we wszystkich zadaniach decyzyjnych. Autorzy pokazujÄ…, Å¼e niewaÅ¼ona suma (RÃ³wnanieÂ 2.1) indywidualnych przyrostÃ³w informacyjnych wszystkich zadaÅ„ jest rÃ³wnowaÅ¼na wspÃ³lnemu przyrostowi informacyjnemu.\n\\[\nIG_U=\\sum_YIG_Y\n\\tag{2.1}\\]\nMaksymalny przyrost informacyjny, zgodnie z propozycjÄ… autorÃ³w jest definiowany po prostu jako maksymalny przyrost informacyjny wszystkich zadaÅ„:\n\\[\nIG_M=\\max_YIG_Y\n\\tag{2.2}\\]\nBadania eksperymentalne pokazaÅ‚y, Å¼e maksymalny przyrost informacyjny wykorzystany do budowania reguÅ‚ podziaÅ‚u, charakteryzuje siÄ™ wyÅ¼szym poziomem dopasowania modeli, niÅ¼ przy zastosowaniu \\(IG_U\\) i \\(IG_J\\).\nW przypadku klasyfikacji z jednÄ… etykietÄ…, algorytm indukcji drzewa decyzyjnego (taki jak C4.5) rekurencyjnie dzieli wÄ™zÅ‚y, dodajÄ…c (zazwyczaj dwa) elementy potomne, aÅ¼ moÅ¼liwe jest utworzenie liÅ›cia takiego, Å¼e znaczna wiÄ™kszoÅ›Ä‡ (lub nawet wszystkie) jego przykÅ‚adowych instancji naleÅ¼y do tej samej klasy. W przypadku wielu wyjÅ›Ä‡, indukcja drzewa niekoniecznie jest tak prosta. RozwaÅ¼my problem klasyfikacji wielowyjÅ›ciowej z dwoma wyjÅ›ciami binarnymi \\(\\nu_1\\) i \\(\\nu_2\\); moÅ¼liwe jest, Å¼e po \\(t\\) podziaÅ‚ach, wÄ™zeÅ‚ zawiera tylko wartoÅ›ci pozytywne dla \\(\\nu_1\\), ale mieszankÄ™ wartoÅ›ci pozytywnych i negatywnych dla \\(\\nu_2\\) - stÄ…d, podczas konstruowania drzew decyzyjnych dla wielu jednoczesnych zadaÅ„, naleÅ¼y pamiÄ™taÄ‡, Å¼e proces decyzyjny dla pewnego zadania moÅ¼e wymagaÄ‡ krÃ³tszej Å›cieÅ¼ki decyzyjnej niÅ¼ inne zadania w ramach tego samego problemu wielowyjÅ›ciowego. MT-DT radzi sobie z tym, sprawdzajÄ…c w kaÅ¼dym wÄ™Åºle, czy moÅ¼liwe jest utworzenie wÄ™zÅ‚a terminalnego dla ktÃ³regokolwiek z zadaÅ„ - w powyÅ¼szym przykÅ‚adzie spowodowaÅ‚oby to utworzenie drzewa, w ktÃ³rym wewnÄ™trzny wÄ™zeÅ‚ \\(t_1\\) jest oznaczony jako wÄ™zeÅ‚ zatrzymania dla \\(\\nu_1\\), oznaczony klasÄ… pozytywnÄ…. PoniewaÅ¼ celem jest prognozowanie dla obu wyjÅ›Ä‡ binarnych, \\(t_1\\) nie jest wÄ™zÅ‚em liÅ›cia - zamiast tego rekurencyjne dzielenie jest kontynuowane od \\(t_1\\), aÅ¼ do znalezienia wÄ™zÅ‚a \\(t_2\\) takiego, Å¼e \\(t_2\\) jest wystarczajÄ…co czysty w odniesieniu do \\(\\nu_2\\), aby moÅ¼na byÅ‚o utworzyÄ‡ reguÅ‚Ä™ klasyfikacji dla drugiego zadania binarnego. W tym momencie wÄ™zÅ‚y decyzyjne (wÄ™zÅ‚y wewnÄ™trzne lub liÅ›cie) zostaÅ‚y znalezione dla wszystkich wynikÃ³w (\\(\\nu_1\\) i \\(\\nu_2\\)), a algorytm indukcji drzewa rekurencyjnego moÅ¼e zostaÄ‡ zakoÅ„czony.\nNic dziwnego, Å¼e klasyfikacja przy uÅ¼yciu juÅ¼ zbudowanego modelu MT-DT przebiega wedÅ‚ug tej samej formuÅ‚y, co jego indukcja - podczas przechodzenia przez drzewo kaÅ¼dy wÄ™zeÅ‚ jest sprawdzany w celu ustalenia, czy moÅ¼na podjÄ…Ä‡ decyzjÄ™ dla ktÃ³regokolwiek z aktualnie nierozstrzygniÄ™tych zadaÅ„. W przykÅ‚adzie \\(\\nu_1\\), \\(\\nu_2\\), klasyfikacja zostanie dokonana dla \\(\\nu_1\\) w wÄ™Åºle \\(t_1\\), poniewaÅ¼ jest on oznaczony jako wÄ™zeÅ‚ zatrzymania dla \\(\\nu_1\\); nastÄ™pnie przejÅ›cie jest kontynuowane do momentu napotkania \\(t_2\\) i klasyfikacja moÅ¼e zostaÄ‡ dokonana dla \\(\\nu_2\\). W tym momencie wszystkie wyjÅ›cia zostaÅ‚y sklasyfikowane, a przechodzenie moÅ¼e siÄ™ zakoÅ„czyÄ‡, zwracajÄ…c dwie wartoÅ›ci w \\(t_1\\) i \\(t_2\\) jako klasyfikacje odpowiednio dla \\(\\nu_1\\) i \\(\\nu_2\\).\n\n\n2.2.2.2 Adaptacja regresyjnego drzewa decyzyjnego\nSegal (1992) zaproponowaÅ‚ rozwiÄ…zanie dla drzew regresyjnych o wielu wyjÅ›ciach (MRT), ktÃ³re sÄ… w stanie przewidywaÄ‡ wyniki dla wielu powiÄ…zanych zadaÅ„ regresyjnych; te wielowyjÅ›ciowe drzewa regresyjne sÄ… oparte na funkcji podziaÅ‚u najmniejszych kwadratÃ³w zaproponowanej w ramach CART (Breiman i in. 2017). W przypadku drzewa regresyjnego o jednej odpowiedzi celem jest minimalizacja nastÄ™pujÄ…cej funkcji celu:\n\\[\n\\phi(t) = SS(t)-SS(t_L)-SS(t_R)\n\\]\ngdzie \\(SS(t)\\) jest zdefiniowana nastÄ™pujÄ…co\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))^2.\n\\]\nSegal (1992) dodaÅ‚ waÅ¼enie macierzÄ… kowariancji do bÅ‚Ä™du kwadratowego, co prowadzi algorytm drzewa do tworzenia wÄ™zÅ‚Ã³w potomnych, ktÃ³re reprezentujÄ… jednorodne klastry w odniesieniu do zestawu odpowiedzi wyjÅ›ciowych:\n\\[\nSS(t) = \\sum_{i\\in t}(y_i-\\bar{y}(t))'V^{-1}(t)(y_i-\\bar{y}(t)),\n\\]\ngdzie \\(V(t)\\) oznacza macierz kowariancji w wÄ™Åºle \\(t\\).\n\n\n\n\nBorchani, Hanen, Gherardo Varando, Concha Bielza, i Pedro LarraÃ±aga. 2015. â€A Survey on Multi-Output Regressionâ€. WIREs Data Mining and Knowledge Discovery 5 (5): 216â€“33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, i Charles J. Stone. 2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, i Massimiliano Pontil. 2004. â€Regularized multiâ€“task learningâ€. Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, sierpieÅ„. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, RÃ©mi Gilleron, i Fabien Torre. 2012. â€Learning multiple tasks with boosted decision treesâ€. W Proceedings of the 2012th European Conference on Machine Learning and Knowledge Discovery in Databases - Volume Part I, 681â€“96. ECMLPKDDâ€™12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, i Remi Gilleron. 2010. â€Boosting Multi-Task Weak Learners with Applications to Textual and Social Dataâ€. W 2010 Ninth International Conference on Machine Learning and Applications, 367â€“72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nIzenman, Alan Julian. 1975. â€Reduced-rank regression for the multivariate linear modelâ€. Journal of multivariate analysis 5 (2): 248â€“64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, i SaÅ¡o DÅ¾eroski. 2013. â€Tree ensembles for predicting structured outputsâ€. Pattern Recognition 46 (3): 817â€“33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, i SebastiÃ¡n Ventura. 2017. â€Multi-Target Support Vector Regression via Correlation Regressor Chainsâ€. Information Sciences 415â€“416 (listopad): 53â€“69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning. Morgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello Mastelini, Fabio Luiz Melquiades, i Sylvio Barbon Jr. 2020. â€Improved Prediction of Soil Properties with Multi-Target Stacked Generalisation on EDXRF Spectraâ€. arXiv preprint arXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. â€Tree-Structured Methods for Longitudinal Dataâ€. Journal of the American Statistical Association 87 (418): 407â€“18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves, i Ioannis Vlahavas. 2016. â€Multi-Target Regression via Input Space Expansion: Treating Targets as Inputsâ€. Machine Learning 104 (1): 55â€“98. https://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, i SaÅ¡o DÅ¾eroski. 2006. â€Constraint Based Induction of Multi-objective Regression Treesâ€. W Knowledge Discovery in Inductive Databases, zredagowane przez Francesco Bonchi i Jean-FranÃ§ois Boulicaut, 222â€“33. Lecture Notes w Computer Science. Springer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, i Victor Sheng. 2013. â€Empirical Comparison of Multi-Label Classification Algorithmsâ€. W Proceedings of the AAAI Conference on Artificial Intelligence, 27:1645â€“46.\n\n\nTsoumakas, Grigorios, i Ioannis Katakis. 2007. â€Multi-Label Classification: An Overviewâ€. International Journal of Data Warehousing and Mining (IJDWM) 3 (3): 1â€“13.\n\n\nVazquez, Emmanuel, i Eric Walter. 2003. â€Multi-Output Suppport Vector Regressionâ€. IFAC Proceedings Volumes, 13th IFAC Symposium on System Identification (SYSID 2003), Rotterdam, The Netherlands, 27-29 August, 2003, 36 (16): 1783â€“88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, i Laisheng Wang. 2013. â€A Twin Multi-Class Classification Support Vector Machineâ€. Cognitive Computation 5 (4): 580â€“88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, i Cheolkon Jung. b.d. â€GBDT-MO: Gradient Boosted Decision Trees for Multiple Outputsâ€. https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Modele z wieloma wyjÅ›ciami</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatura",
    "section": "",
    "text": "Borchani, Hanen, Gherardo Varando, Concha Bielza, and Pedro LarraÃ±aga.\n2015. â€œA Survey on Multi-Output Regression.â€ WIREs Data\nMining and Knowledge Discovery 5 (5): 216â€“33. https://doi.org/10.1002/widm.1157.\n\n\nBreiman, Leo, J. H. Friedman, Richard A. Olshen, and Charles J. Stone.\n2017. Classification and Regression Trees. Routledge. http://search.ebscohost.com/login.aspx?direct=true&db=edsebk&AN=1619230&lang=pl&site=eds-live&scope=site.\n\n\nEvgeniou, Theodoros, and Massimiliano Pontil. 2004. â€œRegularized\nMultiâ€“Task Learning.â€ Proceedings of the Tenth ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining,\nAugust. https://doi.org/10.1145/1014052.1014067.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, RÃ©mi Gilleron, and Fabien\nTorre. 2012. â€œLearning Multiple Tasks with Boosted Decision\nTrees.â€ In Proceedings of the 2012th European Conference on\nMachine Learning and Knowledge Discovery in Databases - Volume Part\ni, 681â€“96. ECMLPKDDâ€™12. Springer-Verlag.\n\n\nFaddoul, Jean Baptiste, Boris Chidlovskii, Fabien Torre, and Remi\nGilleron. 2010. â€œBoosting Multi-Task Weak Learners with\nApplications to Textual and Social Data.â€ In 2010 Ninth\nInternational Conference on Machine Learning and Applications,\n367â€“72. IEEE. https://doi.org/10.1109/ICMLA.2010.61.\n\n\nIzenman, Alan Julian. 1975. â€œReduced-Rank Regression for the\nMultivariate Linear Model.â€ Journal of Multivariate\nAnalysis 5 (2): 248â€“64.\n\n\nKocev, Dragi, Celine Vens, Jan Struyf, and SaÅ¡o DÅ¾eroski. 2013.\nâ€œTree Ensembles for Predicting Structured Outputs.â€\nPattern Recognition 46 (3): 817â€“33. https://doi.org/10.1016/j.patcog.2012.09.023.\n\n\nMelki, Gabriella, Alberto Cano, Vojislav Kecman, and SebastiÃ¡n Ventura.\n2017. â€œMulti-Target Support Vector Regression via Correlation\nRegressor Chains.â€ Information Sciences 415â€“416\n(November): 53â€“69. https://doi.org/10.1016/j.ins.2017.06.017.\n\n\nQuinlan, J Ross. 1993. C4. 5: Programs for Machine Learning.\nMorgan Kaufmann.\n\n\nSantana, Everton Jose, Felipe Rodrigues dos Santos, Saulo Martiello\nMastelini, Fabio Luiz Melquiades, and Sylvio Barbon Jr. 2020.\nâ€œImproved Prediction of Soil Properties with Multi-Target Stacked\nGeneralisation on EDXRF Spectra.â€ arXiv Preprint\narXiv:2002.04312. https://arxiv.org/abs/2002.04312.\n\n\nSegal, Mark Robert. 1992. â€œTree-Structured Methods for\nLongitudinal Data.â€ Journal of the American Statistical\nAssociation 87 (418): 407â€“18. https://doi.org/10.2307/2290271.\n\n\nSpyromitros-Xioufis, Eleftherios, Grigorios Tsoumakas, William Groves,\nand Ioannis Vlahavas. 2016. â€œMulti-Target Regression\nvia Input Space Expansion: Treating Targets as\nInputs.â€ Machine Learning 104 (1): 55â€“98.\nhttps://doi.org/10.1007/s10994-016-5546-z.\n\n\nStruyf, Jan, and SaÅ¡o DÅ¾eroski. 2006. â€œConstraint Based Induction\nof Multi-Objective Regression Trees.â€ In Knowledge Discovery\nin Inductive Databases, edited by Francesco Bonchi and\nJean-FranÃ§ois Boulicaut, 222â€“33. Lecture Notes in Computer Science.\nSpringer. https://doi.org/10.1007/11733492_13.\n\n\nTawiah, Clifford, and Victor Sheng. 2013. â€œEmpirical Comparison of\nMulti-Label Classification Algorithms.â€ In Proceedings of the\nAAAI Conference on Artificial\nIntelligence, 27:1645â€“46.\n\n\nTsoumakas, Grigorios, and Ioannis Katakis. 2007. â€œMulti-Label\nClassification: An Overview.â€ International\nJournal of Data Warehousing and Mining (IJDWM) 3 (3): 1â€“13.\n\n\nVazquez, Emmanuel, and Eric Walter. 2003. â€œMulti-Output Suppport\nVector Regression.â€ IFAC Proceedings Volumes, 13th IFAC\nsymposium on system identification (SYSID 2003), rotterdam, the\nnetherlands, 27-29 august, 2003, 36 (16): 1783â€“88. https://doi.org/10.1016/S1474-6670(17)35018-8.\n\n\nXu, Yitian, Rui Guo, and Laisheng Wang. 2013. â€œA Twin Multi-Class\nClassification Support Vector Machine.â€ Cognitive\nComputation 5 (4): 580â€“88. https://doi.org/10.1007/s12559-012-9179-7.\n\n\nZhang, Zhendong, and Cheolkon Jung. n.d. â€œGBDT-MO: Gradient\nBoosted Decision Trees for Multiple Outputs.â€ https://doi.org/10.48550/arXiv.1909.04373.",
    "crumbs": [
      "Literatura"
    ]
  }
]